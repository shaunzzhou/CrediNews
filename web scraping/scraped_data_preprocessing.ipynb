{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Detection to Credibility: A Machine Learning Framework for Assessing News Source Reliability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n",
    "As media continues to grow in volume, it is becoming increasingly difficult to differentiate real and fake news effectively. It is thus imperative for us to find better ways to identify fake news, and for us, this means with the help of data mining and machine learning.\n",
    "\n",
    "In the first part of our project, we will focus on experimenting with different data processing techniques and predictive models, optimising our final pipeline and model to accurately identify fake news.\n",
    "\n",
    "For the second part, we want to apply our trained model to scraped news data from popular US media outlets and access the credibility of these media outlets. This way we can help the public to make more informed decisions about what media outlets they can trust. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd Part: Fake News Classification Use Case\n",
    "\n",
    "For the second part, we scraped articles from 10 different news sites, split into two categories of news sites. \n",
    "\n",
    "The dimensions are shown below:\n",
    "- **Index:**: Index.\n",
    "- **title:** Title of news article.\n",
    "- **text:** Text content of news article.\n",
    "- **label:** Whether news article is real (0) or fake (1).\n",
    "\n",
    "The Fake News Dataset is split into 3 `csv` files (`part1.csv`, `part2.csv`, `part3.csv`) so that size does not exceed size limit to push changes to GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Please uncomment the code box below to pip install relevant dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# For concurrency (running functions in parallel)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# For caching (to speed up repeated function calls)\n",
    "from functools import lru_cache\n",
    "\n",
    "# For progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting and Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Language Detection packages\n",
    "# `langdetect` for detecting language\n",
    "from langdetect import detect as langdetect_detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "# `langid` for an alternative language detection method\n",
    "from langid import classify as langid_classify\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Part-of-speech tagging\n",
    "from nltk import pos_tag\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "# Regular expressions for text pattern matching\n",
    "import re\n",
    "\n",
    "# Word Cloud generation\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unreliable websites\n",
    "breitbart_df = pd.read_csv('./unreliable websites/breitbart_articles.csv')\n",
    "dailycaller_df = pd.read_csv('./unreliable websites/dailycaller_articles.csv')\n",
    "naturalnews_df = pd.read_csv('./unreliable websites/naturalnews_articles.csv')\n",
    "newsmax_df = pd.read_csv('./unreliable websites/newsmax_articles.csv')\n",
    "zerohedge_df = pd.read_csv('./unreliable websites/zerohedge_articles.csv')\n",
    "\n",
    "# Reliable websites\n",
    "cnn_df = pd.read_csv('./reliable websites/cnn_articles.csv')\n",
    "ap_df = pd.read_csv('./reliable websites/ap_articles.csv')\n",
    "bbc_df = pd.read_csv('./reliable websites/bbc_articles.csv')\n",
    "npr_df = pd.read_csv('./reliable websites/npr_articles.csv')\n",
    "guardian_df = pd.read_csv('./reliable websites/guardian_articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all 10 dataframes into 1\n",
    "Here we combine all 10 dataframes into 1 dataframe (`data_raw`) by concatenating along rows.\n",
    "\n",
    "We also reset the `index` and drop the old `index` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 955 entries, 0 to 954\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   source   955 non-null    object\n",
      " 1   title    955 non-null    object\n",
      " 2   content  955 non-null    object\n",
      " 3   date     955 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 30.0+ KB\n",
      "Dataframe Shape:  (955, 4)\n"
     ]
    }
   ],
   "source": [
    "# Combining all 3 dataframes into 1\n",
    "data_raw = pd.concat([breitbart_df, dailycaller_df, naturalnews_df,newsmax_df,zerohedge_df,cnn_df,ap_df,bbc_df,npr_df,guardian_df], axis=0)\n",
    "\n",
    "# Reset index and drop old index column\n",
    "data_raw = data_raw.reset_index(drop=True)\n",
    "\n",
    "data_raw.info()\n",
    "print(\"Dataframe Shape: \", data_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Russia Practices 'Massive Nuclear Strike' in A...</td>\n",
       "      <td>“Important to have modern and constantly ready...</td>\n",
       "      <td>30/10/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Nolte: Trump and Republican Senate Challenger ...</td>\n",
       "      <td>The final Insider Advantage poll out of Wiscon...</td>\n",
       "      <td>29/10/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Sen. Tom Cotton Barnstorms Nation Campaigning ...</td>\n",
       "      <td>Sen. Tom Cotton (R-AR) is putting in long hour...</td>\n",
       "      <td>29/10/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>U.S. Marines Successfully Test Israel's Iron D...</td>\n",
       "      <td>The U.S. Marines have successfully tested a mo...</td>\n",
       "      <td>30/10/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Seoul, Japan Warn North Korea Preparing Nuclea...</td>\n",
       "      <td>The South Korean Defense Intelligence Agency (...</td>\n",
       "      <td>30/10/2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source                                              title  \\\n",
       "0  Breitbart  Russia Practices 'Massive Nuclear Strike' in A...   \n",
       "1  Breitbart  Nolte: Trump and Republican Senate Challenger ...   \n",
       "2  Breitbart  Sen. Tom Cotton Barnstorms Nation Campaigning ...   \n",
       "3  Breitbart  U.S. Marines Successfully Test Israel's Iron D...   \n",
       "4  Breitbart  Seoul, Japan Warn North Korea Preparing Nuclea...   \n",
       "\n",
       "                                             content        date  \n",
       "0  “Important to have modern and constantly ready...  30/10/2024  \n",
       "1  The final Insider Advantage poll out of Wiscon...  29/10/2024  \n",
       "2  Sen. Tom Cotton (R-AR) is putting in long hour...  29/10/2024  \n",
       "3  The U.S. Marines have successfully tested a mo...  30/10/2024  \n",
       "4  The South Korean Defense Intelligence Agency (...  30/10/2024  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "The Daily Caller    100\n",
       "Natural News        100\n",
       "Zerohedge           100\n",
       "Breitbart            99\n",
       "CNN                  99\n",
       "Guardian             99\n",
       "AP                   97\n",
       "News Max             95\n",
       "NPR                  86\n",
       "BBC                  80\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection Criteria\n",
    "- Language: Only use articles written in English (using language detection if necessary).\n",
    "- Date Range: Focus on articles published in 2024 or, at most, 2023 to ensure credibility remains up-to-date.\n",
    "- Content Focus: Only articles from the politics/election sections of each news website, targeting US election-related topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Filtering\n",
    "- Define Political Content: Articles should contain keywords like “election,” “government,” “policy,” or “candidate” to qualify as political.\n",
    "- Exclude Advertisements: Filter out content with commercial keywords or phrases related to advertisements.\n",
    "- Exclude Opinion Pieces: Identify and exclude articles labeled as “opinion,” “editorial,” or similar terms, or those appearing in “Opinion” sections.\n",
    "- Remove Outliers by Length: Filter out articles with fewer than 100 words or more than 5,000 words to focus on substantial content.\n",
    "- Regex Filtering: Use regular expressions to remove boilerplate or irrelevant sections, such as “All rights reserved,” “Read more,” bylines, and embedded links to other articles or advertisements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Language Detection: 100%|██████████| 955/955 [00:17<00:00, 54.20it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Russia Practices 'Massive Nuclear Strike' in A...</td>\n",
       "      <td>“Important to have modern and constantly ready...</td>\n",
       "      <td>30/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Nolte: Trump and Republican Senate Challenger ...</td>\n",
       "      <td>The final Insider Advantage poll out of Wiscon...</td>\n",
       "      <td>29/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Sen. Tom Cotton Barnstorms Nation Campaigning ...</td>\n",
       "      <td>Sen. Tom Cotton (R-AR) is putting in long hour...</td>\n",
       "      <td>29/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>U.S. Marines Successfully Test Israel's Iron D...</td>\n",
       "      <td>The U.S. Marines have successfully tested a mo...</td>\n",
       "      <td>30/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Seoul, Japan Warn North Korea Preparing Nuclea...</td>\n",
       "      <td>The South Korean Defense Intelligence Agency (...</td>\n",
       "      <td>30/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>How can the candidate with most votes lose? Th...</td>\n",
       "      <td>Even though the United States touts its status...</td>\n",
       "      <td>19/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>The RBA will likely hold interest rates – but ...</td>\n",
       "      <td>When the Reserve Bank board meet next week to ...</td>\n",
       "      <td>30/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>Trump’s mass deportation plan would be ‘econom...</td>\n",
       "      <td>If elected, Donald Trump plans to carry out “t...</td>\n",
       "      <td>30/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>Sun belt to October surprise: US election term...</td>\n",
       "      <td>On Tuesday 5 November, Americans will vote aft...</td>\n",
       "      <td>10/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>I don’t expect stone-cold truths from a chatsh...</td>\n",
       "      <td>In a development that absolutely must not catc...</td>\n",
       "      <td>29/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>955 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source                                              title  \\\n",
       "0    Breitbart  Russia Practices 'Massive Nuclear Strike' in A...   \n",
       "1    Breitbart  Nolte: Trump and Republican Senate Challenger ...   \n",
       "2    Breitbart  Sen. Tom Cotton Barnstorms Nation Campaigning ...   \n",
       "3    Breitbart  U.S. Marines Successfully Test Israel's Iron D...   \n",
       "4    Breitbart  Seoul, Japan Warn North Korea Preparing Nuclea...   \n",
       "..         ...                                                ...   \n",
       "950   Guardian  How can the candidate with most votes lose? Th...   \n",
       "951   Guardian  The RBA will likely hold interest rates – but ...   \n",
       "952   Guardian  Trump’s mass deportation plan would be ‘econom...   \n",
       "953   Guardian  Sun belt to October surprise: US election term...   \n",
       "954   Guardian  I don’t expect stone-cold truths from a chatsh...   \n",
       "\n",
       "                                               content        date language  \n",
       "0    “Important to have modern and constantly ready...  30/10/2024       en  \n",
       "1    The final Insider Advantage poll out of Wiscon...  29/10/2024       en  \n",
       "2    Sen. Tom Cotton (R-AR) is putting in long hour...  29/10/2024       en  \n",
       "3    The U.S. Marines have successfully tested a mo...  30/10/2024       en  \n",
       "4    The South Korean Defense Intelligence Agency (...  30/10/2024       en  \n",
       "..                                                 ...         ...      ...  \n",
       "950  Even though the United States touts its status...  19/10/2024       en  \n",
       "951  When the Reserve Bank board meet next week to ...  30/10/2024       en  \n",
       "952  If elected, Donald Trump plans to carry out “t...  30/10/2024       en  \n",
       "953  On Tuesday 5 November, Americans will vote aft...  10/10/2024       en  \n",
       "954  In a development that absolutely must not catc...  29/10/2024       en  \n",
       "\n",
       "[955 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Set a seed for langdetect to ensure reproducibility\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# 2a) Simplified preprocessing: only remove non-alphabetic characters\n",
    "def preprocess_text_simple(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    return text.strip()\n",
    "\n",
    "# 2b) Check if the text is non-language (e.g., numbers, symbols only)\n",
    "def is_non_language_text(text):\n",
    "    if re.match(r'^[^a-zA-Z]*$', text):  # Check if text has no alphabetic characters\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# 3a) Function to get langdetect prediction\n",
    "def get_langdetect_prediction(text):\n",
    "    try:\n",
    "        # Directly use text without preprocessing for efficiency\n",
    "        if len(text) < 10 or is_non_language_text(text):\n",
    "            return \"unknown\"\n",
    "        lang = langdetect_detect(text)\n",
    "        return lang\n",
    "    except LangDetectException:\n",
    "        return \"unknown\"\n",
    "\n",
    "# 3b) Function to get langid prediction\n",
    "def get_langid_prediction(text):\n",
    "    try:\n",
    "        lang, _ = langid_classify(text)\n",
    "        if len(text) < 10 or is_non_language_text(text):\n",
    "            return \"unknown\"\n",
    "        return lang\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "# 4) Function to calculate majority vote for each language\n",
    "def calculate_majority_vote(predictions):\n",
    "    vote_counts = {}\n",
    "    for lang in predictions:\n",
    "        if lang in vote_counts:\n",
    "            vote_counts[lang] += 1\n",
    "        else:\n",
    "            vote_counts[lang] = 1\n",
    "    return vote_counts\n",
    "\n",
    "# 5) Parallel processing for efficiency with limited workers\n",
    "def parallel_detection(text):\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(executor.map(lambda func: func(text), \n",
    "                                    [get_langdetect_prediction, get_langid_prediction]))\n",
    "    return results\n",
    "\n",
    "# 6) Caching function for repeated inputs\n",
    "@lru_cache(maxsize=500)\n",
    "def get_cached_language(text):\n",
    "    return combined_language_detection(text)\n",
    "\n",
    "# 7) Combined majority voting language detection function\n",
    "def combined_language_detection(text):\n",
    "    # Check if the text is non-language (e.g., numbers, symbols only)\n",
    "    if is_non_language_text(text):\n",
    "        return \"unknown\"\n",
    "    \n",
    "    # Run the detectors in parallel for efficiency\n",
    "    predictions = parallel_detection(text)\n",
    "    \n",
    "    # Calculate majority vote for each language based on predictions\n",
    "    vote_counts = calculate_majority_vote(predictions)\n",
    "    \n",
    "    # Determine the language with the highest majority vote\n",
    "    final_language = max(vote_counts, key=vote_counts.get)\n",
    "    \n",
    "    # If \"unknown\" is the most common or if all detectors fail, return \"unknown\"\n",
    "    if final_language == \"unknown\" or vote_counts[final_language] <= 1:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    return final_language\n",
    "\n",
    "# 8) Apply the cached function to each text in the DataFrame with a progress bar\n",
    "data_raw['language'] = [get_cached_language(text) for text in tqdm(data_raw['content'], desc=\"Language Detection\")]\n",
    "\n",
    "# 9) Display the DataFrame with detected languages\n",
    "data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Russia Practices 'Massive Nuclear Strike' in A...</td>\n",
       "      <td>“Important to have modern and constantly ready...</td>\n",
       "      <td>30/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Nolte: Trump and Republican Senate Challenger ...</td>\n",
       "      <td>The final Insider Advantage poll out of Wiscon...</td>\n",
       "      <td>29/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Sen. Tom Cotton Barnstorms Nation Campaigning ...</td>\n",
       "      <td>Sen. Tom Cotton (R-AR) is putting in long hour...</td>\n",
       "      <td>29/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>U.S. Marines Successfully Test Israel's Iron D...</td>\n",
       "      <td>The U.S. Marines have successfully tested a mo...</td>\n",
       "      <td>30/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Seoul, Japan Warn North Korea Preparing Nuclea...</td>\n",
       "      <td>The South Korean Defense Intelligence Agency (...</td>\n",
       "      <td>30/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>How can the candidate with most votes lose? Th...</td>\n",
       "      <td>Even though the United States touts its status...</td>\n",
       "      <td>19/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>The RBA will likely hold interest rates – but ...</td>\n",
       "      <td>When the Reserve Bank board meet next week to ...</td>\n",
       "      <td>30/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>Trump’s mass deportation plan would be ‘econom...</td>\n",
       "      <td>If elected, Donald Trump plans to carry out “t...</td>\n",
       "      <td>30/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>Sun belt to October surprise: US election term...</td>\n",
       "      <td>On Tuesday 5 November, Americans will vote aft...</td>\n",
       "      <td>10/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>I don’t expect stone-cold truths from a chatsh...</td>\n",
       "      <td>In a development that absolutely must not catc...</td>\n",
       "      <td>29/10/2024</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>954 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source                                              title  \\\n",
       "0    Breitbart  Russia Practices 'Massive Nuclear Strike' in A...   \n",
       "1    Breitbart  Nolte: Trump and Republican Senate Challenger ...   \n",
       "2    Breitbart  Sen. Tom Cotton Barnstorms Nation Campaigning ...   \n",
       "3    Breitbart  U.S. Marines Successfully Test Israel's Iron D...   \n",
       "4    Breitbart  Seoul, Japan Warn North Korea Preparing Nuclea...   \n",
       "..         ...                                                ...   \n",
       "949   Guardian  How can the candidate with most votes lose? Th...   \n",
       "950   Guardian  The RBA will likely hold interest rates – but ...   \n",
       "951   Guardian  Trump’s mass deportation plan would be ‘econom...   \n",
       "952   Guardian  Sun belt to October surprise: US election term...   \n",
       "953   Guardian  I don’t expect stone-cold truths from a chatsh...   \n",
       "\n",
       "                                               content        date language  \n",
       "0    “Important to have modern and constantly ready...  30/10/2024       en  \n",
       "1    The final Insider Advantage poll out of Wiscon...  29/10/2024       en  \n",
       "2    Sen. Tom Cotton (R-AR) is putting in long hour...  29/10/2024       en  \n",
       "3    The U.S. Marines have successfully tested a mo...  30/10/2024       en  \n",
       "4    The South Korean Defense Intelligence Agency (...  30/10/2024       en  \n",
       "..                                                 ...         ...      ...  \n",
       "949  Even though the United States touts its status...  19/10/2024       en  \n",
       "950  When the Reserve Bank board meet next week to ...  30/10/2024       en  \n",
       "951  If elected, Donald Trump plans to carry out “t...  30/10/2024       en  \n",
       "952  On Tuesday 5 November, Americans will vote aft...  10/10/2024       en  \n",
       "953  In a development that absolutely must not catc...  29/10/2024       en  \n",
       "\n",
       "[954 rows x 5 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows where language is NOT in english and reset the index\n",
    "data_raw = data_raw[data_raw['language'] == 'en'].reset_index(drop=True)\n",
    "data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Russia Practices 'Massive Nuclear Strike' in A...</td>\n",
       "      <td>“Important to have modern and constantly ready...</td>\n",
       "      <td>30/10/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Nolte: Trump and Republican Senate Challenger ...</td>\n",
       "      <td>The final Insider Advantage poll out of Wiscon...</td>\n",
       "      <td>29/10/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Sen. Tom Cotton Barnstorms Nation Campaigning ...</td>\n",
       "      <td>Sen. Tom Cotton (R-AR) is putting in long hour...</td>\n",
       "      <td>29/10/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>U.S. Marines Successfully Test Israel's Iron D...</td>\n",
       "      <td>The U.S. Marines have successfully tested a mo...</td>\n",
       "      <td>30/10/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Seoul, Japan Warn North Korea Preparing Nuclea...</td>\n",
       "      <td>The South Korean Defense Intelligence Agency (...</td>\n",
       "      <td>30/10/2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source                                              title  \\\n",
       "0  Breitbart  Russia Practices 'Massive Nuclear Strike' in A...   \n",
       "1  Breitbart  Nolte: Trump and Republican Senate Challenger ...   \n",
       "2  Breitbart  Sen. Tom Cotton Barnstorms Nation Campaigning ...   \n",
       "3  Breitbart  U.S. Marines Successfully Test Israel's Iron D...   \n",
       "4  Breitbart  Seoul, Japan Warn North Korea Preparing Nuclea...   \n",
       "\n",
       "                                             content        date  \n",
       "0  “Important to have modern and constantly ready...  30/10/2024  \n",
       "1  The final Insider Advantage poll out of Wiscon...  29/10/2024  \n",
       "2  Sen. Tom Cotton (R-AR) is putting in long hour...  29/10/2024  \n",
       "3  The U.S. Marines have successfully tested a mo...  30/10/2024  \n",
       "4  The South Korean Defense Intelligence Agency (...  30/10/2024  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw['language'].value_counts()\n",
    "\n",
    "# Drop the  'language' column\n",
    "data_raw = data_raw.drop(columns=['language'])\n",
    "\n",
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles before filtering: 954\n",
      "Number of articles after political keyword filtering: 917\n",
      "Number of articles after opinion piece filtering: 818\n",
      "Number of articles after irrelevant content cleaning: 818\n",
      "Number of articles after all filtering: 818\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 818 entries, 0 to 952\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   source   818 non-null    object\n",
      " 1   title    818 non-null    object\n",
      " 2   content  818 non-null    object\n",
      " 3   date     818 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 32.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#  Copy data for safety\n",
    "data_filtered = data_raw.copy()\n",
    "print(\"Number of articles before filtering:\", len(data_filtered))\n",
    "\n",
    "# Step 2: Political Keywords Filter\n",
    "us_politics_keywords = [\n",
    "    # Core Keywords\n",
    "    \"election\", \"elections\", \"2024 election\", \"presidential election\", \"campaign\", \"campaigning\",\n",
    "    \"primary\", \"primaries\", \"polling\", \"polls\",\n",
    "\n",
    "    # Key Political Figures\n",
    "    \"Kamala Harris\", \"Donald Trump\", \"Joe Biden\", \"Ron DeSantis\", \"Gavin Newsom\", \n",
    "    \"Mike Pence\", \"Vivek Ramaswamy\", \"Tim Walz\"\n",
    "\n",
    "    # Political Parties & Groups\n",
    "    \"Democrat\", \"Democratic Party\", \"Republican\", \"GOP\", \"Republican Party\",\n",
    "    \"Independent\", \"Third party\", \"PAC\", \"Super PAC\",\n",
    "\n",
    "    # Political Issues & Controversies\n",
    "    \"voting rights\", \"voter suppression\", \"absentee ballot\", \"mail-in ballot\",\n",
    "    \"Electoral College\", \"Supreme Court\", \"abortion\", \"Roe v. Wade\", \"gun control\",\n",
    "    \"Second Amendment\", \"immigration\", \"border security\", \"healthcare\", \n",
    "    \"Medicare\", \"Affordable Care Act\", \"climate change\", \"Green New Deal\", \n",
    "    \"inflation\", \"economic policy\", \"tax cuts\", \"tax reform\", \"foreign policy\", \n",
    "    \"foreign relations\",\n",
    "\n",
    "    # U.S. Government Bodies & Offices\n",
    "    \"Congress\", \"Senate\", \"Senators\", \"House of Representatives\", \"White House\",\n",
    "    \"Supreme Court\", \"Federal government\", \"State government\",\n",
    "\n",
    "    # Policies & Bills\n",
    "    \"voting reform\", \"healthcare reform\", \"climate policy\", \"gun legislation\", \n",
    "    \"economic recovery\", \"infrastructure bill\", \"Social Security\", \"student loan forgiveness\",\n",
    "\n",
    "    # Social & Cultural Issues\n",
    "    \"social justice\", \"racial equality\", \"police reform\", \"civil rights\",\n",
    "    \"freedom of speech\", \"religious freedom\",\n",
    "\n",
    "    # Election Processes\n",
    "    \"debates\", \"presidential debate\", \"swing state\", \"battleground state\",\n",
    "    \"electoral votes\",\n",
    "\n",
    "    # Additional Relevant Terms\n",
    "    \"approval rating\", \"national convention\", \"lobbying\", \"lobbyist\",\n",
    "    \"scandal\", \"investigation\", \"political rally\", \"rally\"\n",
    "]\n",
    "data_filtered = data_filtered[data_filtered['content'].str.contains('|'.join(us_politics_keywords), case=False, na=False)]\n",
    "print(\"Number of articles after political keyword filtering:\", len(data_filtered))\n",
    "\n",
    "# Step 3: Exclude Opinion Pieces (assuming titles or content contain specific indicators of opinion)\n",
    "opinion_keywords = ['opinion', 'editorial', 'op-ed']\n",
    "data_filtered = data_filtered[~data_filtered['title'].str.contains('|'.join(opinion_keywords), case=False, na=False)]\n",
    "data_filtered = data_filtered[~data_filtered['content'].str.contains('|'.join(opinion_keywords), case=False, na=False)]\n",
    "print(\"Number of articles after opinion piece filtering:\", len(data_filtered))\n",
    "\n",
    "# Step 4: Remove Irrelevant Content Using Regex\n",
    "# Define patterns for irrelevant content\n",
    "irrelevant_patterns = [\n",
    "    r'All rights reserved', r'Read more', r'For more information', r'Follow us', \n",
    "    r'Find us on', r'Contact the author', r'Subscribe for updates', r'\\bWATCH\\b',\n",
    "    r'Advertisement', r'^[\\W_]+$'  # Removes lines that are mostly symbols or whitespace\n",
    "]\n",
    "\n",
    "def clean_irrelevant_content(text):\n",
    "    for pattern in irrelevant_patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "data_filtered['content'] = data_filtered['content'].apply(clean_irrelevant_content)\n",
    "print(\"Number of articles after irrelevant content cleaning:\", len(data_filtered))\n",
    "\n",
    "# Final DataFrame after filtering\n",
    "print(\"Number of articles after all filtering:\", len(data_filtered))\n",
    "data_filtered.head()\n",
    "data_filtered.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "The Daily Caller    94\n",
       "Breitbart           91\n",
       "CNN                 85\n",
       "AP                  85\n",
       "News Max            83\n",
       "Zerohedge           83\n",
       "Guardian            82\n",
       "Natural News        78\n",
       "NPR                 77\n",
       "BBC                 60\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "Here we select the relevant features for fake news classification.\n",
    "- `title`, `text`\n",
    "- Create a new DataFrame (`data`) by selecting the specifc columns mentioned above from the original DataFrame `data_raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                               title  \\\n",
      "0  Russia Practices 'Massive Nuclear Strike' in A...   \n",
      "1  Nolte: Trump and Republican Senate Challenger ...   \n",
      "2  Sen. Tom Cotton Barnstorms Nation Campaigning ...   \n",
      "3  U.S. Marines Successfully Test Israel's Iron D...   \n",
      "4  Seoul, Japan Warn North Korea Preparing Nuclea...   \n",
      "\n",
      "                                             content     source  \n",
      "0  “Important to have modern and constantly ready...  Breitbart  \n",
      "1  The final Insider Advantage poll out of Wiscon...  Breitbart  \n",
      "2  Sen. Tom Cotton (R-AR) is putting in long hour...  Breitbart  \n",
      "3  The U.S. Marines have successfully tested a mo...  Breitbart  \n",
      "4  The South Korean Defense Intelligence Agency (...  Breitbart  \n",
      "\n",
      "The old shape is:  (818, 3)\n"
     ]
    }
   ],
   "source": [
    "data = data_filtered[['title', 'content', 'source',]]\n",
    "print(type(data))\n",
    "print(data.head())\n",
    "\n",
    "# Shape before dropping duplicates\n",
    "print(\"\\nThe old shape is: \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Remove Duplicate Rows\n",
    "- Drop duplicate rows from the dataframe (`data`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new shape is:  (818, 3)\n"
     ]
    }
   ],
   "source": [
    "data_ = data.drop_duplicates()\n",
    "\n",
    "# Display the new dataframe shape\n",
    "print(\"The new shape is: \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Outliers\n",
    "\n",
    "### `text`\n",
    "\n",
    "The `text` column of `data`, which is of string type, may contain values with unusually long lengths, indicating the presence of outliers. We will identify the outliers using [Z-score method].\n",
    "\n",
    "1. Create a new column `text_length` in the DataFrame `data` by calculating the length of each review. (Set the value as 0 if the correponding `text` column has NaN values.)\n",
    "\n",
    "2. Check the statistics of `text_length` using `describe()` method.\n",
    "\n",
    "3. Calculate the mean and standard deviation of the `text_length` column.\n",
    "\n",
    "4. Set the Z-score threshold for identifying outliers to 3.\n",
    "\n",
    "5. Identify outliers of the `text_length` column and set the corresponding `text` to np.nan.\n",
    "\n",
    "6. Drop the `text_length` column from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Russia Practices 'Massive Nuclear Strike' in A...   \n",
      "1  Nolte: Trump and Republican Senate Challenger ...   \n",
      "2  Sen. Tom Cotton Barnstorms Nation Campaigning ...   \n",
      "\n",
      "                                             content     source  text_length  \n",
      "0  “Important to have modern and constantly ready...  Breitbart         3755  \n",
      "1  The final Insider Advantage poll out of Wiscon...  Breitbart         2713  \n",
      "2  Sen. Tom Cotton (R-AR) is putting in long hour...  Breitbart         4010  \n",
      "count      818.000000\n",
      "mean      4741.350856\n",
      "std       3330.378252\n",
      "min        115.000000\n",
      "25%       2647.750000\n",
      "50%       3763.000000\n",
      "75%       6180.500000\n",
      "max      24327.000000\n",
      "Name: text_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data['text_length'] = data['content'].apply(lambda x: len(x) if pd.notna(x) else 0)\n",
    "print(data.head(3))\n",
    "\n",
    "TL = data[\"text_length\"]\n",
    "stats_TL = TL.describe()\n",
    "print(stats_TL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Russia Practices 'Massive Nuclear Strike' in A...</td>\n",
       "      <td>“Important to have modern and constantly ready...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nolte: Trump and Republican Senate Challenger ...</td>\n",
       "      <td>The final Insider Advantage poll out of Wiscon...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sen. Tom Cotton Barnstorms Nation Campaigning ...</td>\n",
       "      <td>Sen. Tom Cotton (R-AR) is putting in long hour...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U.S. Marines Successfully Test Israel's Iron D...</td>\n",
       "      <td>The U.S. Marines have successfully tested a mo...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Seoul, Japan Warn North Korea Preparing Nuclea...</td>\n",
       "      <td>The South Korean Defense Intelligence Agency (...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Russia Practices 'Massive Nuclear Strike' in A...   \n",
       "1  Nolte: Trump and Republican Senate Challenger ...   \n",
       "2  Sen. Tom Cotton Barnstorms Nation Campaigning ...   \n",
       "3  U.S. Marines Successfully Test Israel's Iron D...   \n",
       "4  Seoul, Japan Warn North Korea Preparing Nuclea...   \n",
       "\n",
       "                                             content     source  \n",
       "0  “Important to have modern and constantly ready...  Breitbart  \n",
       "1  The final Insider Advantage poll out of Wiscon...  Breitbart  \n",
       "2  Sen. Tom Cotton (R-AR) is putting in long hour...  Breitbart  \n",
       "3  The U.S. Marines have successfully tested a mo...  Breitbart  \n",
       "4  The South Korean Defense Intelligence Agency (...  Breitbart  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_TL = TL.mean()\n",
    "# print(mean_TL)\n",
    "\n",
    "sd_TL = TL.std()\n",
    "# print(sd_TL)\n",
    "\n",
    "threshold = 3\n",
    "\n",
    "z_score = zscore(TL)\n",
    "# print(z_score)\n",
    "\n",
    "# Remove 'text' of lengths that are greater than 3 standard deviations above the mean\n",
    "data.loc[abs(z_score) > threshold, 'content'] = np.nan\n",
    "# print(data.head(3))\n",
    "\n",
    "data = data.drop(\"text_length\", axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `title`\n",
    "\n",
    "Similarly, the `title` column of `data` (of type `str`) may also contain values with unusually long lengths, indicating the presence of outliers.\n",
    "\n",
    "1. Create a new column `title_length` in the DataFrame `data` by calculating the length of each price value. (Set the value as 0 if the correponding `title` column has NaN values.)\n",
    "\n",
    "2. Check the statistics of `title_length` using `describe()` method and display its unique values.\n",
    "\n",
    "3. Identify the outlier values by inspecting the content in `title` corresponding to the abnormal value in `title_length` and set the corresponding value of `title` to np.nan.\n",
    "\n",
    "4. Drop the `title_length` column from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Russia Practices 'Massive Nuclear Strike' in A...   \n",
      "1  Nolte: Trump and Republican Senate Challenger ...   \n",
      "2  Sen. Tom Cotton Barnstorms Nation Campaigning ...   \n",
      "\n",
      "                                             content     source  title_length  \n",
      "0  “Important to have modern and constantly ready...  Breitbart            72  \n",
      "1  The final Insider Advantage poll out of Wiscon...  Breitbart            63  \n",
      "2  Sen. Tom Cotton (R-AR) is putting in long hour...  Breitbart            89  \n",
      "count    818.000000\n",
      "mean      79.930318\n",
      "std       23.854029\n",
      "min        9.000000\n",
      "25%       63.000000\n",
      "50%       79.000000\n",
      "75%       94.000000\n",
      "max      186.000000\n",
      "Name: title_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data['title_length'] = data['title'].apply(lambda x: len(x) if pd.notna(x) else 0)\n",
    "print(data.head(3))\n",
    "\n",
    "TL = data[\"title_length\"]\n",
    "stats_TL = TL.describe()\n",
    "print(stats_TL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Russia Practices 'Massive Nuclear Strike' in A...</td>\n",
       "      <td>“Important to have modern and constantly ready...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nolte: Trump and Republican Senate Challenger ...</td>\n",
       "      <td>The final Insider Advantage poll out of Wiscon...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sen. Tom Cotton Barnstorms Nation Campaigning ...</td>\n",
       "      <td>Sen. Tom Cotton (R-AR) is putting in long hour...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U.S. Marines Successfully Test Israel's Iron D...</td>\n",
       "      <td>The U.S. Marines have successfully tested a mo...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Seoul, Japan Warn North Korea Preparing Nuclea...</td>\n",
       "      <td>The South Korean Defense Intelligence Agency (...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Russia Practices 'Massive Nuclear Strike' in A...   \n",
       "1  Nolte: Trump and Republican Senate Challenger ...   \n",
       "2  Sen. Tom Cotton Barnstorms Nation Campaigning ...   \n",
       "3  U.S. Marines Successfully Test Israel's Iron D...   \n",
       "4  Seoul, Japan Warn North Korea Preparing Nuclea...   \n",
       "\n",
       "                                             content     source  \n",
       "0  “Important to have modern and constantly ready...  Breitbart  \n",
       "1  The final Insider Advantage poll out of Wiscon...  Breitbart  \n",
       "2  Sen. Tom Cotton (R-AR) is putting in long hour...  Breitbart  \n",
       "3  The U.S. Marines have successfully tested a mo...  Breitbart  \n",
       "4  The South Korean Defense Intelligence Agency (...  Breitbart  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_TL = TL.mean()\n",
    "# print(mean_TL)\n",
    "\n",
    "sd_TL = TL.std()\n",
    "# print(sd_TL)\n",
    "\n",
    "threshold = 3\n",
    "\n",
    "z_score = zscore(TL)\n",
    "# print(z_score)\n",
    "\n",
    "# Remove 'title' of lengths that are greater than 3 standard deviations above the mean\n",
    "data.loc[abs(z_score) > threshold, 'title'] = np.nan\n",
    "# print(data.head(3))\n",
    "\n",
    "data = data.drop(\"title_length\", axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title       5\n",
       "content    15\n",
       "source      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "### Create new column `full_review`\n",
    "Since there are some rows with empty `text` and `title`, we will concatenate both columns (`text` and `title`) to form a new column `full_content`.\n",
    "1. Replace `NaN` values in `text` and `title` with an empty string.\n",
    "\n",
    "2. Combine `text` and `title` into `full_content`.\n",
    "\n",
    "3. Strip any leading/trailing whitespaces in `full_content`.\n",
    "\n",
    "4. Drop `text` and `title` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      source                                       full_content\n",
      "0  Breitbart  “Important to have modern and constantly ready...\n",
      "1  Breitbart  The final Insider Advantage poll out of Wiscon...\n",
      "2  Breitbart  Sen. Tom Cotton (R-AR) is putting in long hour...\n",
      "3  Breitbart  The U.S. Marines have successfully tested a mo...\n",
      "4  Breitbart  The South Korean Defense Intelligence Agency (...\n",
      "\n",
      "The old shape is: (818, 2)\n"
     ]
    }
   ],
   "source": [
    "# 1) Fill NaN values in 'text' and 'title' with an empty string\n",
    "data['title'] = data['title'].fillna('')\n",
    "data['content'] = data['content'].fillna('')\n",
    "\n",
    "# 2) Combine 'text' and 'title' into 'content'\n",
    "data['full_content'] = data['content'] + \" \" + data['title']\n",
    "\n",
    "# 3) Strip any leading/trailing whitespace\n",
    "data['full_content'] = data['full_content'].str.strip()\n",
    "\n",
    "# 4) Drop `text` and `title` columns\n",
    "data = data.drop(columns = ['content', 'title'])\n",
    "\n",
    "# Check if the 'full_review' column was added and if 'text' and 'title' columns has been dropped\n",
    "print(data.head())\n",
    "print(\"\\nThe old shape is:\",data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Values\n",
    "1. Drop rows where `full_review` are empty strings and reset the index.\n",
    "\n",
    "2. Check if there are no more null values in `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new shape is: (818, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "source          0\n",
       "full_content    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Drop rows where `full_review` are empty strings and reset the index\n",
    "data = data[data['full_content'] != \"\"].reset_index(drop=True)\n",
    "print(\"The new shape is:\",data.shape)\n",
    "\n",
    "# 2) Check if there are no more null values in `data`\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>full_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>“Important to have modern and constantly ready...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>The final Insider Advantage poll out of Wiscon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Sen. Tom Cotton (R-AR) is putting in long hour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>The U.S. Marines have successfully tested a mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>The South Korean Defense Intelligence Agency (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>A California “home” for rent is shining a ligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>Even though the United States touts its status...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>When the Reserve Bank board meet next week to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>If elected, Donald Trump plans to carry out “t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>On Tuesday 5 November, Americans will vote aft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>818 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source                                       full_content\n",
       "0    Breitbart  “Important to have modern and constantly ready...\n",
       "1    Breitbart  The final Insider Advantage poll out of Wiscon...\n",
       "2    Breitbart  Sen. Tom Cotton (R-AR) is putting in long hour...\n",
       "3    Breitbart  The U.S. Marines have successfully tested a mo...\n",
       "4    Breitbart  The South Korean Defense Intelligence Agency (...\n",
       "..         ...                                                ...\n",
       "813   Guardian  A California “home” for rent is shining a ligh...\n",
       "814   Guardian  Even though the United States touts its status...\n",
       "815   Guardian  When the Reserve Bank board meet next week to ...\n",
       "816   Guardian  If elected, Donald Trump plans to carry out “t...\n",
       "817   Guardian  On Tuesday 5 November, Americans will vote aft...\n",
       "\n",
       "[818 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing for NLP\n",
    "\n",
    "Here we will define a function `process_full_review` that takes a textual value as input and applies the following processing steps in sequence:\n",
    "\n",
    "1. Convert the input text to lowercase using the `lower()` function.\n",
    "\n",
    "2. Tokenize the lowercase text using the `word_tokenize` function from the NLTK library.\n",
    "\n",
    "3. Create a list (`alphabetic_tokens`) containing only alphanetic tokens using a list comprehension with a regular expression match.\n",
    "\n",
    "4. Remove stopwords\n",
    "-   Obtain a set of English stopwords using the `stopwords.words('english')` method.\n",
    "-   Define a list of `allowed_words` that should not be removed.\n",
    "-   Remove the stopwords (excluding those that should not be removed).\n",
    "\n",
    "5. Apply stemming to each token in the list (`lemmatized_words`) using the `lemmatize` method.\n",
    "\n",
    "6. Join the stemmed tokens into a single processed text using the `join` method and return the processed text.\n",
    "\n",
    "Create  new columns (`processed_full_review`) in `data` by applying the `process_full_review` function to the `full_review` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure require NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to process text\n",
    "import string\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "def process_full_review(text):\n",
    "    # Convert to lowercase and tokenize\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    stemmer = PorterStemmer()\n",
    "    # List of stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    allowed_words = [\"no\", \"not\", \"don't\", \"dont\", \"don\", \"but\", \n",
    "                     \"however\", \"never\", \"wasn't\", \"wasnt\", \"shouldn't\",\n",
    "                     \"shouldnt\", \"mustn't\", \"musnt\"]\n",
    "\n",
    "    stemmed = [stemmer.stem(word) for word in tokens if word not in stop_words or word in allowed_words]\n",
    "    return ' '.join(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">The code below will take approximately 16 minutes to run!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing News Articles: 100%|██████████| 818/818 [00:09<00:00, 84.53it/s] \n"
     ]
    }
   ],
   "source": [
    "# Enable tqdm for pandas (progress bar)\n",
    "tqdm.pandas(desc=\"Processing News Articles\")\n",
    "\n",
    "# Apply process_full_review function with tqdm progress bar and expand the results into separate columns.\n",
    "processed_columns = ['processed_full_content']\n",
    "data[processed_columns] = data['full_content'].progress_apply(lambda x: pd.Series(process_full_review(x)))\n",
    "\n",
    "data\n",
    "\n",
    "data_copy = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "The Daily Caller    94\n",
       "Breitbart           91\n",
       "CNN                 85\n",
       "AP                  85\n",
       "News Max            83\n",
       "Zerohedge           83\n",
       "Guardian            82\n",
       "Natural News        78\n",
       "NPR                 77\n",
       "BBC                 60\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['source'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles after sampling: 600\n",
      "source\n",
      "AP                  60\n",
      "BBC                 60\n",
      "Breitbart           60\n",
      "CNN                 60\n",
      "Guardian            60\n",
      "NPR                 60\n",
      "Natural News        60\n",
      "News Max            60\n",
      "The Daily Caller    60\n",
      "Zerohedge           60\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17876\\1002029283.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=sample_size, random_state=42))\n"
     ]
    }
   ],
   "source": [
    "# Sample 61 articles from each source without including the grouping column in the operation\n",
    "sample_size = 60\n",
    "\n",
    "data_sampled = (\n",
    "    data.groupby('source', group_keys=False)\n",
    "    .apply(lambda x: x.sample(n=sample_size, random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of articles after sampling:\", len(data_sampled))\n",
    "print(data_sampled['source'].value_counts())  # Check the counts per source\n",
    "data_sampled.head()\n",
    "\n",
    "data_sampled = data_sampled.drop(columns=['full_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>processed_full_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AP</td>\n",
       "      <td>abort law motiv women north carolina vote elec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AP</td>\n",
       "      <td>vote us elect step must take vote us elect ste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP</td>\n",
       "      <td>voter drown ad ‘ obscen ’ amount cash flood mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP</td>\n",
       "      <td>don ’ count recount chang winner close elect f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP</td>\n",
       "      <td>battleground georgia poor peopl see no reason ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>mani peopl might think attorney gener ken paxt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>latest vivid demonstr once-dur democrat consti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>former vatican roman cathol archbishop carlo m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>author sam dorman via epoch time emphasi steve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>former unit state presid donald trump ’ lead t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source                             processed_full_content\n",
       "0           AP  abort law motiv women north carolina vote elec...\n",
       "1           AP  vote us elect step must take vote us elect ste...\n",
       "2           AP  voter drown ad ‘ obscen ’ amount cash flood mo...\n",
       "3           AP  don ’ count recount chang winner close elect f...\n",
       "4           AP  battleground georgia poor peopl see no reason ...\n",
       "..         ...                                                ...\n",
       "595  Zerohedge  mani peopl might think attorney gener ken paxt...\n",
       "596  Zerohedge  latest vivid demonstr once-dur democrat consti...\n",
       "597  Zerohedge  former vatican roman cathol archbishop carlo m...\n",
       "598  Zerohedge  author sam dorman via epoch time emphasi steve...\n",
       "599  Zerohedge  former unit state presid donald trump ’ lead t...\n",
       "\n",
       "[600 rows x 2 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying our best model (LSTM + GloVe 300D) on the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../processed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "400/400 [==============================] - 20s 47ms/step - loss: 0.2032 - accuracy: 0.9276 - val_loss: 0.1412 - val_accuracy: 0.9517\n",
      "Epoch 2/10\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.0931 - accuracy: 0.9729 - val_loss: 0.1234 - val_accuracy: 0.9625\n",
      "Epoch 3/10\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.0609 - accuracy: 0.9848 - val_loss: 0.1154 - val_accuracy: 0.9638\n",
      "Epoch 4/10\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.0399 - accuracy: 0.9922 - val_loss: 0.1281 - val_accuracy: 0.9603\n",
      "Epoch 5/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0318 - accuracy: 0.9951 - val_loss: 0.1252 - val_accuracy: 0.9644\n",
      "Epoch 6/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0271 - accuracy: 0.9965 - val_loss: 0.1309 - val_accuracy: 0.9625\n",
      "400/400 [==============================] - 2s 4ms/step\n",
      "400/400 [==============================] - 2s 4ms/step\n",
      "Fold Accuracy: 0.963827121829001, Precision: 0.9644607843137255, Recall: 0.9550970873786407, F1 Score: 0.9597560975609756, ROC AUC: 0.9927637818017205\n",
      "Epoch 1/10\n",
      "400/400 [==============================] - 21s 48ms/step - loss: 0.2115 - accuracy: 0.9253 - val_loss: 0.1235 - val_accuracy: 0.9602\n",
      "Epoch 2/10\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 0.0964 - accuracy: 0.9725 - val_loss: 0.1104 - val_accuracy: 0.9650\n",
      "Epoch 3/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0605 - accuracy: 0.9858 - val_loss: 0.1218 - val_accuracy: 0.9652\n",
      "Epoch 4/10\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 0.0411 - accuracy: 0.9920 - val_loss: 0.1173 - val_accuracy: 0.9662\n",
      "Epoch 5/10\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 0.0325 - accuracy: 0.9945 - val_loss: 0.1270 - val_accuracy: 0.9639\n",
      "400/400 [==============================] - 2s 5ms/step\n",
      "400/400 [==============================] - 2s 4ms/step\n",
      "Fold Accuracy: 0.9650015659254619, Precision: 0.9573026092849881, Recall: 0.9666381522668948, F1 Score: 0.9619477313356601, ROC AUC: 0.9938413218727427\n",
      "Epoch 1/10\n",
      "400/400 [==============================] - 20s 47ms/step - loss: 0.2033 - accuracy: 0.9265 - val_loss: 0.1326 - val_accuracy: 0.9566\n",
      "Epoch 2/10\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.0938 - accuracy: 0.9729 - val_loss: 0.1216 - val_accuracy: 0.9634\n",
      "Epoch 3/10\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.0589 - accuracy: 0.9854 - val_loss: 0.1413 - val_accuracy: 0.9612\n",
      "Epoch 4/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0406 - accuracy: 0.9920 - val_loss: 0.1115 - val_accuracy: 0.9659\n",
      "Epoch 5/10\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.0328 - accuracy: 0.9941 - val_loss: 0.1536 - val_accuracy: 0.9570\n",
      "Epoch 6/10\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.0246 - accuracy: 0.9967 - val_loss: 0.1277 - val_accuracy: 0.9666\n",
      "Epoch 7/10\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.0220 - accuracy: 0.9973 - val_loss: 0.1418 - val_accuracy: 0.9609\n",
      "400/400 [==============================] - 2s 4ms/step\n",
      "400/400 [==============================] - 2s 4ms/step\n",
      "Fold Accuracy: 0.9658628249295333, Precision: 0.9608454608454609, Recall: 0.9635163307852675, F1 Score: 0.9621790423317141, ROC AUC: 0.9935927747398381\n",
      "Epoch 1/10\n",
      "400/400 [==============================] - 20s 47ms/step - loss: 0.2119 - accuracy: 0.9239 - val_loss: 0.1258 - val_accuracy: 0.9583\n",
      "Epoch 2/10\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.0952 - accuracy: 0.9733 - val_loss: 0.1091 - val_accuracy: 0.9648\n",
      "Epoch 3/10\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.0616 - accuracy: 0.9852 - val_loss: 0.1130 - val_accuracy: 0.9655\n",
      "Epoch 4/10\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.0400 - accuracy: 0.9925 - val_loss: 0.1255 - val_accuracy: 0.9653\n",
      "Epoch 5/10\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.0334 - accuracy: 0.9938 - val_loss: 0.1291 - val_accuracy: 0.9644\n",
      "400/400 [==============================] - 2s 4ms/step\n",
      "400/400 [==============================] - 2s 4ms/step\n",
      "Fold Accuracy: 0.9648449733792671, Precision: 0.9667186687467498, Recall: 0.9559478916695234, F1 Score: 0.9613031112643282, ROC AUC: 0.9944265645677227\n",
      "Epoch 1/10\n",
      "400/400 [==============================] - 20s 47ms/step - loss: 0.2065 - accuracy: 0.9250 - val_loss: 0.1227 - val_accuracy: 0.9616\n",
      "Epoch 2/10\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.0943 - accuracy: 0.9728 - val_loss: 0.1013 - val_accuracy: 0.9681\n",
      "Epoch 3/10\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.0594 - accuracy: 0.9853 - val_loss: 0.1154 - val_accuracy: 0.9650\n",
      "Epoch 4/10\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.0409 - accuracy: 0.9919 - val_loss: 0.1103 - val_accuracy: 0.9680\n",
      "Epoch 5/10\n",
      "400/400 [==============================] - 18s 46ms/step - loss: 0.0329 - accuracy: 0.9945 - val_loss: 0.1136 - val_accuracy: 0.9666\n",
      "400/400 [==============================] - 2s 4ms/step\n",
      "400/400 [==============================] - 2s 4ms/step\n",
      "Fold Accuracy: 0.9681334168493579, Precision: 0.9676867840656522, Recall: 0.9629125552909152, F1 Score: 0.9652937665217021, ROC AUC: 0.9953260292221697\n",
      "\n",
      "Average Metrics Across Folds:\n",
      "Accuracy: 0.9655\n",
      "Precision: 0.9634\n",
      "Recall: 0.9608\n",
      "F1: 0.9621\n",
      "Roc_auc: 0.9940\n",
      "Final Average Metrics: {'accuracy': 0.9655339805825243, 'precision': 0.9634028614513153, 'recall': 0.9608224034782484, 'f1': 0.962095949802876, 'roc_auc': 0.9939900944408387}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "# Load and process GloVe embeddings\n",
    "def load_glove_embeddings(glove_file, word_index, embedding_dim=300):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefficients = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefficients\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "# Define LSTM model with Dropout and L2 regularization\n",
    "def create_lstm_model(vocab_size, embedding_matrix, input_length, learning_rate=0.001, l2_lambda=0.01):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_matrix.shape[1],\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=input_length,\n",
    "                        trainable=True))\n",
    "    model.add(LSTM(units=64, return_sequences=False, dropout=0.2))\n",
    "    model.add(Dropout(0.2))  # Added Dropout layer here\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(l2_lambda)))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # or 'val_accuracy' if you prefer to monitor accuracy\n",
    "    patience=3,          # stop after 3 epochs with no improvement\n",
    "    restore_best_weights=True  # revert to the best weights after stopping\n",
    ")\n",
    "# K-Fold Cross-Validation with additional metrics\n",
    "def k_fold_cross_validation(X, y, embedding_matrix, vocab_size, max_len, n_splits=5, batch_size=128):\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "\n",
    "    # Define early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',  # monitor validation loss\n",
    "        patience=3,          # number of epochs with no improvement\n",
    "        restore_best_weights=True  # revert to the best model weights\n",
    "    )\n",
    "\n",
    "    for train_idx, val_idx in kfold.split(X):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model = create_lstm_model(vocab_size, embedding_matrix, max_len, learning_rate=0.001, l2_lambda=0.01)\n",
    "        \n",
    "        # Fit model with early stopping\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=10,             # maximum number of epochs\n",
    "            batch_size=128,\n",
    "            verbose=1,\n",
    "            callbacks=[early_stopping]  # add early stopping\n",
    "        )\n",
    "        \n",
    "        y_pred = (model.predict(X_val) > 0.5).astype(\"int32\")\n",
    "        y_pred_prob = model.predict(X_val).ravel()\n",
    "\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision = precision_score(y_val, y_pred)\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        roc_auc = roc_auc_score(y_val, y_pred_prob)\n",
    "\n",
    "        metrics['accuracy'].append(accuracy)\n",
    "        metrics['precision'].append(precision)\n",
    "        metrics['recall'].append(recall)\n",
    "        metrics['f1'].append(f1)\n",
    "        metrics['roc_auc'].append(roc_auc)\n",
    "\n",
    "        print(f\"Fold Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}, ROC AUC: {roc_auc}\")\n",
    "\n",
    "    avg_metrics = {metric: np.mean(scores) for metric, scores in metrics.items()}\n",
    "    print(\"\\nAverage Metrics Across Folds:\")\n",
    "    for metric, avg_score in avg_metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {avg_score:.4f}\")\n",
    "    \n",
    "    return avg_metrics, model\n",
    "\n",
    "# Example usage\n",
    "# Load your data and tokenize it\n",
    "texts = data[\"processed_full_content\"]\n",
    "num_samples = len(texts)\n",
    "labels = data[\"label\"].values  # Adjusted labels for each sample\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "X = tokenizer.texts_to_sequences(texts)\n",
    "max_len = 100\n",
    "X = pad_sequences(X, maxlen=max_len)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "embedding_dim = 300\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = load_glove_embeddings(\"../model_experiements/glove.6B.300d.txt\", tokenizer.word_index, embedding_dim)\n",
    "\n",
    "# Perform K-Fold cross-validation\n",
    "avg_metrics, trained_model = k_fold_cross_validation(X, labels, embedding_matrix, vocab_size, max_len)\n",
    "print(\"Final Average Metrics:\", avg_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "scraped_texts = data_sampled['processed_full_content']\n",
    "scraped_sequences = tokenizer.texts_to_sequences(scraped_texts)\n",
    "scraped_padded_sequences = pad_sequences(scraped_sequences, maxlen=max_len)\n",
    "\n",
    "# Get predictions from the trained model\n",
    "new_predictions = trained_model.predict(scraped_padded_sequences).ravel()\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "# For binary classification with sigmoid activation, threshold at 0.5\n",
    "predicted_classes = (new_predictions > 0.5).astype(\"int32\")\n",
    "data_sampled['predicted_label'] = predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>processed_full_content</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AP</td>\n",
       "      <td>abort law motiv women north carolina vote elec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AP</td>\n",
       "      <td>vote us elect step must take vote us elect ste...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP</td>\n",
       "      <td>voter drown ad ‘ obscen ’ amount cash flood mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP</td>\n",
       "      <td>don ’ count recount chang winner close elect f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP</td>\n",
       "      <td>battleground georgia poor peopl see no reason ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>mani peopl might think attorney gener ken paxt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>latest vivid demonstr once-dur democrat consti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>former vatican roman cathol archbishop carlo m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>author sam dorman via epoch time emphasi steve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>former unit state presid donald trump ’ lead t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source                             processed_full_content  \\\n",
       "0           AP  abort law motiv women north carolina vote elec...   \n",
       "1           AP  vote us elect step must take vote us elect ste...   \n",
       "2           AP  voter drown ad ‘ obscen ’ amount cash flood mo...   \n",
       "3           AP  don ’ count recount chang winner close elect f...   \n",
       "4           AP  battleground georgia poor peopl see no reason ...   \n",
       "..         ...                                                ...   \n",
       "595  Zerohedge  mani peopl might think attorney gener ken paxt...   \n",
       "596  Zerohedge  latest vivid demonstr once-dur democrat consti...   \n",
       "597  Zerohedge  former vatican roman cathol archbishop carlo m...   \n",
       "598  Zerohedge  author sam dorman via epoch time emphasi steve...   \n",
       "599  Zerohedge  former unit state presid donald trump ’ lead t...   \n",
       "\n",
       "     predicted_label  \n",
       "0                  0  \n",
       "1                  1  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "..               ...  \n",
       "595                1  \n",
       "596                0  \n",
       "597                1  \n",
       "598                0  \n",
       "599                1  \n",
       "\n",
       "[600 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             source  predicted_label\n",
      "0      Natural News               57\n",
      "1  The Daily Caller               39\n",
      "2         Zerohedge               39\n",
      "3          Guardian               31\n",
      "4          News Max               23\n",
      "5               CNN               21\n",
      "6               BBC               20\n",
      "7         Breitbart               15\n",
      "8               NPR               11\n",
      "9                AP               10\n"
     ]
    }
   ],
   "source": [
    "# Group by 'source' and calculate the sum of 'predicted_label' for each group\n",
    "grouped_data = data_sampled.groupby('source')['predicted_label'].sum()\n",
    "\n",
    "# Sort the grouped data in descending order based on the sum of 'predicted_label'\n",
    "sorted_data = grouped_data.sort_values(ascending=False)\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "sorted_data = sorted_data.reset_index()\n",
    "\n",
    "# Display the result\n",
    "print(sorted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "scraped_texts_test = data_copy['processed_full_content']\n",
    "scraped_sequences_test = tokenizer.texts_to_sequences(scraped_texts_test)\n",
    "scraped_padded_sequences_test = pad_sequences(scraped_sequences_test, maxlen=max_len)\n",
    "\n",
    "# Get predictions from the trained model\n",
    "new_predictions_test = trained_model.predict(scraped_padded_sequences_test).ravel()\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "# For binary classification with sigmoid activation, threshold at 0.5\n",
    "predicted_classes_test = (new_predictions_test > 0.5).astype(\"int32\")\n",
    "data_copy['predicted_label'] = predicted_classes_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             source  predicted_label_sum  row_count  predicted_label_ratio\n",
      "0      Natural News                   71         78               0.910256\n",
      "1  The Daily Caller                   66         94               0.702128\n",
      "2         Zerohedge                   56         83               0.674699\n",
      "3          Guardian                   44         82               0.536585\n",
      "4          News Max                   32         83               0.385542\n",
      "5               BBC                   20         60               0.333333\n",
      "6               CNN                   25         85               0.294118\n",
      "7         Breitbart                   19         91               0.208791\n",
      "8                AP                   17         85               0.200000\n",
      "9               NPR                   15         77               0.194805\n"
     ]
    }
   ],
   "source": [
    "# Group by 'source' and calculate the sum of 'predicted_label' and the count of rows\n",
    "grouped_data_test = data_copy.groupby('source').agg(\n",
    "    predicted_label_sum=('predicted_label', 'sum'),\n",
    "    row_count=('predicted_label', 'size')\n",
    ")\n",
    "\n",
    "# Calculate the ratio\n",
    "grouped_data_test['predicted_label_ratio'] = grouped_data_test['predicted_label_sum'] / grouped_data_test['row_count']\n",
    "\n",
    "# Sort the data in descending order based on the ratio\n",
    "sorted_data_test = grouped_data_test.sort_values(by='predicted_label_ratio', ascending=False).reset_index()\n",
    "\n",
    "# Display the result\n",
    "print(sorted_data_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
