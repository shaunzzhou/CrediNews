{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Detection to Credibility: A Machine Learning Framework for Assessing News Source Reliability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n",
    "As media continues to grow in volume, it is becoming increasingly difficult to differentiate real and fake news effectively. It is thus imperative for us to find better ways to identify fake news, and for us, this means with the help of data mining and machine learning.\n",
    "\n",
    "In the first part of our project, we will focus on experimenting with different data processing techniques and predictive models, optimising our final pipeline and model to accurately identify fake news.\n",
    "\n",
    "For the second part, we want to apply our trained model to scraped news data from popular US media outlets and access the credibility of these media outlets. This way we can help the public to make more informed decisions about what media outlets they can trust. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd Part: Fake News Classification Use Case\n",
    "\n",
    "For the second part, we scraped articles from 10 different news sites, split into two categories of news sites. \n",
    "\n",
    "The dimensions are shown below:\n",
    "- **Index:**: Index.\n",
    "- **title:** Title of news article.\n",
    "- **text:** Text content of news article.\n",
    "- **label:** Whether news article is real (0) or fake (1).\n",
    "\n",
    "The Fake News Dataset is split into 3 `csv` files (`part1.csv`, `part2.csv`, `part3.csv`) so that size does not exceed size limit to push changes to GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Please uncomment the code box below to pip install relevant dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# For concurrency (running functions in parallel)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# For caching (to speed up repeated function calls)\n",
    "from functools import lru_cache\n",
    "\n",
    "# For progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting and Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Language Detection packages\n",
    "# `langdetect` for detecting language\n",
    "from langdetect import detect as langdetect_detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "# `langid` for an alternative language detection method\n",
    "from langid import classify as langid_classify\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Part-of-speech tagging\n",
    "from nltk import pos_tag\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "# Regular expressions for text pattern matching\n",
    "import re\n",
    "\n",
    "# Word Cloud generation\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Unreliable websites\n",
    "# breitbart_df = pd.read_csv('./unreliable websites/breitbart_articles.csv')\n",
    "# dailycaller_df = pd.read_csv('./unreliable websites/dailycaller_articles.csv')\n",
    "# naturalnews_df = pd.read_csv('./unreliable websites/naturalnews_articles.csv')\n",
    "# newsmax_df = pd.read_csv('./unreliable websites/newsmax_articles.csv')\n",
    "# zerohedge_df = pd.read_csv('./unreliable websites/zerohedge_articles.csv')\n",
    "\n",
    "# # Reliable websites\n",
    "# cnn_df = pd.read_csv('./reliable websites/cnn_articles.csv')\n",
    "# ap_df = pd.read_csv('./reliable websites/ap_articles.csv')\n",
    "# bbc_df = pd.read_csv('./reliable websites/bbc_articles.csv')\n",
    "# npr_df = pd.read_csv('./reliable websites/npr_articles.csv')\n",
    "# guardian_df = pd.read_csv('./reliable websites/guardian_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unreliable websites\n",
    "breitbart_df = pd.read_csv(\"./test_websites/breitbart_articles_1.csv\")\n",
    "dailycaller_df = pd.read_csv('./test_websites/daily_caller_articles_1.csv')\n",
    "naturalnews_df = pd.read_csv('./test_websites/naturalnews_articles_1.csv')\n",
    "newsmax_df = pd.read_csv('./test_websites/newsmax_articles_1.csv')\n",
    "zerohedge_df = pd.read_csv('./test_websites/zerohedge_articles_1.csv')\n",
    "\n",
    "# Reliable websites\n",
    "cnn_df = pd.read_csv('./test_websites/cnn_articles_1.csv')\n",
    "ap_df = pd.read_csv('./test_websites/ap_articles_1.csv')\n",
    "bbc_df = pd.read_csv('./test_websites/bbc_articles_1.csv')\n",
    "npr_df = pd.read_csv('./test_websites/npr_articles_1.csv')\n",
    "guardian_df = pd.read_csv('./test_websites/guardian_articles_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all 10 dataframes into 1\n",
    "Here we combine all 10 dataframes into 1 dataframe (`data_raw`) by concatenating along rows.\n",
    "\n",
    "We also reset the `index` and drop the old `index` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1522 entries, 0 to 1521\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   source   1522 non-null   object\n",
      " 1   title    1522 non-null   object\n",
      " 2   content  1522 non-null   object\n",
      " 3   date     1522 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 47.7+ KB\n",
      "Dataframe Shape:  (1522, 4)\n"
     ]
    }
   ],
   "source": [
    "# Combining all 3 dataframes into 1\n",
    "data_raw = pd.concat([breitbart_df, dailycaller_df, naturalnews_df,newsmax_df,zerohedge_df,cnn_df,ap_df,bbc_df,npr_df,guardian_df], axis=0)\n",
    "\n",
    "# Reset index and drop old index column\n",
    "data_raw = data_raw.reset_index(drop=True)\n",
    "\n",
    "data_raw.info()\n",
    "print(\"Dataframe Shape: \", data_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>GOP: WH May Have Illegally Altered Biden's 'Ga...</td>\n",
       "      <td>House Republicans say the Biden-Harris White H...</td>\n",
       "      <td>2024-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Mavericks Principal Owners Donate $100 Million...</td>\n",
       "      <td>Miriam Adelson, the principal owner of the NBA...</td>\n",
       "      <td>2024-10-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Fact Check: Harris Campaign Twists Trump Comme...</td>\n",
       "      <td>CLAIM: Vice President Kamala Harris’s campaign...</td>\n",
       "      <td>2024-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Harris Co-Chair: She's Different from Biden, H...</td>\n",
       "      <td>On Tuesday’s broadcast of CNN’s “Laura Coates ...</td>\n",
       "      <td>2024-10-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Quinnipiac Poll: Trump Takes Lead in Pennsylvania</td>\n",
       "      <td>Former President Donald Trump has a two point ...</td>\n",
       "      <td>2024-10-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source                                              title  \\\n",
       "0  Breitbart  GOP: WH May Have Illegally Altered Biden's 'Ga...   \n",
       "1  Breitbart  Mavericks Principal Owners Donate $100 Million...   \n",
       "2  Breitbart  Fact Check: Harris Campaign Twists Trump Comme...   \n",
       "3  Breitbart  Harris Co-Chair: She's Different from Biden, H...   \n",
       "4  Breitbart  Quinnipiac Poll: Trump Takes Lead in Pennsylvania   \n",
       "\n",
       "                                             content        date  \n",
       "0  House Republicans say the Biden-Harris White H...  2024-10-31  \n",
       "1  Miriam Adelson, the principal owner of the NBA...  2024-10-30  \n",
       "2  CLAIM: Vice President Kamala Harris’s campaign...  2024-11-01  \n",
       "3  On Tuesday’s broadcast of CNN’s “Laura Coates ...  2024-10-29  \n",
       "4  Former President Donald Trump has a two point ...  2024-10-30  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "News Max            200\n",
       "Zerohedge           200\n",
       "Breitbart           198\n",
       "The Daily Caller    175\n",
       "Natural News        160\n",
       "AP                  157\n",
       "Guardian            133\n",
       "CNN                 125\n",
       "NPR                  88\n",
       "BBC                  86\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection Criteria\n",
    "- Language: Only use articles written in English (using language detection if necessary).\n",
    "- Date Range: Focus on articles published in 2024 or, at most, 2023 to ensure credibility remains up-to-date.\n",
    "- Content Focus: Only articles from the politics/election sections of each news website, targeting US election-related topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Filtering\n",
    "- Define Political Content: Articles should contain keywords like “election,” “government,” “policy,” or “candidate” to qualify as political.\n",
    "- Exclude Advertisements: Filter out content with commercial keywords or phrases related to advertisements.\n",
    "- Exclude Opinion Pieces: Identify and exclude articles labeled as “opinion,” “editorial,” or similar terms, or those appearing in “Opinion” sections.\n",
    "- Remove Outliers by Length: Filter out articles with fewer than 100 words or more than 5,000 words to focus on substantial content.\n",
    "- Regex Filtering: Use regular expressions to remove boilerplate or irrelevant sections, such as “All rights reserved,” “Read more,” bylines, and embedded links to other articles or advertisements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Language Detection: 100%|██████████| 1522/1522 [00:26<00:00, 56.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>GOP: WH May Have Illegally Altered Biden's 'Ga...</td>\n",
       "      <td>House Republicans say the Biden-Harris White H...</td>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Mavericks Principal Owners Donate $100 Million...</td>\n",
       "      <td>Miriam Adelson, the principal owner of the NBA...</td>\n",
       "      <td>2024-10-30</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Fact Check: Harris Campaign Twists Trump Comme...</td>\n",
       "      <td>CLAIM: Vice President Kamala Harris’s campaign...</td>\n",
       "      <td>2024-11-01</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Harris Co-Chair: She's Different from Biden, H...</td>\n",
       "      <td>On Tuesday’s broadcast of CNN’s “Laura Coates ...</td>\n",
       "      <td>2024-10-29</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Quinnipiac Poll: Trump Takes Lead in Pennsylvania</td>\n",
       "      <td>Former President Donald Trump has a two point ...</td>\n",
       "      <td>2024-10-30</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>US elections 2024: seven key House races to watch</td>\n",
       "      <td>Much attention has been paid to the rematch be...</td>\n",
       "      <td>2024-06-11</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>Michigan congresswoman Rashida Tlaib declines ...</td>\n",
       "      <td>Michigan congresswoman Rashida Tlaib declined ...</td>\n",
       "      <td>2024-11-02</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>Donald Trump says he will meet Ukrainian presi...</td>\n",
       "      <td>Donald Trump said he would meet with Ukrainian...</td>\n",
       "      <td>2024-09-26</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>Middle East crisis live: ‘apocalyptic’ north G...</td>\n",
       "      <td>Deadly Israeli strikes on 'apocalyptic' north ...</td>\n",
       "      <td>2024-11-02</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>Newcastle United 1-0 Arsenal: Premier League –...</td>\n",
       "      <td>Eddie Howe’s turn on TNT. “I’m tired, actually...</td>\n",
       "      <td>2024-11-02</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1522 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         source                                              title  \\\n",
       "0     Breitbart  GOP: WH May Have Illegally Altered Biden's 'Ga...   \n",
       "1     Breitbart  Mavericks Principal Owners Donate $100 Million...   \n",
       "2     Breitbart  Fact Check: Harris Campaign Twists Trump Comme...   \n",
       "3     Breitbart  Harris Co-Chair: She's Different from Biden, H...   \n",
       "4     Breitbart  Quinnipiac Poll: Trump Takes Lead in Pennsylvania   \n",
       "...         ...                                                ...   \n",
       "1517   Guardian  US elections 2024: seven key House races to watch   \n",
       "1518   Guardian  Michigan congresswoman Rashida Tlaib declines ...   \n",
       "1519   Guardian  Donald Trump says he will meet Ukrainian presi...   \n",
       "1520   Guardian  Middle East crisis live: ‘apocalyptic’ north G...   \n",
       "1521   Guardian  Newcastle United 1-0 Arsenal: Premier League –...   \n",
       "\n",
       "                                                content        date language  \n",
       "0     House Republicans say the Biden-Harris White H...  2024-10-31       en  \n",
       "1     Miriam Adelson, the principal owner of the NBA...  2024-10-30       en  \n",
       "2     CLAIM: Vice President Kamala Harris’s campaign...  2024-11-01       en  \n",
       "3     On Tuesday’s broadcast of CNN’s “Laura Coates ...  2024-10-29       en  \n",
       "4     Former President Donald Trump has a two point ...  2024-10-30       en  \n",
       "...                                                 ...         ...      ...  \n",
       "1517  Much attention has been paid to the rematch be...  2024-06-11       en  \n",
       "1518  Michigan congresswoman Rashida Tlaib declined ...  2024-11-02       en  \n",
       "1519  Donald Trump said he would meet with Ukrainian...  2024-09-26       en  \n",
       "1520  Deadly Israeli strikes on 'apocalyptic' north ...  2024-11-02       en  \n",
       "1521  Eddie Howe’s turn on TNT. “I’m tired, actually...  2024-11-02       en  \n",
       "\n",
       "[1522 rows x 5 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Set a seed for langdetect to ensure reproducibility\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# 2a) Simplified preprocessing: only remove non-alphabetic characters\n",
    "def preprocess_text_simple(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    return text.strip()\n",
    "\n",
    "# 2b) Check if the text is non-language (e.g., numbers, symbols only)\n",
    "def is_non_language_text(text):\n",
    "    if re.match(r'^[^a-zA-Z]*$', text):  # Check if text has no alphabetic characters\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# 3a) Function to get langdetect prediction\n",
    "def get_langdetect_prediction(text):\n",
    "    try:\n",
    "        # Directly use text without preprocessing for efficiency\n",
    "        if len(text) < 10 or is_non_language_text(text):\n",
    "            return \"unknown\"\n",
    "        lang = langdetect_detect(text)\n",
    "        return lang\n",
    "    except LangDetectException:\n",
    "        return \"unknown\"\n",
    "\n",
    "# 3b) Function to get langid prediction\n",
    "def get_langid_prediction(text):\n",
    "    try:\n",
    "        lang, _ = langid_classify(text)\n",
    "        if len(text) < 10 or is_non_language_text(text):\n",
    "            return \"unknown\"\n",
    "        return lang\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "# 4) Function to calculate majority vote for each language\n",
    "def calculate_majority_vote(predictions):\n",
    "    vote_counts = {}\n",
    "    for lang in predictions:\n",
    "        if lang in vote_counts:\n",
    "            vote_counts[lang] += 1\n",
    "        else:\n",
    "            vote_counts[lang] = 1\n",
    "    return vote_counts\n",
    "\n",
    "# 5) Parallel processing for efficiency with limited workers\n",
    "def parallel_detection(text):\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(executor.map(lambda func: func(text), \n",
    "                                    [get_langdetect_prediction, get_langid_prediction]))\n",
    "    return results\n",
    "\n",
    "# 6) Caching function for repeated inputs\n",
    "@lru_cache(maxsize=500)\n",
    "def get_cached_language(text):\n",
    "    return combined_language_detection(text)\n",
    "\n",
    "# 7) Combined majority voting language detection function\n",
    "def combined_language_detection(text):\n",
    "    # Check if the text is non-language (e.g., numbers, symbols only)\n",
    "    if is_non_language_text(text):\n",
    "        return \"unknown\"\n",
    "    \n",
    "    # Run the detectors in parallel for efficiency\n",
    "    predictions = parallel_detection(text)\n",
    "    \n",
    "    # Calculate majority vote for each language based on predictions\n",
    "    vote_counts = calculate_majority_vote(predictions)\n",
    "    \n",
    "    # Determine the language with the highest majority vote\n",
    "    final_language = max(vote_counts, key=vote_counts.get)\n",
    "    \n",
    "    # If \"unknown\" is the most common or if all detectors fail, return \"unknown\"\n",
    "    if final_language == \"unknown\" or vote_counts[final_language] <= 1:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    return final_language\n",
    "\n",
    "# 8) Apply the cached function to each text in the DataFrame with a progress bar\n",
    "data_raw['language'] = [get_cached_language(text) for text in tqdm(data_raw['content'], desc=\"Language Detection\")]\n",
    "\n",
    "# 9) Display the DataFrame with detected languages\n",
    "data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>GOP: WH May Have Illegally Altered Biden's 'Ga...</td>\n",
       "      <td>House Republicans say the Biden-Harris White H...</td>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Mavericks Principal Owners Donate $100 Million...</td>\n",
       "      <td>Miriam Adelson, the principal owner of the NBA...</td>\n",
       "      <td>2024-10-30</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Fact Check: Harris Campaign Twists Trump Comme...</td>\n",
       "      <td>CLAIM: Vice President Kamala Harris’s campaign...</td>\n",
       "      <td>2024-11-01</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Harris Co-Chair: She's Different from Biden, H...</td>\n",
       "      <td>On Tuesday’s broadcast of CNN’s “Laura Coates ...</td>\n",
       "      <td>2024-10-29</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Quinnipiac Poll: Trump Takes Lead in Pennsylvania</td>\n",
       "      <td>Former President Donald Trump has a two point ...</td>\n",
       "      <td>2024-10-30</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>US elections 2024: seven key House races to watch</td>\n",
       "      <td>Much attention has been paid to the rematch be...</td>\n",
       "      <td>2024-06-11</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>Michigan congresswoman Rashida Tlaib declines ...</td>\n",
       "      <td>Michigan congresswoman Rashida Tlaib declined ...</td>\n",
       "      <td>2024-11-02</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>Donald Trump says he will meet Ukrainian presi...</td>\n",
       "      <td>Donald Trump said he would meet with Ukrainian...</td>\n",
       "      <td>2024-09-26</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>Middle East crisis live: ‘apocalyptic’ north G...</td>\n",
       "      <td>Deadly Israeli strikes on 'apocalyptic' north ...</td>\n",
       "      <td>2024-11-02</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>Newcastle United 1-0 Arsenal: Premier League –...</td>\n",
       "      <td>Eddie Howe’s turn on TNT. “I’m tired, actually...</td>\n",
       "      <td>2024-11-02</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1522 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         source                                              title  \\\n",
       "0     Breitbart  GOP: WH May Have Illegally Altered Biden's 'Ga...   \n",
       "1     Breitbart  Mavericks Principal Owners Donate $100 Million...   \n",
       "2     Breitbart  Fact Check: Harris Campaign Twists Trump Comme...   \n",
       "3     Breitbart  Harris Co-Chair: She's Different from Biden, H...   \n",
       "4     Breitbart  Quinnipiac Poll: Trump Takes Lead in Pennsylvania   \n",
       "...         ...                                                ...   \n",
       "1517   Guardian  US elections 2024: seven key House races to watch   \n",
       "1518   Guardian  Michigan congresswoman Rashida Tlaib declines ...   \n",
       "1519   Guardian  Donald Trump says he will meet Ukrainian presi...   \n",
       "1520   Guardian  Middle East crisis live: ‘apocalyptic’ north G...   \n",
       "1521   Guardian  Newcastle United 1-0 Arsenal: Premier League –...   \n",
       "\n",
       "                                                content        date language  \n",
       "0     House Republicans say the Biden-Harris White H...  2024-10-31       en  \n",
       "1     Miriam Adelson, the principal owner of the NBA...  2024-10-30       en  \n",
       "2     CLAIM: Vice President Kamala Harris’s campaign...  2024-11-01       en  \n",
       "3     On Tuesday’s broadcast of CNN’s “Laura Coates ...  2024-10-29       en  \n",
       "4     Former President Donald Trump has a two point ...  2024-10-30       en  \n",
       "...                                                 ...         ...      ...  \n",
       "1517  Much attention has been paid to the rematch be...  2024-06-11       en  \n",
       "1518  Michigan congresswoman Rashida Tlaib declined ...  2024-11-02       en  \n",
       "1519  Donald Trump said he would meet with Ukrainian...  2024-09-26       en  \n",
       "1520  Deadly Israeli strikes on 'apocalyptic' north ...  2024-11-02       en  \n",
       "1521  Eddie Howe’s turn on TNT. “I’m tired, actually...  2024-11-02       en  \n",
       "\n",
       "[1522 rows x 5 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows where language is NOT in english and reset the index\n",
    "data_raw = data_raw[data_raw['language'] == 'en'].reset_index(drop=True)\n",
    "data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>GOP: WH May Have Illegally Altered Biden's 'Ga...</td>\n",
       "      <td>House Republicans say the Biden-Harris White H...</td>\n",
       "      <td>2024-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Mavericks Principal Owners Donate $100 Million...</td>\n",
       "      <td>Miriam Adelson, the principal owner of the NBA...</td>\n",
       "      <td>2024-10-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Fact Check: Harris Campaign Twists Trump Comme...</td>\n",
       "      <td>CLAIM: Vice President Kamala Harris’s campaign...</td>\n",
       "      <td>2024-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Harris Co-Chair: She's Different from Biden, H...</td>\n",
       "      <td>On Tuesday’s broadcast of CNN’s “Laura Coates ...</td>\n",
       "      <td>2024-10-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Quinnipiac Poll: Trump Takes Lead in Pennsylvania</td>\n",
       "      <td>Former President Donald Trump has a two point ...</td>\n",
       "      <td>2024-10-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source                                              title  \\\n",
       "0  Breitbart  GOP: WH May Have Illegally Altered Biden's 'Ga...   \n",
       "1  Breitbart  Mavericks Principal Owners Donate $100 Million...   \n",
       "2  Breitbart  Fact Check: Harris Campaign Twists Trump Comme...   \n",
       "3  Breitbart  Harris Co-Chair: She's Different from Biden, H...   \n",
       "4  Breitbart  Quinnipiac Poll: Trump Takes Lead in Pennsylvania   \n",
       "\n",
       "                                             content        date  \n",
       "0  House Republicans say the Biden-Harris White H...  2024-10-31  \n",
       "1  Miriam Adelson, the principal owner of the NBA...  2024-10-30  \n",
       "2  CLAIM: Vice President Kamala Harris’s campaign...  2024-11-01  \n",
       "3  On Tuesday’s broadcast of CNN’s “Laura Coates ...  2024-10-29  \n",
       "4  Former President Donald Trump has a two point ...  2024-10-30  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw['language'].value_counts()\n",
    "\n",
    "# Drop the  'language' column\n",
    "data_raw = data_raw.drop(columns=['language'])\n",
    "\n",
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles before filtering: 1522\n",
      "Number of articles after political keyword filtering: 1456\n",
      "Number of articles after opinion piece filtering: 1313\n",
      "Number of articles after irrelevant content cleaning: 1313\n",
      "Number of articles after all filtering: 1313\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1313 entries, 0 to 1520\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   source   1313 non-null   object\n",
      " 1   title    1313 non-null   object\n",
      " 2   content  1313 non-null   object\n",
      " 3   date     1313 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 51.3+ KB\n"
     ]
    }
   ],
   "source": [
    "#  Copy data for safety\n",
    "data_filtered = data_raw.copy()\n",
    "print(\"Number of articles before filtering:\", len(data_filtered))\n",
    "\n",
    "# Step 2: Political Keywords Filter\n",
    "us_politics_keywords = [\n",
    "    # Core Keywords\n",
    "    \"election\", \"elections\", \"2024 election\", \"presidential election\", \"campaign\", \"campaigning\",\n",
    "    \"primary\", \"primaries\", \"polling\", \"polls\",\n",
    "\n",
    "    # Key Political Figures\n",
    "    \"Kamala Harris\", \"Donald Trump\", \"Joe Biden\", \"Ron DeSantis\", \"Gavin Newsom\", \n",
    "    \"Mike Pence\", \"Vivek Ramaswamy\", \"Tim Walz\"\n",
    "\n",
    "    # Political Parties & Groups\n",
    "    \"Democrat\", \"Democratic Party\", \"Republican\", \"GOP\", \"Republican Party\",\n",
    "    \"Independent\", \"Third party\", \"PAC\", \"Super PAC\",\n",
    "\n",
    "    # Political Issues & Controversies\n",
    "    \"voting rights\", \"voter suppression\", \"absentee ballot\", \"mail-in ballot\",\n",
    "    \"Electoral College\", \"Supreme Court\", \"abortion\", \"Roe v. Wade\", \"gun control\",\n",
    "    \"Second Amendment\", \"immigration\", \"border security\", \"healthcare\", \n",
    "    \"Medicare\", \"Affordable Care Act\", \"climate change\", \"Green New Deal\", \n",
    "    \"inflation\", \"economic policy\", \"tax cuts\", \"tax reform\", \"foreign policy\", \n",
    "    \"foreign relations\",\n",
    "\n",
    "    # U.S. Government Bodies & Offices\n",
    "    \"Congress\", \"Senate\", \"Senators\", \"House of Representatives\", \"White House\",\n",
    "    \"Supreme Court\", \"Federal government\", \"State government\",\n",
    "\n",
    "    # Policies & Bills\n",
    "    \"voting reform\", \"healthcare reform\", \"climate policy\", \"gun legislation\", \n",
    "    \"economic recovery\", \"infrastructure bill\", \"Social Security\", \"student loan forgiveness\",\n",
    "\n",
    "    # Social & Cultural Issues\n",
    "    \"social justice\", \"racial equality\", \"police reform\", \"civil rights\",\n",
    "    \"freedom of speech\", \"religious freedom\",\n",
    "\n",
    "    # Election Processes\n",
    "    \"debates\", \"presidential debate\", \"swing state\", \"battleground state\",\n",
    "    \"electoral votes\",\n",
    "\n",
    "    # Additional Relevant Terms\n",
    "    \"approval rating\", \"national convention\", \"lobbying\", \"lobbyist\",\n",
    "    \"scandal\", \"investigation\", \"political rally\", \"rally\"\n",
    "]\n",
    "data_filtered = data_filtered[data_filtered['content'].str.contains('|'.join(us_politics_keywords), case=False, na=False)]\n",
    "print(\"Number of articles after political keyword filtering:\", len(data_filtered))\n",
    "\n",
    "# Step 3: Exclude Opinion Pieces (assuming titles or content contain specific indicators of opinion)\n",
    "opinion_keywords = ['opinion', 'editorial', 'op-ed']\n",
    "data_filtered = data_filtered[~data_filtered['title'].str.contains('|'.join(opinion_keywords), case=False, na=False)]\n",
    "data_filtered = data_filtered[~data_filtered['content'].str.contains('|'.join(opinion_keywords), case=False, na=False)]\n",
    "print(\"Number of articles after opinion piece filtering:\", len(data_filtered))\n",
    "\n",
    "# Step 4: Remove Irrelevant Content Using Regex\n",
    "# Define patterns for irrelevant content\n",
    "irrelevant_patterns = [\n",
    "    r'All rights reserved', r'Read more', r'For more information', r'Follow us', \n",
    "    r'Find us on', r'Contact the author', r'Subscribe for updates', r'\\bWATCH\\b',\n",
    "    r'Advertisement', r'^[\\W_]+$'  # Removes lines that are mostly symbols or whitespace\n",
    "]\n",
    "\n",
    "def clean_irrelevant_content(text):\n",
    "    for pattern in irrelevant_patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "data_filtered['content'] = data_filtered['content'].apply(clean_irrelevant_content)\n",
    "print(\"Number of articles after irrelevant content cleaning:\", len(data_filtered))\n",
    "\n",
    "# Final DataFrame after filtering\n",
    "print(\"Number of articles after all filtering:\", len(data_filtered))\n",
    "data_filtered.head()\n",
    "data_filtered.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "News Max            189\n",
       "Breitbart           176\n",
       "The Daily Caller    161\n",
       "Zerohedge           158\n",
       "AP                  144\n",
       "Natural News        128\n",
       "CNN                 108\n",
       "Guardian             99\n",
       "NPR                  79\n",
       "BBC                  71\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "Here we select the relevant features for fake news classification.\n",
    "- `title`, `text`\n",
    "- Create a new DataFrame (`data`) by selecting the specifc columns mentioned above from the original DataFrame `data_raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                               title  \\\n",
      "0  GOP: WH May Have Illegally Altered Biden's 'Ga...   \n",
      "1  Mavericks Principal Owners Donate $100 Million...   \n",
      "2  Fact Check: Harris Campaign Twists Trump Comme...   \n",
      "3  Harris Co-Chair: She's Different from Biden, H...   \n",
      "4  Quinnipiac Poll: Trump Takes Lead in Pennsylvania   \n",
      "\n",
      "                                             content     source  \n",
      "0  House Republicans say the Biden-Harris White H...  Breitbart  \n",
      "1  Miriam Adelson, the principal owner of the NBA...  Breitbart  \n",
      "2  CLAIM: Vice President Kamala Harris’s campaign...  Breitbart  \n",
      "3  On Tuesday’s broadcast of CNN’s “Laura Coates ...  Breitbart  \n",
      "4  Former President Donald Trump has a two point ...  Breitbart  \n",
      "\n",
      "The old shape is:  (1313, 3)\n"
     ]
    }
   ],
   "source": [
    "data = data_filtered[['title', 'content', 'source',]]\n",
    "print(type(data))\n",
    "print(data.head())\n",
    "\n",
    "# Shape before dropping duplicates\n",
    "print(\"\\nThe old shape is: \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Remove Duplicate Rows\n",
    "- Drop duplicate rows from the dataframe (`data`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new shape is:  (1313, 3)\n"
     ]
    }
   ],
   "source": [
    "data_ = data.drop_duplicates()\n",
    "\n",
    "# Display the new dataframe shape\n",
    "print(\"The new shape is: \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Outliers\n",
    "\n",
    "### `text`\n",
    "\n",
    "The `text` column of `data`, which is of string type, may contain values with unusually long lengths, indicating the presence of outliers. We will identify the outliers using [Z-score method].\n",
    "\n",
    "1. Create a new column `text_length` in the DataFrame `data` by calculating the length of each review. (Set the value as 0 if the correponding `text` column has NaN values.)\n",
    "\n",
    "2. Check the statistics of `text_length` using `describe()` method.\n",
    "\n",
    "3. Calculate the mean and standard deviation of the `text_length` column.\n",
    "\n",
    "4. Set the Z-score threshold for identifying outliers to 3.\n",
    "\n",
    "5. Identify outliers of the `text_length` column and set the corresponding `text` to np.nan.\n",
    "\n",
    "6. Drop the `text_length` column from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  GOP: WH May Have Illegally Altered Biden's 'Ga...   \n",
      "1  Mavericks Principal Owners Donate $100 Million...   \n",
      "2  Fact Check: Harris Campaign Twists Trump Comme...   \n",
      "\n",
      "                                             content     source  text_length  \n",
      "0  House Republicans say the Biden-Harris White H...  Breitbart         2749  \n",
      "1  Miriam Adelson, the principal owner of the NBA...  Breitbart         2387  \n",
      "2  CLAIM: Vice President Kamala Harris’s campaign...  Breitbart         3202  \n",
      "count     1313.000000\n",
      "mean      4435.329018\n",
      "std       3379.529718\n",
      "min        119.000000\n",
      "25%       2526.000000\n",
      "50%       3550.000000\n",
      "75%       5451.000000\n",
      "max      38821.000000\n",
      "Name: text_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data['text_length'] = data['content'].apply(lambda x: len(x) if pd.notna(x) else 0)\n",
    "print(data.head(3))\n",
    "\n",
    "TL = data[\"text_length\"]\n",
    "stats_TL = TL.describe()\n",
    "print(stats_TL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GOP: WH May Have Illegally Altered Biden's 'Ga...</td>\n",
       "      <td>House Republicans say the Biden-Harris White H...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mavericks Principal Owners Donate $100 Million...</td>\n",
       "      <td>Miriam Adelson, the principal owner of the NBA...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fact Check: Harris Campaign Twists Trump Comme...</td>\n",
       "      <td>CLAIM: Vice President Kamala Harris’s campaign...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harris Co-Chair: She's Different from Biden, H...</td>\n",
       "      <td>On Tuesday’s broadcast of CNN’s “Laura Coates ...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quinnipiac Poll: Trump Takes Lead in Pennsylvania</td>\n",
       "      <td>Former President Donald Trump has a two point ...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  GOP: WH May Have Illegally Altered Biden's 'Ga...   \n",
       "1  Mavericks Principal Owners Donate $100 Million...   \n",
       "2  Fact Check: Harris Campaign Twists Trump Comme...   \n",
       "3  Harris Co-Chair: She's Different from Biden, H...   \n",
       "4  Quinnipiac Poll: Trump Takes Lead in Pennsylvania   \n",
       "\n",
       "                                             content     source  \n",
       "0  House Republicans say the Biden-Harris White H...  Breitbart  \n",
       "1  Miriam Adelson, the principal owner of the NBA...  Breitbart  \n",
       "2  CLAIM: Vice President Kamala Harris’s campaign...  Breitbart  \n",
       "3  On Tuesday’s broadcast of CNN’s “Laura Coates ...  Breitbart  \n",
       "4  Former President Donald Trump has a two point ...  Breitbart  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_TL = TL.mean()\n",
    "# print(mean_TL)\n",
    "\n",
    "sd_TL = TL.std()\n",
    "# print(sd_TL)\n",
    "\n",
    "threshold = 3\n",
    "\n",
    "z_score = zscore(TL)\n",
    "# print(z_score)\n",
    "\n",
    "# Remove 'text' of lengths that are greater than 3 standard deviations above the mean\n",
    "data.loc[abs(z_score) > threshold, 'content'] = np.nan\n",
    "# print(data.head(3))\n",
    "\n",
    "data = data.drop(\"text_length\", axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `title`\n",
    "\n",
    "Similarly, the `title` column of `data` (of type `str`) may also contain values with unusually long lengths, indicating the presence of outliers.\n",
    "\n",
    "1. Create a new column `title_length` in the DataFrame `data` by calculating the length of each price value. (Set the value as 0 if the correponding `title` column has NaN values.)\n",
    "\n",
    "2. Check the statistics of `title_length` using `describe()` method and display its unique values.\n",
    "\n",
    "3. Identify the outlier values by inspecting the content in `title` corresponding to the abnormal value in `title_length` and set the corresponding value of `title` to np.nan.\n",
    "\n",
    "4. Drop the `title_length` column from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  GOP: WH May Have Illegally Altered Biden's 'Ga...   \n",
      "1  Mavericks Principal Owners Donate $100 Million...   \n",
      "2  Fact Check: Harris Campaign Twists Trump Comme...   \n",
      "\n",
      "                                             content     source  title_length  \n",
      "0  House Republicans say the Biden-Harris White H...  Breitbart            59  \n",
      "1  Miriam Adelson, the principal owner of the NBA...  Breitbart           120  \n",
      "2  CLAIM: Vice President Kamala Harris’s campaign...  Breitbart            99  \n",
      "count    1313.000000\n",
      "mean       80.137091\n",
      "std        24.709738\n",
      "min         9.000000\n",
      "25%        62.000000\n",
      "50%        77.000000\n",
      "75%        96.000000\n",
      "max       186.000000\n",
      "Name: title_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data['title_length'] = data['title'].apply(lambda x: len(x) if pd.notna(x) else 0)\n",
    "print(data.head(3))\n",
    "\n",
    "TL = data[\"title_length\"]\n",
    "stats_TL = TL.describe()\n",
    "print(stats_TL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GOP: WH May Have Illegally Altered Biden's 'Ga...</td>\n",
       "      <td>House Republicans say the Biden-Harris White H...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mavericks Principal Owners Donate $100 Million...</td>\n",
       "      <td>Miriam Adelson, the principal owner of the NBA...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fact Check: Harris Campaign Twists Trump Comme...</td>\n",
       "      <td>CLAIM: Vice President Kamala Harris’s campaign...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harris Co-Chair: She's Different from Biden, H...</td>\n",
       "      <td>On Tuesday’s broadcast of CNN’s “Laura Coates ...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quinnipiac Poll: Trump Takes Lead in Pennsylvania</td>\n",
       "      <td>Former President Donald Trump has a two point ...</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  GOP: WH May Have Illegally Altered Biden's 'Ga...   \n",
       "1  Mavericks Principal Owners Donate $100 Million...   \n",
       "2  Fact Check: Harris Campaign Twists Trump Comme...   \n",
       "3  Harris Co-Chair: She's Different from Biden, H...   \n",
       "4  Quinnipiac Poll: Trump Takes Lead in Pennsylvania   \n",
       "\n",
       "                                             content     source  \n",
       "0  House Republicans say the Biden-Harris White H...  Breitbart  \n",
       "1  Miriam Adelson, the principal owner of the NBA...  Breitbart  \n",
       "2  CLAIM: Vice President Kamala Harris’s campaign...  Breitbart  \n",
       "3  On Tuesday’s broadcast of CNN’s “Laura Coates ...  Breitbart  \n",
       "4  Former President Donald Trump has a two point ...  Breitbart  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_TL = TL.mean()\n",
    "# print(mean_TL)\n",
    "\n",
    "sd_TL = TL.std()\n",
    "# print(sd_TL)\n",
    "\n",
    "threshold = 3\n",
    "\n",
    "z_score = zscore(TL)\n",
    "# print(z_score)\n",
    "\n",
    "# Remove 'title' of lengths that are greater than 3 standard deviations above the mean\n",
    "data.loc[abs(z_score) > threshold, 'title'] = np.nan\n",
    "# print(data.head(3))\n",
    "\n",
    "data = data.drop(\"title_length\", axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title       6\n",
       "content    21\n",
       "source      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "### Create new column `full_review`\n",
    "Since there are some rows with empty `text` and `title`, we will concatenate both columns (`text` and `title`) to form a new column `full_content`.\n",
    "1. Replace `NaN` values in `text` and `title` with an empty string.\n",
    "\n",
    "2. Combine `text` and `title` into `full_content`.\n",
    "\n",
    "3. Strip any leading/trailing whitespaces in `full_content`.\n",
    "\n",
    "4. Drop `text` and `title` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      source                                       full_content\n",
      "0  Breitbart  House Republicans say the Biden-Harris White H...\n",
      "1  Breitbart  Miriam Adelson, the principal owner of the NBA...\n",
      "2  Breitbart  CLAIM: Vice President Kamala Harris’s campaign...\n",
      "3  Breitbart  On Tuesday’s broadcast of CNN’s “Laura Coates ...\n",
      "4  Breitbart  Former President Donald Trump has a two point ...\n",
      "\n",
      "The old shape is: (1313, 2)\n"
     ]
    }
   ],
   "source": [
    "# 1) Fill NaN values in 'text' and 'title' with an empty string\n",
    "data['title'] = data['title'].fillna('')\n",
    "data['content'] = data['content'].fillna('')\n",
    "\n",
    "# 2) Combine 'text' and 'title' into 'content'\n",
    "data['full_content'] = data['content'] + \" \" + data['title']\n",
    "\n",
    "# 3) Strip any leading/trailing whitespace\n",
    "data['full_content'] = data['full_content'].str.strip()\n",
    "\n",
    "# 4) Drop `text` and `title` columns\n",
    "data = data.drop(columns = ['content', 'title'])\n",
    "\n",
    "# Check if the 'full_review' column was added and if 'text' and 'title' columns has been dropped\n",
    "print(data.head())\n",
    "print(\"\\nThe old shape is:\",data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Values\n",
    "1. Drop rows where `full_review` are empty strings and reset the index.\n",
    "\n",
    "2. Check if there are no more null values in `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new shape is: (1313, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "source          0\n",
       "full_content    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Drop rows where `full_review` are empty strings and reset the index\n",
    "data = data[data['full_content'] != \"\"].reset_index(drop=True)\n",
    "print(\"The new shape is:\",data.shape)\n",
    "\n",
    "# 2) Check if there are no more null values in `data`\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>full_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>House Republicans say the Biden-Harris White H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Miriam Adelson, the principal owner of the NBA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>CLAIM: Vice President Kamala Harris’s campaign...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>On Tuesday’s broadcast of CNN’s “Laura Coates ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Former President Donald Trump has a two point ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>Striking Boeing staff have been urged by their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>Much attention has been paid to the rematch be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>Michigan congresswoman Rashida Tlaib declined ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>Donald Trump said he would meet with Ukrainian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>Guardian</td>\n",
       "      <td>Middle East crisis live: ‘apocalyptic’ north G...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1313 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         source                                       full_content\n",
       "0     Breitbart  House Republicans say the Biden-Harris White H...\n",
       "1     Breitbart  Miriam Adelson, the principal owner of the NBA...\n",
       "2     Breitbart  CLAIM: Vice President Kamala Harris’s campaign...\n",
       "3     Breitbart  On Tuesday’s broadcast of CNN’s “Laura Coates ...\n",
       "4     Breitbart  Former President Donald Trump has a two point ...\n",
       "...         ...                                                ...\n",
       "1308   Guardian  Striking Boeing staff have been urged by their...\n",
       "1309   Guardian  Much attention has been paid to the rematch be...\n",
       "1310   Guardian  Michigan congresswoman Rashida Tlaib declined ...\n",
       "1311   Guardian  Donald Trump said he would meet with Ukrainian...\n",
       "1312   Guardian  Middle East crisis live: ‘apocalyptic’ north G...\n",
       "\n",
       "[1313 rows x 2 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing for NLP\n",
    "\n",
    "Here we will define a function `process_full_review` that takes a textual value as input and applies the following processing steps in sequence:\n",
    "\n",
    "1. Convert the input text to lowercase using the `lower()` function.\n",
    "\n",
    "2. Tokenize the lowercase text using the `word_tokenize` function from the NLTK library.\n",
    "\n",
    "3. Create a list (`alphabetic_tokens`) containing only alphanetic tokens using a list comprehension with a regular expression match.\n",
    "\n",
    "4. Remove stopwords\n",
    "-   Obtain a set of English stopwords using the `stopwords.words('english')` method.\n",
    "-   Define a list of `allowed_words` that should not be removed.\n",
    "-   Remove the stopwords (excluding those that should not be removed).\n",
    "\n",
    "5. Apply stemming to each token in the list (`lemmatized_words`) using the `lemmatize` method.\n",
    "\n",
    "6. Join the stemmed tokens into a single processed text using the `join` method and return the processed text.\n",
    "\n",
    "Create  new columns (`processed_full_review`) in `data` by applying the `process_full_review` function to the `full_review` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure require NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to process text\n",
    "import string\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "def process_full_review(text):\n",
    "    # Convert to lowercase and tokenize\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    stemmer = PorterStemmer()\n",
    "    # List of stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    allowed_words = [\"no\", \"not\", \"don't\", \"dont\", \"don\", \"but\", \n",
    "                     \"however\", \"never\", \"wasn't\", \"wasnt\", \"shouldn't\",\n",
    "                     \"shouldnt\", \"mustn't\", \"musnt\"]\n",
    "\n",
    "    stemmed = [stemmer.stem(word) for word in tokens if word not in stop_words or word in allowed_words]\n",
    "    return ' '.join(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">The code below will take approximately 16 minutes to run!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing News Articles: 100%|██████████| 1313/1313 [00:13<00:00, 96.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Enable tqdm for pandas (progress bar)\n",
    "tqdm.pandas(desc=\"Processing News Articles\")\n",
    "\n",
    "# Apply process_full_review function with tqdm progress bar and expand the results into separate columns.\n",
    "processed_columns = ['processed_full_content']\n",
    "data[processed_columns] = data['full_content'].progress_apply(lambda x: pd.Series(process_full_review(x)))\n",
    "\n",
    "data\n",
    "\n",
    "data_copy = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "News Max            189\n",
       "Breitbart           176\n",
       "The Daily Caller    161\n",
       "Zerohedge           158\n",
       "AP                  144\n",
       "Natural News        128\n",
       "CNN                 108\n",
       "Guardian             99\n",
       "NPR                  79\n",
       "BBC                  71\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['source'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles after sampling: 600\n",
      "source\n",
      "AP                  60\n",
      "BBC                 60\n",
      "Breitbart           60\n",
      "CNN                 60\n",
      "Guardian            60\n",
      "NPR                 60\n",
      "Natural News        60\n",
      "News Max            60\n",
      "The Daily Caller    60\n",
      "Zerohedge           60\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17876\\1002029283.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=sample_size, random_state=42))\n"
     ]
    }
   ],
   "source": [
    "# Sample 61 articles from each source without including the grouping column in the operation\n",
    "sample_size = 60\n",
    "\n",
    "data_sampled = (\n",
    "    data.groupby('source', group_keys=False)\n",
    "    .apply(lambda x: x.sample(n=sample_size, random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of articles after sampling:\", len(data_sampled))\n",
    "print(data_sampled['source'].value_counts())  # Check the counts per source\n",
    "data_sampled.head()\n",
    "\n",
    "data_sampled = data_sampled.drop(columns=['full_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>processed_full_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AP</td>\n",
       "      <td>hous back measur overturn biden auto emiss rul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AP</td>\n",
       "      <td>brett favr appear us hous panel look welfar mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP</td>\n",
       "      <td>nevada immigr nuanc issu but polit paint black...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP</td>\n",
       "      <td>speaker johnson set hous vote govern fund bill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP</td>\n",
       "      <td>struggl senat control goe wire spend shatter r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>former unit state presid donald trump ’ lead t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>current nation climat poll seem point trump vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>key solv urban crise hous shortag traffic cong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>former presid donald trump previou member admi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>spokesman harri campaign confirm kamala said n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source                             processed_full_content\n",
       "0           AP  hous back measur overturn biden auto emiss rul...\n",
       "1           AP  brett favr appear us hous panel look welfar mi...\n",
       "2           AP  nevada immigr nuanc issu but polit paint black...\n",
       "3           AP  speaker johnson set hous vote govern fund bill...\n",
       "4           AP  struggl senat control goe wire spend shatter r...\n",
       "..         ...                                                ...\n",
       "595  Zerohedge  former unit state presid donald trump ’ lead t...\n",
       "596  Zerohedge  current nation climat poll seem point trump vi...\n",
       "597  Zerohedge  key solv urban crise hous shortag traffic cong...\n",
       "598  Zerohedge  former presid donald trump previou member admi...\n",
       "599  Zerohedge  spokesman harri campaign confirm kamala said n...\n",
       "\n",
       "[600 rows x 2 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying our best model (LSTM + GloVe 300D) on the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../processed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "400/400 [==============================] - 22s 51ms/step - loss: 0.2060 - accuracy: 0.9262 - val_loss: 0.1517 - val_accuracy: 0.9498\n",
      "Epoch 2/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0915 - accuracy: 0.9740 - val_loss: 0.1160 - val_accuracy: 0.9637\n",
      "Epoch 3/10\n",
      "400/400 [==============================] - 19s 46ms/step - loss: 0.0618 - accuracy: 0.9843 - val_loss: 0.1289 - val_accuracy: 0.9612\n",
      "Epoch 4/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0417 - accuracy: 0.9916 - val_loss: 0.1201 - val_accuracy: 0.9667\n",
      "Epoch 5/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0296 - accuracy: 0.9953 - val_loss: 0.1299 - val_accuracy: 0.9630\n",
      "400/400 [==============================] - 2s 5ms/step\n",
      "400/400 [==============================] - 2s 5ms/step\n",
      "Fold Accuracy: 0.9636705292828062, Precision: 0.9705464868701207, Recall: 0.9483356449375867, F1 Score: 0.9593125219221326, ROC AUC: 0.9933864322428001\n",
      "Epoch 1/10\n",
      "400/400 [==============================] - 21s 48ms/step - loss: 0.2047 - accuracy: 0.9273 - val_loss: 0.1257 - val_accuracy: 0.9609\n",
      "Epoch 2/10\n",
      "400/400 [==============================] - 20s 49ms/step - loss: 0.0914 - accuracy: 0.9745 - val_loss: 0.1078 - val_accuracy: 0.9680\n",
      "Epoch 3/10\n",
      "400/400 [==============================] - 20s 50ms/step - loss: 0.0563 - accuracy: 0.9871 - val_loss: 0.1060 - val_accuracy: 0.9688\n",
      "Epoch 4/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0438 - accuracy: 0.9904 - val_loss: 0.1183 - val_accuracy: 0.9654\n",
      "Epoch 5/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0301 - accuracy: 0.9950 - val_loss: 0.1232 - val_accuracy: 0.9676\n",
      "Epoch 6/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0257 - accuracy: 0.9968 - val_loss: 0.1329 - val_accuracy: 0.9654\n",
      "400/400 [==============================] - 2s 5ms/step\n",
      "400/400 [==============================] - 2s 5ms/step\n",
      "Fold Accuracy: 0.9687597870341371, Precision: 0.9694827586206897, Recall: 0.9620188195038495, F1 Score: 0.9657363675397166, ROC AUC: 0.9942624433740944\n",
      "Epoch 1/10\n",
      "400/400 [==============================] - 21s 50ms/step - loss: 0.2109 - accuracy: 0.9233 - val_loss: 0.1410 - val_accuracy: 0.9536\n",
      "Epoch 2/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0928 - accuracy: 0.9732 - val_loss: 0.1089 - val_accuracy: 0.9657\n",
      "Epoch 3/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0567 - accuracy: 0.9862 - val_loss: 0.1157 - val_accuracy: 0.9664\n",
      "Epoch 4/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0396 - accuracy: 0.9920 - val_loss: 0.1250 - val_accuracy: 0.9654\n",
      "Epoch 5/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0296 - accuracy: 0.9950 - val_loss: 0.1678 - val_accuracy: 0.9587\n",
      "400/400 [==============================] - 2s 5ms/step\n",
      "400/400 [==============================] - 2s 5ms/step\n",
      "Fold Accuracy: 0.9657062323833385, Precision: 0.9653482674133707, Recall: 0.9583043780403058, F1 Score: 0.9618134263295554, ROC AUC: 0.9937271593252947\n",
      "Epoch 1/10\n",
      "400/400 [==============================] - 22s 50ms/step - loss: 0.2030 - accuracy: 0.9268 - val_loss: 0.1296 - val_accuracy: 0.9573\n",
      "Epoch 2/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0905 - accuracy: 0.9742 - val_loss: 0.1092 - val_accuracy: 0.9646\n",
      "Epoch 3/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0619 - accuracy: 0.9845 - val_loss: 0.1329 - val_accuracy: 0.9616\n",
      "Epoch 4/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0415 - accuracy: 0.9918 - val_loss: 0.1220 - val_accuracy: 0.9658\n",
      "Epoch 5/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0287 - accuracy: 0.9956 - val_loss: 0.1286 - val_accuracy: 0.9623\n",
      "400/400 [==============================] - 3s 6ms/step\n",
      "400/400 [==============================] - 2s 5ms/step\n",
      "Fold Accuracy: 0.964610084559975, Precision: 0.9614197530864198, Recall: 0.9610901611244429, F1 Score: 0.9612549288530773, ROC AUC: 0.9940960130438826\n",
      "Epoch 1/10\n",
      "400/400 [==============================] - 22s 51ms/step - loss: 0.2082 - accuracy: 0.9241 - val_loss: 0.1196 - val_accuracy: 0.9625\n",
      "Epoch 2/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0970 - accuracy: 0.9725 - val_loss: 0.1052 - val_accuracy: 0.9665\n",
      "Epoch 3/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0593 - accuracy: 0.9856 - val_loss: 0.1098 - val_accuracy: 0.9655\n",
      "Epoch 4/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0443 - accuracy: 0.9907 - val_loss: 0.1266 - val_accuracy: 0.9649\n",
      "Epoch 5/10\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 0.0320 - accuracy: 0.9947 - val_loss: 0.1129 - val_accuracy: 0.9656\n",
      "400/400 [==============================] - 2s 4ms/step\n",
      "400/400 [==============================] - 2s 5ms/step\n",
      "Fold Accuracy: 0.9664891951143125, Precision: 0.9691804407713499, Recall: 0.9576386526029261, F1 Score: 0.96337497860688, ROC AUC: 0.9948769748447618\n",
      "\n",
      "Average Metrics Across Folds:\n",
      "Accuracy: 0.9658\n",
      "Precision: 0.9672\n",
      "Recall: 0.9575\n",
      "F1: 0.9623\n",
      "Roc_auc: 0.9941\n",
      "Final Average Metrics: {'accuracy': 0.9658471656749139, 'precision': 0.9671955413523902, 'recall': 0.9574775312418222, 'f1': 0.9622984446502724, 'roc_auc': 0.9940698045661668}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "# Load and process GloVe embeddings\n",
    "def load_glove_embeddings(glove_file, word_index, embedding_dim=300):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefficients = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefficients\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "# Define LSTM model with Dropout and L2 regularization\n",
    "def create_lstm_model(vocab_size, embedding_matrix, input_length, learning_rate=0.001, l2_lambda=0.01):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_matrix.shape[1],\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=input_length,\n",
    "                        trainable=True))\n",
    "    model.add(LSTM(units=64, return_sequences=False, dropout=0.2))\n",
    "    model.add(Dropout(0.2))  # Added Dropout layer here\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(l2_lambda)))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # or 'val_accuracy' if you prefer to monitor accuracy\n",
    "    patience=3,          # stop after 3 epochs with no improvement\n",
    "    restore_best_weights=True  # revert to the best weights after stopping\n",
    ")\n",
    "# K-Fold Cross-Validation with additional metrics\n",
    "def k_fold_cross_validation(X, y, embedding_matrix, vocab_size, max_len, n_splits=5, batch_size=128):\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "\n",
    "    # Define early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',  # monitor validation loss\n",
    "        patience=3,          # number of epochs with no improvement\n",
    "        restore_best_weights=True  # revert to the best model weights\n",
    "    )\n",
    "\n",
    "    for train_idx, val_idx in kfold.split(X):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model = create_lstm_model(vocab_size, embedding_matrix, max_len, learning_rate=0.001, l2_lambda=0.01)\n",
    "        \n",
    "        # Fit model with early stopping\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=10,             # maximum number of epochs\n",
    "            batch_size=128,\n",
    "            verbose=1,\n",
    "            callbacks=[early_stopping]  # add early stopping\n",
    "        )\n",
    "        \n",
    "        y_pred = (model.predict(X_val) > 0.5).astype(\"int32\")\n",
    "        y_pred_prob = model.predict(X_val).ravel()\n",
    "\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision = precision_score(y_val, y_pred)\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        roc_auc = roc_auc_score(y_val, y_pred_prob)\n",
    "\n",
    "        metrics['accuracy'].append(accuracy)\n",
    "        metrics['precision'].append(precision)\n",
    "        metrics['recall'].append(recall)\n",
    "        metrics['f1'].append(f1)\n",
    "        metrics['roc_auc'].append(roc_auc)\n",
    "\n",
    "        print(f\"Fold Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}, ROC AUC: {roc_auc}\")\n",
    "\n",
    "    avg_metrics = {metric: np.mean(scores) for metric, scores in metrics.items()}\n",
    "    print(\"\\nAverage Metrics Across Folds:\")\n",
    "    for metric, avg_score in avg_metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {avg_score:.4f}\")\n",
    "    \n",
    "    return avg_metrics, model\n",
    "\n",
    "# Example usage\n",
    "# Load your data and tokenize it\n",
    "texts = data[\"processed_full_content\"]\n",
    "num_samples = len(texts)\n",
    "labels = data[\"label\"].values  # Adjusted labels for each sample\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "X = tokenizer.texts_to_sequences(texts)\n",
    "max_len = 100\n",
    "X = pad_sequences(X, maxlen=max_len)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "embedding_dim = 300\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = load_glove_embeddings(\"../model_experiements/glove.6B.300d.txt\", tokenizer.word_index, embedding_dim)\n",
    "\n",
    "# Perform K-Fold cross-validation\n",
    "avg_metrics, trained_model = k_fold_cross_validation(X, labels, embedding_matrix, vocab_size, max_len)\n",
    "print(\"Final Average Metrics:\", avg_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "scraped_texts = data_sampled['processed_full_content']\n",
    "scraped_sequences = tokenizer.texts_to_sequences(scraped_texts)\n",
    "scraped_padded_sequences = pad_sequences(scraped_sequences, maxlen=max_len)\n",
    "\n",
    "# Get predictions from the trained model\n",
    "new_predictions = trained_model.predict(scraped_padded_sequences).ravel()\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "# For binary classification with sigmoid activation, threshold at 0.5\n",
    "predicted_classes = (new_predictions > 0.5).astype(\"int32\")\n",
    "data_sampled['predicted_label'] = predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>processed_full_content</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AP</td>\n",
       "      <td>hous back measur overturn biden auto emiss rul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AP</td>\n",
       "      <td>brett favr appear us hous panel look welfar mi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP</td>\n",
       "      <td>nevada immigr nuanc issu but polit paint black...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP</td>\n",
       "      <td>speaker johnson set hous vote govern fund bill...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP</td>\n",
       "      <td>struggl senat control goe wire spend shatter r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>former unit state presid donald trump ’ lead t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>current nation climat poll seem point trump vi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>key solv urban crise hous shortag traffic cong...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>former presid donald trump previou member admi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>Zerohedge</td>\n",
       "      <td>spokesman harri campaign confirm kamala said n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source                             processed_full_content  \\\n",
       "0           AP  hous back measur overturn biden auto emiss rul...   \n",
       "1           AP  brett favr appear us hous panel look welfar mi...   \n",
       "2           AP  nevada immigr nuanc issu but polit paint black...   \n",
       "3           AP  speaker johnson set hous vote govern fund bill...   \n",
       "4           AP  struggl senat control goe wire spend shatter r...   \n",
       "..         ...                                                ...   \n",
       "595  Zerohedge  former unit state presid donald trump ’ lead t...   \n",
       "596  Zerohedge  current nation climat poll seem point trump vi...   \n",
       "597  Zerohedge  key solv urban crise hous shortag traffic cong...   \n",
       "598  Zerohedge  former presid donald trump previou member admi...   \n",
       "599  Zerohedge  spokesman harri campaign confirm kamala said n...   \n",
       "\n",
       "     predicted_label  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  1  \n",
       "3                  0  \n",
       "4                  0  \n",
       "..               ...  \n",
       "595                1  \n",
       "596                0  \n",
       "597                1  \n",
       "598                0  \n",
       "599                1  \n",
       "\n",
       "[600 rows x 3 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             source  predicted_label\n",
      "0      Natural News               54\n",
      "1         Zerohedge               40\n",
      "2  The Daily Caller               34\n",
      "3          News Max               31\n",
      "4          Guardian               28\n",
      "5               BBC               17\n",
      "6               CNN               14\n",
      "7         Breitbart               12\n",
      "8               NPR               12\n",
      "9                AP                8\n"
     ]
    }
   ],
   "source": [
    "# Group by 'source' and calculate the sum of 'predicted_label' for each group\n",
    "grouped_data = data_sampled.groupby('source')['predicted_label'].sum()\n",
    "\n",
    "# Sort the grouped data in descending order based on the sum of 'predicted_label'\n",
    "sorted_data = grouped_data.sort_values(ascending=False)\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "sorted_data = sorted_data.reset_index()\n",
    "\n",
    "# Display the result\n",
    "print(sorted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "scraped_texts_test = data_copy['processed_full_content']\n",
    "scraped_sequences_test = tokenizer.texts_to_sequences(scraped_texts_test)\n",
    "scraped_padded_sequences_test = pad_sequences(scraped_sequences_test, maxlen=max_len)\n",
    "\n",
    "# Get predictions from the trained model\n",
    "new_predictions_test = trained_model.predict(scraped_padded_sequences_test).ravel()\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "# For binary classification with sigmoid activation, threshold at 0.5\n",
    "predicted_classes_test = (new_predictions_test > 0.5).astype(\"int32\")\n",
    "data_copy['predicted_label'] = predicted_classes_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             source  predicted_label_sum  row_count  predicted_label_ratio\n",
      "0      Natural News                  117        128               0.914062\n",
      "1  The Daily Caller                  104        161               0.645963\n",
      "2         Zerohedge                   97        158               0.613924\n",
      "3          Guardian                   48         99               0.484848\n",
      "4          News Max                   85        189               0.449735\n",
      "5               BBC                   23         71               0.323944\n",
      "6               CNN                   30        108               0.277778\n",
      "7         Breitbart                   28        176               0.159091\n",
      "8               NPR                   12         79               0.151899\n",
      "9                AP                   19        144               0.131944\n"
     ]
    }
   ],
   "source": [
    "# Group by 'source' and calculate the sum of 'predicted_label' and the count of rows\n",
    "grouped_data_test = data_copy.groupby('source').agg(\n",
    "    predicted_label_sum=('predicted_label', 'sum'),\n",
    "    row_count=('predicted_label', 'size')\n",
    ")\n",
    "\n",
    "# Calculate the ratio\n",
    "grouped_data_test['predicted_label_ratio'] = grouped_data_test['predicted_label_sum'] / grouped_data_test['row_count']\n",
    "\n",
    "# Sort the data in descending order based on the ratio\n",
    "sorted_data_test = grouped_data_test.sort_values(by='predicted_label_ratio', ascending=False).reset_index()\n",
    "\n",
    "# Display the result\n",
    "print(sorted_data_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
