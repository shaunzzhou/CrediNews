{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working dir: /Users/inflaton/code/engd/papers/DM-Fake-News-Detection\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if \"workding_dir\" not in locals():\n",
    "    workding_dir = str(Path.cwd().parent)\n",
    "os.chdir(workding_dir)\n",
    "sys.path.append(workding_dir)\n",
    "print(\"working dir:\", workding_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Detection to Credibility: A Machine Learning Framework for Assessing News Source Reliability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# For concurrency (running functions in parallel)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# For caching (to speed up repeated function calls)\n",
    "from functools import lru_cache\n",
    "\n",
    "# For progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "import nltk\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Part-of-speech tagging\n",
    "from nltk import pos_tag\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation (Loading CSV)\n",
    "\n",
    "Load the processed_data `csv` file into pandas DataFrames\n",
    "- `processed_data.csv` is loaded into `data` DataFrame (stemming has been performed to reduce processing time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    34770\n",
       "1    28162\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62932 entries, 0 to 62931\n",
      "Data columns (total 3 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   label                   62932 non-null  int64 \n",
      " 1   full_content            62932 non-null  object\n",
      " 2   processed_full_content  62932 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.4+ MB\n",
      "Dataframe Shape: (62932, 3)\n"
     ]
    }
   ],
   "source": [
    "data.info()\n",
    "print(\"Dataframe Shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure required NLTK data is downloaded\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Convolutional Neural Network (Tokenizer + Embedding Layer) + 5 Fold Cross-Validation + L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training fold 1...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inflaton/anaconda3/envs/fake-news/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.8411 - loss: 0.7438 - val_accuracy: 0.9598 - val_loss: 0.2297\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9624 - loss: 0.2133 - val_accuracy: 0.9651 - val_loss: 0.2023\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9747 - loss: 0.1804 - val_accuracy: 0.9641 - val_loss: 0.1930\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9808 - loss: 0.1604 - val_accuracy: 0.9627 - val_loss: 0.1895\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9859 - loss: 0.1461 - val_accuracy: 0.9591 - val_loss: 0.1898\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.9879 - loss: 0.1360 - val_accuracy: 0.9583 - val_loss: 0.1890\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9911 - loss: 0.1276 - val_accuracy: 0.9575 - val_loss: 0.1880\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - accuracy: 0.9925 - loss: 0.1202 - val_accuracy: 0.9554 - val_loss: 0.1897\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.9934 - loss: 0.1140 - val_accuracy: 0.9584 - val_loss: 0.1840\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.9944 - loss: 0.1092 - val_accuracy: 0.9587 - val_loss: 0.1803\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 1 - Accuracy: 0.9587, Precision: 0.9830, Recall: 0.9237, F1 Score: 0.9524\n",
      "\n",
      "Training fold 2...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inflaton/anaconda3/envs/fake-news/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.8355 - loss: 0.7390 - val_accuracy: 0.9523 - val_loss: 0.2346\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.9589 - loss: 0.2173 - val_accuracy: 0.9595 - val_loss: 0.2092\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 25ms/step - accuracy: 0.9727 - loss: 0.1834 - val_accuracy: 0.9615 - val_loss: 0.1936\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.9789 - loss: 0.1633 - val_accuracy: 0.9626 - val_loss: 0.1884\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.9838 - loss: 0.1488 - val_accuracy: 0.9615 - val_loss: 0.1837\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 25ms/step - accuracy: 0.9866 - loss: 0.1379 - val_accuracy: 0.9634 - val_loss: 0.1788\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.9900 - loss: 0.1288 - val_accuracy: 0.9629 - val_loss: 0.1756\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 25ms/step - accuracy: 0.9913 - loss: 0.1217 - val_accuracy: 0.9639 - val_loss: 0.1718\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.9926 - loss: 0.1157 - val_accuracy: 0.9632 - val_loss: 0.1693\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - accuracy: 0.9935 - loss: 0.1105 - val_accuracy: 0.9640 - val_loss: 0.1664\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 2 - Accuracy: 0.9640, Precision: 0.9547, Recall: 0.9654, F1 Score: 0.9600\n",
      "\n",
      "Training fold 3...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inflaton/anaconda3/envs/fake-news/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 24ms/step - accuracy: 0.8437 - loss: 0.7434 - val_accuracy: 0.9571 - val_loss: 0.2248\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.9630 - loss: 0.2126 - val_accuracy: 0.9612 - val_loss: 0.2031\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 25ms/step - accuracy: 0.9740 - loss: 0.1804 - val_accuracy: 0.9627 - val_loss: 0.1946\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 25ms/step - accuracy: 0.9805 - loss: 0.1613 - val_accuracy: 0.9641 - val_loss: 0.1873\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.9847 - loss: 0.1468 - val_accuracy: 0.9640 - val_loss: 0.1838\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.9875 - loss: 0.1369 - val_accuracy: 0.9647 - val_loss: 0.1792\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.9904 - loss: 0.1281 - val_accuracy: 0.9648 - val_loss: 0.1760\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.9923 - loss: 0.1210 - val_accuracy: 0.9645 - val_loss: 0.1729\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.9927 - loss: 0.1157 - val_accuracy: 0.9641 - val_loss: 0.1700\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 25ms/step - accuracy: 0.9938 - loss: 0.1096 - val_accuracy: 0.9650 - val_loss: 0.1674\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Fold 3 - Accuracy: 0.9650, Precision: 0.9604, Recall: 0.9615, F1 Score: 0.9610\n",
      "\n",
      "Training fold 4...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inflaton/anaconda3/envs/fake-news/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - accuracy: 0.8385 - loss: 0.7469 - val_accuracy: 0.9604 - val_loss: 0.2228\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - accuracy: 0.9614 - loss: 0.2154 - val_accuracy: 0.9647 - val_loss: 0.1988\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - accuracy: 0.9721 - loss: 0.1817 - val_accuracy: 0.9649 - val_loss: 0.1910\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - accuracy: 0.9795 - loss: 0.1612 - val_accuracy: 0.9638 - val_loss: 0.1849\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - accuracy: 0.9844 - loss: 0.1472 - val_accuracy: 0.9646 - val_loss: 0.1809\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - accuracy: 0.9877 - loss: 0.1360 - val_accuracy: 0.9650 - val_loss: 0.1763\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.9899 - loss: 0.1276 - val_accuracy: 0.9663 - val_loss: 0.1726\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 25ms/step - accuracy: 0.9931 - loss: 0.1197 - val_accuracy: 0.9673 - val_loss: 0.1689\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 25ms/step - accuracy: 0.9936 - loss: 0.1140 - val_accuracy: 0.9660 - val_loss: 0.1683\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.9948 - loss: 0.1080 - val_accuracy: 0.9664 - val_loss: 0.1650\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "Fold 4 - Accuracy: 0.9664, Precision: 0.9663, Recall: 0.9583, F1 Score: 0.9623\n",
      "\n",
      "Training fold 5...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inflaton/anaconda3/envs/fake-news/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.8481 - loss: 0.7581 - val_accuracy: 0.9600 - val_loss: 0.2264\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9645 - loss: 0.2144 - val_accuracy: 0.9618 - val_loss: 0.2055\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9750 - loss: 0.1806 - val_accuracy: 0.9620 - val_loss: 0.1975\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9814 - loss: 0.1608 - val_accuracy: 0.9619 - val_loss: 0.1920\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.9853 - loss: 0.1465 - val_accuracy: 0.9633 - val_loss: 0.1845\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.9890 - loss: 0.1355 - val_accuracy: 0.9644 - val_loss: 0.1816\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.9910 - loss: 0.1271 - val_accuracy: 0.9631 - val_loss: 0.1805\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.9930 - loss: 0.1198 - val_accuracy: 0.9632 - val_loss: 0.1775\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9940 - loss: 0.1136 - val_accuracy: 0.9631 - val_loss: 0.1757\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9948 - loss: 0.1080 - val_accuracy: 0.9627 - val_loss: 0.1742\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "Fold 5 - Accuracy: 0.9627, Precision: 0.9575, Recall: 0.9592, F1 Score: 0.9583\n",
      "\n",
      "Average Evaluation Metrics across 5 folds:\n",
      "Average Accuracy: 0.9634\n",
      "Average Precision: 0.9644\n",
      "Average Recall: 0.9536\n",
      "Average F1 Score: 0.9588\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Tokenization and Padding Parameters\n",
    "max_words = 10000  # Max vocabulary size\n",
    "max_sequence_length = 300  # Max length of sequences\n",
    "\n",
    "# Tokenize and Pad Sequences\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['processed_full_content'])\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_full_content'])\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "y = data['label'].values  # Target labels\n",
    "\n",
    "# Define the CNN Model with L2 Regularization\n",
    "def create_basic_cnn():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_sequence_length))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.2))  # Add dropout for regularization\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01)))  # Binary classification\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 5-Fold Cross-Validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "all_fold_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    print(f\"\\nTraining fold {fold}...\")\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    model = create_basic_cnn()\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val), verbose=1)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = (model.predict(X_val) > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='binary')\n",
    "    \n",
    "    # Store metrics for this fold\n",
    "    all_fold_metrics['accuracy'].append(accuracy)\n",
    "    all_fold_metrics['precision'].append(precision)\n",
    "    all_fold_metrics['recall'].append(recall)\n",
    "    all_fold_metrics['f1'].append(f1)\n",
    "    \n",
    "    print(f\"Fold {fold} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "    fold += 1\n",
    "\n",
    "# Calculate and print average metrics across all folds\n",
    "avg_accuracy = np.mean(all_fold_metrics['accuracy'])\n",
    "avg_precision = np.mean(all_fold_metrics['precision'])\n",
    "avg_recall = np.mean(all_fold_metrics['recall'])\n",
    "avg_f1 = np.mean(all_fold_metrics['f1'])\n",
    "\n",
    "print(\"\\nAverage Evaluation Metrics across 5 folds:\")\n",
    "print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average F1 Score: {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network + TF-IDF Vectorizer\n",
    "\n",
    "Using TF-IDF vectorizer along with CNN led to a drastic fall in performance. Below are some reasons why we should not use TF-IDF vectorizer along with a CNN or other neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lack of Spatial Structure:\n",
    "\n",
    "TF-IDF vectors are sparse and non-sequential representations where each position in the vector represents a word, not a spatial pattern.\n",
    "CNNs are designed to detect patterns in sequential or spatially structured data (e.g., images or sentences), so they might struggle to find meaningful patterns in TF-IDF vectors.\n",
    "\n",
    "#### High-Dimensional Sparse Data:\n",
    "\n",
    "TF-IDF vectors, especially with a high max_features value (like 10,000), result in a high-dimensional but sparse input.\n",
    "CNNs are generally not well-suited for such high-dimensional sparse data; they perform better with dense embeddings where words have contextually meaningful dimensions.\n",
    "\n",
    "#### Mismatch Between Input Type and CNN Architecture:\n",
    "\n",
    "CNNs are typically effective when applied to word embeddings (like GloVe or Word2Vec) because embeddings maintain semantic relationships and neighborhood structures.\n",
    "TF-IDF, however, does not capture word order or semantic relationships, which means the convolution operation might not yield meaningful feature maps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 78ms/step - accuracy: 0.5434 - loss: 0.6882 - val_accuracy: 0.5696 - val_loss: 0.6805\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 78ms/step - accuracy: 0.5523 - loss: 0.6852 - val_accuracy: 0.5800 - val_loss: 0.6777\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 78ms/step - accuracy: 0.5543 - loss: 0.6822 - val_accuracy: 0.5769 - val_loss: 0.6762\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 77ms/step - accuracy: 0.5575 - loss: 0.6809 - val_accuracy: 0.5761 - val_loss: 0.6762\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 78ms/step - accuracy: 0.5582 - loss: 0.6803 - val_accuracy: 0.5769 - val_loss: 0.6746\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 77ms/step - accuracy: 0.5587 - loss: 0.6803 - val_accuracy: 0.5798 - val_loss: 0.6744\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 77ms/step - accuracy: 0.5578 - loss: 0.6801 - val_accuracy: 0.5766 - val_loss: 0.6755\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 80ms/step - accuracy: 0.5594 - loss: 0.6798 - val_accuracy: 0.5789 - val_loss: 0.6746\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 80ms/step - accuracy: 0.5580 - loss: 0.6797 - val_accuracy: 0.5796 - val_loss: 0.6736\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 83ms/step - accuracy: 0.5596 - loss: 0.6798 - val_accuracy: 0.5802 - val_loss: 0.6735\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step\n",
      "\n",
      "Evaluation Metrics:\n",
      "Accuracy: 0.5802\n",
      "Precision: 0.5523\n",
      "Recall: 0.2019\n",
      "F1 Score: 0.2957\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout, Reshape, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Step 1: Apply TF-IDF Vectorization\n",
    "max_features = 10000  # Limit TF-IDF to top 10,000 features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['processed_full_content']).toarray()\n",
    "\n",
    "# Convert the labels\n",
    "y = data['label'].values  # Target labels\n",
    "\n",
    "# Step 2: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Define the CNN Model for TF-IDF Input\n",
    "def create_cnn_with_tfidf():\n",
    "    inputs = Input(shape=(max_features,))\n",
    "    x = Reshape((max_features, 1))(inputs)  # Reshape TF-IDF output to be compatible with Conv1D\n",
    "\n",
    "    # Convolutional layer\n",
    "    x = Conv1D(filters=128, kernel_size=5, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)  # Dropout for regularization\n",
    "    outputs = Dense(1, activation='sigmoid')(x)  # Output layer for binary classification\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Step 4: Train the Model\n",
    "model = create_cnn_with_tfidf()\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks + Count Vectorization (conversion to sequences) + Stratified 5-Fold CV + L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.7907 - loss: 0.8565 - val_accuracy: 0.9303 - val_loss: 0.2637\n",
      "Epoch 2/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9325 - loss: 0.2664 - val_accuracy: 0.9450 - val_loss: 0.2344\n",
      "Epoch 3/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9484 - loss: 0.2316 - val_accuracy: 0.9483 - val_loss: 0.2259\n",
      "Epoch 4/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 26ms/step - accuracy: 0.9608 - loss: 0.2088 - val_accuracy: 0.9514 - val_loss: 0.2198\n",
      "Epoch 5/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9676 - loss: 0.1931 - val_accuracy: 0.9533 - val_loss: 0.2171\n",
      "Epoch 6/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.9726 - loss: 0.1792 - val_accuracy: 0.9554 - val_loss: 0.2114\n",
      "Epoch 7/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.9772 - loss: 0.1689 - val_accuracy: 0.9552 - val_loss: 0.2124\n",
      "Epoch 8/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9826 - loss: 0.1579 - val_accuracy: 0.9541 - val_loss: 0.2135\n",
      "Epoch 9/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9845 - loss: 0.1503 - val_accuracy: 0.9552 - val_loss: 0.2122\n",
      "Epoch 10/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9873 - loss: 0.1439 - val_accuracy: 0.9544 - val_loss: 0.2071\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 26ms/step - accuracy: 0.7928 - loss: 0.8504 - val_accuracy: 0.9257 - val_loss: 0.2680\n",
      "Epoch 2/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.9315 - loss: 0.2682 - val_accuracy: 0.9343 - val_loss: 0.2445\n",
      "Epoch 3/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9387 - loss: 0.2419 - val_accuracy: 0.9353 - val_loss: 0.2419\n",
      "Epoch 4/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9495 - loss: 0.2229 - val_accuracy: 0.9370 - val_loss: 0.2397\n",
      "Epoch 5/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9569 - loss: 0.2082 - val_accuracy: 0.9445 - val_loss: 0.2301\n",
      "Epoch 6/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9644 - loss: 0.1934 - val_accuracy: 0.9431 - val_loss: 0.2363\n",
      "Epoch 7/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 26ms/step - accuracy: 0.9694 - loss: 0.1823 - val_accuracy: 0.9427 - val_loss: 0.2408\n",
      "Epoch 8/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 26ms/step - accuracy: 0.9736 - loss: 0.1735 - val_accuracy: 0.9435 - val_loss: 0.2397\n",
      "Epoch 9/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9784 - loss: 0.1645 - val_accuracy: 0.9398 - val_loss: 0.2483\n",
      "Epoch 10/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 26ms/step - accuracy: 0.9814 - loss: 0.1570 - val_accuracy: 0.9393 - val_loss: 0.2519\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.7987 - loss: 0.8555 - val_accuracy: 0.9367 - val_loss: 0.2501\n",
      "Epoch 2/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.9338 - loss: 0.2657 - val_accuracy: 0.9459 - val_loss: 0.2311\n",
      "Epoch 3/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9454 - loss: 0.2373 - val_accuracy: 0.9517 - val_loss: 0.2203\n",
      "Epoch 4/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.9556 - loss: 0.2156 - val_accuracy: 0.9537 - val_loss: 0.2150\n",
      "Epoch 5/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9639 - loss: 0.1970 - val_accuracy: 0.9544 - val_loss: 0.2114\n",
      "Epoch 6/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9693 - loss: 0.1832 - val_accuracy: 0.9550 - val_loss: 0.2107\n",
      "Epoch 7/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9774 - loss: 0.1706 - val_accuracy: 0.9545 - val_loss: 0.2102\n",
      "Epoch 8/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9808 - loss: 0.1599 - val_accuracy: 0.9543 - val_loss: 0.2084\n",
      "Epoch 9/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 26ms/step - accuracy: 0.9847 - loss: 0.1513 - val_accuracy: 0.9540 - val_loss: 0.2098\n",
      "Epoch 10/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 26ms/step - accuracy: 0.9868 - loss: 0.1454 - val_accuracy: 0.9537 - val_loss: 0.2084\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 26ms/step - accuracy: 0.7817 - loss: 0.8696 - val_accuracy: 0.9330 - val_loss: 0.2582\n",
      "Epoch 2/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9322 - loss: 0.2649 - val_accuracy: 0.9426 - val_loss: 0.2392\n",
      "Epoch 3/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9453 - loss: 0.2348 - val_accuracy: 0.9505 - val_loss: 0.2230\n",
      "Epoch 4/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.9579 - loss: 0.2108 - val_accuracy: 0.9519 - val_loss: 0.2202\n",
      "Epoch 5/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9668 - loss: 0.1954 - val_accuracy: 0.9527 - val_loss: 0.2184\n",
      "Epoch 6/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9733 - loss: 0.1832 - val_accuracy: 0.9551 - val_loss: 0.2134\n",
      "Epoch 7/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.9777 - loss: 0.1721 - val_accuracy: 0.9541 - val_loss: 0.2133\n",
      "Epoch 8/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.9800 - loss: 0.1635 - val_accuracy: 0.9548 - val_loss: 0.2128\n",
      "Epoch 9/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9833 - loss: 0.1553 - val_accuracy: 0.9523 - val_loss: 0.2129\n",
      "Epoch 10/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9857 - loss: 0.1503 - val_accuracy: 0.9536 - val_loss: 0.2103\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.7829 - loss: 0.8832 - val_accuracy: 0.9364 - val_loss: 0.2546\n",
      "Epoch 2/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9323 - loss: 0.2682 - val_accuracy: 0.9427 - val_loss: 0.2323\n",
      "Epoch 3/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9401 - loss: 0.2410 - val_accuracy: 0.9472 - val_loss: 0.2228\n",
      "Epoch 4/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9491 - loss: 0.2228 - val_accuracy: 0.9508 - val_loss: 0.2166\n",
      "Epoch 5/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.9599 - loss: 0.2059 - val_accuracy: 0.9528 - val_loss: 0.2104\n",
      "Epoch 6/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9666 - loss: 0.1939 - val_accuracy: 0.9529 - val_loss: 0.2100\n",
      "Epoch 7/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9700 - loss: 0.1813 - val_accuracy: 0.9534 - val_loss: 0.2092\n",
      "Epoch 8/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 26ms/step - accuracy: 0.9750 - loss: 0.1729 - val_accuracy: 0.9550 - val_loss: 0.2089\n",
      "Epoch 9/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9784 - loss: 0.1641 - val_accuracy: 0.9548 - val_loss: 0.2072\n",
      "Epoch 10/10\n",
      "\u001b[1m630/630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.9811 - loss: 0.1573 - val_accuracy: 0.9548 - val_loss: 0.2048\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\n",
      "Cross-Validation Metrics:\n",
      "Average Accuracy: 0.9480\n",
      "Average Precision: 0.9410\n",
      "Average Recall: 0.9437\n",
      "Average F1 Score: 0.9420\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Step 1: Text Vectorization using CountVectorizer\n",
    "max_features = 10000  # Max vocabulary size for CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=max_features)\n",
    "X_counts = vectorizer.fit_transform(data['processed_full_content'])\n",
    "word_index = vectorizer.vocabulary_\n",
    "\n",
    "# Convert CountVectorizer output to sequences\n",
    "index_to_word = {i: word for word, i in word_index.items()}\n",
    "\n",
    "def counts_to_sequences(X_counts):\n",
    "    sequences = []\n",
    "    for i in range(X_counts.shape[0]):\n",
    "        indices = X_counts[i].nonzero()[1]\n",
    "        words = [index_to_word[idx] for idx in indices]\n",
    "        seq = [word_index[word] + 1 for word in words]  # +1 because 0 is reserved for padding\n",
    "        sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "sequences = counts_to_sequences(X_counts)\n",
    "max_sequence_length = 300  # Adjust to your needs\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "y = data['label'].values  # Target labels\n",
    "\n",
    "# Define the Basic CNN Model with L2 Regularization\n",
    "def create_basic_cnn_with_l2():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding layer with random initialization\n",
    "    model.add(Embedding(input_dim=max_features + 1, output_dim=128))\n",
    "    \n",
    "    # Convolutional layer with L2 regularization\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    \n",
    "    # Fully connected layer with L2 regularization\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))  # Add dropout for regularization\n",
    "    \n",
    "    # Output layer with L2 regularization\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01)))  # Binary classification\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Step 4: Stratified 5-Fold Cross-Validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = create_basic_cnn_with_l2()\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "    \n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Step 5: Print Cross-Validation Results\n",
    "print(\"\\nCross-Validation Metrics:\")\n",
    "print(f\"Average Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Average Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Average Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network + Custom-trained Word2Vec Embeddings + 5-Fold Cross Validation + L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we use word embedding over other preprocessing techniques (eg. tf-idf, count vectorizer), for our task of fake news classification?\n",
    "\n",
    "\n",
    "##### 1. Word embeddings capture the semantic relationships between words in a dense, low-dimensional space.\n",
    "Fake news often uses subtle language, and word embeddings like GloVe can capture the semantic context of words, allowing the model to understand relationships between words that simple vectorizers would miss. This helps in detecting nuanced differences in language use between real and fake news.\n",
    "\n",
    "##### 2. Word embeddings produce dense, low-dimensional vectors (e.g., 100-300 dimensions) that capture rich word information.\n",
    "Pre-trained embeddings are built on large corpora like Wikipedia and news articles, giving our model external knowledge that’s useful for distinguishing between real news and fake news. This boosts the model's ability to generalize on unseen test data from our web scraping.\n",
    "\n",
    "##### 3. Efficient Representation of Semantics\n",
    "Words in fake news can appear in different contexts, but with similar underlying meanings (e.g., \"hoax\" and \"lie\"). GloVe embeddings represent these similar words in close proximity in the vector space, helping the model recognize fake news patterns more effectively than TF-IDF or Count Vectorizer.\n",
    "\n",
    "##### 4. Handling Synonyms and Rare Words:\n",
    "Fake news often uses alternative phrases or rare terminology. Pre-trained embeddings like GloVe can handle these rare words because they’ve seen a broad variety of language during training, making our model more robust against unusual vocabulary choices in fake news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation \n",
    "We use Stratified K-Fold Cross-Validation with n_splits=5 to evaluate the model on different splits of the data. \n",
    "For each fold, we store the metrics (accuracy, precision, recall, and F1 score) and then calculate the average metrics across all folds for a robust evaluation.\n",
    "\n",
    "Cross-validation helps us understand the model’s performance more robustly by testing it on multiple splits of the data. This approach gives a more reliable estimate of model performance and helps reduce the risk of overfitting to any single train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "Epoch 1/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 76ms/step - accuracy: 0.8040 - loss: 2.0129 - val_accuracy: 0.9501 - val_loss: 0.6970\n",
      "Epoch 2/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 75ms/step - accuracy: 0.9505 - loss: 0.5997 - val_accuracy: 0.9655 - val_loss: 0.3715\n",
      "Epoch 3/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 76ms/step - accuracy: 0.9633 - loss: 0.3513 - val_accuracy: 0.9683 - val_loss: 0.2660\n",
      "Epoch 4/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 75ms/step - accuracy: 0.9674 - loss: 0.2568 - val_accuracy: 0.9705 - val_loss: 0.2148\n",
      "Epoch 5/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 74ms/step - accuracy: 0.9721 - loss: 0.2070 - val_accuracy: 0.9722 - val_loss: 0.1870\n",
      "Epoch 6/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 74ms/step - accuracy: 0.9755 - loss: 0.1797 - val_accuracy: 0.9715 - val_loss: 0.1752\n",
      "Epoch 7/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 74ms/step - accuracy: 0.9776 - loss: 0.1650 - val_accuracy: 0.9745 - val_loss: 0.1635\n",
      "Epoch 8/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 74ms/step - accuracy: 0.9773 - loss: 0.1558 - val_accuracy: 0.9739 - val_loss: 0.1595\n",
      "Epoch 9/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - accuracy: 0.9807 - loss: 0.1484 - val_accuracy: 0.9711 - val_loss: 0.1612\n",
      "Epoch 10/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 74ms/step - accuracy: 0.9808 - loss: 0.1426 - val_accuracy: 0.9674 - val_loss: 0.1636\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "\n",
      "Fold 1 Results:\n",
      "Accuracy: 0.9674\n",
      "Precision: 0.9826\n",
      "Recall: 0.9439\n",
      "F1 Score: 0.9629\n",
      "\n",
      "Fold 2\n",
      "Epoch 1/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 74ms/step - accuracy: 0.8064 - loss: 1.9683 - val_accuracy: 0.9534 - val_loss: 0.6195\n",
      "Epoch 2/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 74ms/step - accuracy: 0.9519 - loss: 0.5338 - val_accuracy: 0.9642 - val_loss: 0.3431\n",
      "Epoch 3/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - accuracy: 0.9626 - loss: 0.3254 - val_accuracy: 0.9577 - val_loss: 0.2782\n",
      "Epoch 4/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 75ms/step - accuracy: 0.9676 - loss: 0.2482 - val_accuracy: 0.9630 - val_loss: 0.2278\n",
      "Epoch 5/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - accuracy: 0.9716 - loss: 0.2048 - val_accuracy: 0.9701 - val_loss: 0.1918\n",
      "Epoch 6/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - accuracy: 0.9732 - loss: 0.1791 - val_accuracy: 0.9705 - val_loss: 0.1752\n",
      "Epoch 7/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 74ms/step - accuracy: 0.9760 - loss: 0.1639 - val_accuracy: 0.9705 - val_loss: 0.1725\n",
      "Epoch 8/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 74ms/step - accuracy: 0.9772 - loss: 0.1551 - val_accuracy: 0.9712 - val_loss: 0.1617\n",
      "Epoch 9/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - accuracy: 0.9789 - loss: 0.1488 - val_accuracy: 0.9735 - val_loss: 0.1556\n",
      "Epoch 10/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - accuracy: 0.9800 - loss: 0.1447 - val_accuracy: 0.9727 - val_loss: 0.1558\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "Fold 2 Results:\n",
      "Accuracy: 0.9727\n",
      "Precision: 0.9598\n",
      "Recall: 0.9801\n",
      "F1 Score: 0.9699\n",
      "\n",
      "Fold 3\n",
      "Epoch 1/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 73ms/step - accuracy: 0.7777 - loss: 2.2320 - val_accuracy: 0.9485 - val_loss: 0.7393\n",
      "Epoch 2/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - accuracy: 0.9460 - loss: 0.6426 - val_accuracy: 0.9626 - val_loss: 0.4069\n",
      "Epoch 3/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - accuracy: 0.9607 - loss: 0.3846 - val_accuracy: 0.9653 - val_loss: 0.2976\n",
      "Epoch 4/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - accuracy: 0.9648 - loss: 0.2909 - val_accuracy: 0.9679 - val_loss: 0.2390\n",
      "Epoch 5/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - accuracy: 0.9712 - loss: 0.2318 - val_accuracy: 0.9710 - val_loss: 0.2039\n",
      "Epoch 6/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - accuracy: 0.9726 - loss: 0.1992 - val_accuracy: 0.9706 - val_loss: 0.1847\n",
      "Epoch 7/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - accuracy: 0.9759 - loss: 0.1772 - val_accuracy: 0.9716 - val_loss: 0.1717\n",
      "Epoch 8/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - accuracy: 0.9770 - loss: 0.1655 - val_accuracy: 0.9729 - val_loss: 0.1640\n",
      "Epoch 9/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - accuracy: 0.9781 - loss: 0.1562 - val_accuracy: 0.9735 - val_loss: 0.1575\n",
      "Epoch 10/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - accuracy: 0.9786 - loss: 0.1511 - val_accuracy: 0.9735 - val_loss: 0.1536\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\n",
      "Fold 3 Results:\n",
      "Accuracy: 0.9735\n",
      "Precision: 0.9799\n",
      "Recall: 0.9604\n",
      "F1 Score: 0.9701\n",
      "\n",
      "Fold 4\n",
      "Epoch 1/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 75ms/step - accuracy: 0.7962 - loss: 2.1022 - val_accuracy: 0.9476 - val_loss: 0.7367\n",
      "Epoch 2/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 75ms/step - accuracy: 0.9479 - loss: 0.6359 - val_accuracy: 0.9619 - val_loss: 0.4123\n",
      "Epoch 3/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m607s\u001b[0m 3s/step - accuracy: 0.9605 - loss: 0.3828 - val_accuracy: 0.9673 - val_loss: 0.2950\n",
      "Epoch 4/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 68ms/step - accuracy: 0.9676 - loss: 0.2813 - val_accuracy: 0.9719 - val_loss: 0.2340\n",
      "Epoch 5/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 68ms/step - accuracy: 0.9729 - loss: 0.2241 - val_accuracy: 0.9736 - val_loss: 0.2005\n",
      "Epoch 6/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.9745 - loss: 0.1921 - val_accuracy: 0.9735 - val_loss: 0.1824\n",
      "Epoch 7/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.9766 - loss: 0.1720 - val_accuracy: 0.9743 - val_loss: 0.1695\n",
      "Epoch 8/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.9771 - loss: 0.1595 - val_accuracy: 0.9747 - val_loss: 0.1633\n",
      "Epoch 9/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.9775 - loss: 0.1543 - val_accuracy: 0.9754 - val_loss: 0.1565\n",
      "Epoch 10/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.9810 - loss: 0.1446 - val_accuracy: 0.9755 - val_loss: 0.1525\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "Fold 4 Results:\n",
      "Accuracy: 0.9755\n",
      "Precision: 0.9762\n",
      "Recall: 0.9689\n",
      "F1 Score: 0.9726\n",
      "\n",
      "Fold 5\n",
      "Epoch 1/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.7961 - loss: 2.2115 - val_accuracy: 0.9484 - val_loss: 0.7954\n",
      "Epoch 2/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.9485 - loss: 0.6884 - val_accuracy: 0.9614 - val_loss: 0.4386\n",
      "Epoch 3/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.9608 - loss: 0.4136 - val_accuracy: 0.9642 - val_loss: 0.3198\n",
      "Epoch 4/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.9677 - loss: 0.3066 - val_accuracy: 0.9685 - val_loss: 0.2538\n",
      "Epoch 5/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.9704 - loss: 0.2446 - val_accuracy: 0.9696 - val_loss: 0.2149\n",
      "Epoch 6/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.9738 - loss: 0.2050 - val_accuracy: 0.9696 - val_loss: 0.1897\n",
      "Epoch 7/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.9761 - loss: 0.1798 - val_accuracy: 0.9711 - val_loss: 0.1762\n",
      "Epoch 8/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.9772 - loss: 0.1662 - val_accuracy: 0.9665 - val_loss: 0.1780\n",
      "Epoch 9/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.9770 - loss: 0.1570 - val_accuracy: 0.9635 - val_loss: 0.1792\n",
      "Epoch 10/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.9784 - loss: 0.1507 - val_accuracy: 0.9672 - val_loss: 0.1680\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "Fold 5 Results:\n",
      "Accuracy: 0.9672\n",
      "Precision: 0.9457\n",
      "Recall: 0.9831\n",
      "F1 Score: 0.9640\n",
      "\n",
      "Average Metrics Across All Folds:\n",
      "Average Accuracy: 0.9713\n",
      "Average Precision: 0.9689\n",
      "Average Recall: 0.9673\n",
      "Average F1 Score: 0.9679\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import random\n",
    "\n",
    "# Set seeds\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Parameters\n",
    "max_words = 5000\n",
    "max_sequence_length = 300\n",
    "embedding_dim = 100\n",
    "\n",
    "def create_embedding_matrix(word2vec_model, tokenizer, vocab_size, embedding_dim):\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if i < vocab_size:\n",
    "            if word in word2vec_model.wv:\n",
    "                embedding_matrix[i] = word2vec_model.wv[word]\n",
    "            else:\n",
    "                embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
    "    return embedding_matrix\n",
    "\n",
    "def create_cnn_with_l2(vocab_size, embedding_dim, embedding_matrix):\n",
    "    input_layer = Input(shape=(max_sequence_length,))\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=True\n",
    "    )(input_layer)\n",
    "    \n",
    "    x = Conv1D(filters=128, kernel_size=5, activation='relu', kernel_regularizer=l2(0.01))(embedding_layer)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01))(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Implement cross-validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(data['processed_full_content'], data['label']), 1):\n",
    "    print(f\"\\nFold {fold}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_texts = data['processed_full_content'].iloc[train_idx]\n",
    "    val_texts = data['processed_full_content'].iloc[val_idx]\n",
    "    train_labels = data['label'].iloc[train_idx]\n",
    "    val_labels = data['label'].iloc[val_idx]\n",
    "    \n",
    "    # Tokenization\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train = pad_sequences(tokenizer.texts_to_sequences(train_texts), maxlen=max_sequence_length)\n",
    "    X_val = pad_sequences(tokenizer.texts_to_sequences(val_texts), maxlen=max_sequence_length)\n",
    "    \n",
    "    # Train Word2Vec on training data only\n",
    "    train_sentences = [text.split() for text in train_texts]\n",
    "    word2vec_model = Word2Vec(train_sentences, vector_size=embedding_dim, window=5, min_count=2, workers=4)\n",
    "    \n",
    "    # Create embedding matrix\n",
    "    vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
    "    embedding_matrix = create_embedding_matrix(word2vec_model, tokenizer, vocab_size, embedding_dim)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_cnn_with_l2(vocab_size, embedding_dim, embedding_matrix)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, train_labels,\n",
    "        epochs=10,\n",
    "        batch_size=256,\n",
    "        validation_data=(X_val, val_labels),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = (model.predict(X_val) > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(val_labels, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(val_labels, y_pred, average='binary')\n",
    "    \n",
    "    fold_metrics.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nFold {fold} Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Calculate and display average metrics\n",
    "avg_metrics = {\n",
    "    'accuracy': np.mean([m['accuracy'] for m in fold_metrics]),\n",
    "    'precision': np.mean([m['precision'] for m in fold_metrics]),\n",
    "    'recall': np.mean([m['recall'] for m in fold_metrics]),\n",
    "    'f1': np.mean([m['f1'] for m in fold_metrics])\n",
    "}\n",
    "\n",
    "print(\"\\nAverage Metrics Across All Folds:\")\n",
    "print(f\"Average Accuracy: {avg_metrics['accuracy']:.4f}\")\n",
    "print(f\"Average Precision: {avg_metrics['precision']:.4f}\")\n",
    "print(f\"Average Recall: {avg_metrics['recall']:.4f}\")\n",
    "print(f\"Average F1 Score: {avg_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural network + GloVe word embeddings (100D) + 5-Fold Cross Validation + L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n",
      "Loaded 400000 word vectors.\n",
      "\n",
      "Fold 1\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inflaton/anaconda3/envs/fake-news/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - accuracy: 0.7694 - loss: 2.4560 - val_accuracy: 0.9422 - val_loss: 0.6089\n",
      "Epoch 2/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - accuracy: 0.9420 - loss: 0.5295 - val_accuracy: 0.9546 - val_loss: 0.3673\n",
      "Epoch 3/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - accuracy: 0.9525 - loss: 0.3485 - val_accuracy: 0.9584 - val_loss: 0.2968\n",
      "Epoch 4/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9548 - loss: 0.2914 - val_accuracy: 0.9581 - val_loss: 0.2729\n",
      "Epoch 5/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - accuracy: 0.9572 - loss: 0.2721 - val_accuracy: 0.9594 - val_loss: 0.2647\n",
      "Epoch 6/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - accuracy: 0.9578 - loss: 0.2641 - val_accuracy: 0.9597 - val_loss: 0.2611\n",
      "Epoch 7/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - accuracy: 0.9578 - loss: 0.2608 - val_accuracy: 0.9588 - val_loss: 0.2609\n",
      "Epoch 8/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9582 - loss: 0.2590 - val_accuracy: 0.9604 - val_loss: 0.2581\n",
      "Epoch 9/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9605 - loss: 0.2574 - val_accuracy: 0.9589 - val_loss: 0.2584\n",
      "Epoch 10/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9604 - loss: 0.2560 - val_accuracy: 0.9594 - val_loss: 0.2579\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "Fold 1 Results:\n",
      "Accuracy: 0.9594\n",
      "Precision: 0.9604\n",
      "Recall: 0.9483\n",
      "F1-score: 0.9544\n",
      "\n",
      "Fold 2\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inflaton/anaconda3/envs/fake-news/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - accuracy: 0.7632 - loss: 2.4318 - val_accuracy: 0.9383 - val_loss: 0.5954\n",
      "Epoch 2/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 54ms/step - accuracy: 0.9393 - loss: 0.5210 - val_accuracy: 0.9492 - val_loss: 0.3738\n",
      "Epoch 3/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 54ms/step - accuracy: 0.9507 - loss: 0.3518 - val_accuracy: 0.9510 - val_loss: 0.3071\n",
      "Epoch 4/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 56ms/step - accuracy: 0.9555 - loss: 0.2956 - val_accuracy: 0.9541 - val_loss: 0.2822\n",
      "Epoch 5/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - accuracy: 0.9564 - loss: 0.2743 - val_accuracy: 0.9513 - val_loss: 0.2774\n",
      "Epoch 6/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - accuracy: 0.9590 - loss: 0.2639 - val_accuracy: 0.9510 - val_loss: 0.2747\n",
      "Epoch 7/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - accuracy: 0.9588 - loss: 0.2608 - val_accuracy: 0.9433 - val_loss: 0.2903\n",
      "Epoch 8/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9576 - loss: 0.2605 - val_accuracy: 0.9483 - val_loss: 0.2786\n",
      "Epoch 9/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.9571 - loss: 0.2589 - val_accuracy: 0.9505 - val_loss: 0.2748\n",
      "Epoch 10/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.9587 - loss: 0.2574 - val_accuracy: 0.9509 - val_loss: 0.2736\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "Fold 2 Results:\n",
      "Accuracy: 0.9509\n",
      "Precision: 0.9147\n",
      "Recall: 0.9819\n",
      "F1-score: 0.9471\n",
      "\n",
      "Fold 3\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inflaton/anaconda3/envs/fake-news/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - accuracy: 0.7281 - loss: 2.4923 - val_accuracy: 0.9386 - val_loss: 0.5944\n",
      "Epoch 2/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - accuracy: 0.9365 - loss: 0.5256 - val_accuracy: 0.9490 - val_loss: 0.3722\n",
      "Epoch 3/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 56ms/step - accuracy: 0.9500 - loss: 0.3552 - val_accuracy: 0.9518 - val_loss: 0.3046\n",
      "Epoch 4/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9549 - loss: 0.2980 - val_accuracy: 0.9527 - val_loss: 0.2796\n",
      "Epoch 5/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - accuracy: 0.9564 - loss: 0.2772 - val_accuracy: 0.9533 - val_loss: 0.2701\n",
      "Epoch 6/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - accuracy: 0.9570 - loss: 0.2672 - val_accuracy: 0.9558 - val_loss: 0.2617\n",
      "Epoch 7/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.9574 - loss: 0.2624 - val_accuracy: 0.9565 - val_loss: 0.2601\n",
      "Epoch 8/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9581 - loss: 0.2609 - val_accuracy: 0.9577 - val_loss: 0.2567\n",
      "Epoch 9/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9593 - loss: 0.2582 - val_accuracy: 0.9581 - val_loss: 0.2558\n",
      "Epoch 10/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9594 - loss: 0.2571 - val_accuracy: 0.9588 - val_loss: 0.2552\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "Fold 3 Results:\n",
      "Accuracy: 0.9588\n",
      "Precision: 0.9469\n",
      "Recall: 0.9620\n",
      "F1-score: 0.9544\n",
      "\n",
      "Fold 4\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inflaton/anaconda3/envs/fake-news/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.7584 - loss: 2.3604 - val_accuracy: 0.9432 - val_loss: 0.5397\n",
      "Epoch 2/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9415 - loss: 0.4803 - val_accuracy: 0.9536 - val_loss: 0.3378\n",
      "Epoch 3/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9533 - loss: 0.3266 - val_accuracy: 0.9564 - val_loss: 0.2826\n",
      "Epoch 4/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9559 - loss: 0.2827 - val_accuracy: 0.9572 - val_loss: 0.2646\n",
      "Epoch 5/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9559 - loss: 0.2684 - val_accuracy: 0.9590 - val_loss: 0.2580\n",
      "Epoch 6/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9575 - loss: 0.2624 - val_accuracy: 0.9587 - val_loss: 0.2551\n",
      "Epoch 7/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9584 - loss: 0.2590 - val_accuracy: 0.9594 - val_loss: 0.2531\n",
      "Epoch 8/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 68ms/step - accuracy: 0.9595 - loss: 0.2578 - val_accuracy: 0.9604 - val_loss: 0.2527\n",
      "Epoch 9/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.9596 - loss: 0.2554 - val_accuracy: 0.9600 - val_loss: 0.2518\n",
      "Epoch 10/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9595 - loss: 0.2556 - val_accuracy: 0.9588 - val_loss: 0.2547\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "Fold 4 Results:\n",
      "Accuracy: 0.9588\n",
      "Precision: 0.9406\n",
      "Recall: 0.9693\n",
      "F1-score: 0.9547\n",
      "\n",
      "Fold 5\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inflaton/anaconda3/envs/fake-news/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.7681 - loss: 2.3442 - val_accuracy: 0.9337 - val_loss: 0.5304\n",
      "Epoch 2/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9394 - loss: 0.4653 - val_accuracy: 0.9522 - val_loss: 0.3369\n",
      "Epoch 3/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - accuracy: 0.9511 - loss: 0.3278 - val_accuracy: 0.9571 - val_loss: 0.2850\n",
      "Epoch 4/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9545 - loss: 0.2860 - val_accuracy: 0.9587 - val_loss: 0.2664\n",
      "Epoch 5/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9571 - loss: 0.2699 - val_accuracy: 0.9592 - val_loss: 0.2594\n",
      "Epoch 6/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9572 - loss: 0.2618 - val_accuracy: 0.9600 - val_loss: 0.2567\n",
      "Epoch 7/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.9589 - loss: 0.2590 - val_accuracy: 0.9588 - val_loss: 0.2550\n",
      "Epoch 8/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9575 - loss: 0.2589 - val_accuracy: 0.9592 - val_loss: 0.2547\n",
      "Epoch 9/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9598 - loss: 0.2551 - val_accuracy: 0.9596 - val_loss: 0.2535\n",
      "Epoch 10/10\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9604 - loss: 0.2555 - val_accuracy: 0.9590 - val_loss: 0.2535\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "Fold 5 Results:\n",
      "Accuracy: 0.9590\n",
      "Precision: 0.9561\n",
      "Recall: 0.9521\n",
      "F1-score: 0.9541\n",
      "\n",
      "Average Metrics Across All Folds:\n",
      "Accuracy: 0.9574\n",
      "Precision: 0.9437\n",
      "Recall: 0.9627\n",
      "F1: 0.9529\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Concatenate, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Set seeds\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Load GloVe embeddings once (this doesn't cause data leakage)\n",
    "def load_glove_embeddings(path, embedding_dim=100):\n",
    "    print(\"Loading GloVe embeddings...\")\n",
    "    embeddings_index = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print(f\"Loaded {len(embeddings_index)} word vectors.\")\n",
    "    return embeddings_index\n",
    "\n",
    "def create_embedding_matrix(word_index, embeddings_index, vocab_size, embedding_dim):\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= vocab_size:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "def create_model(vocab_size, embedding_matrix, max_sequence_length):\n",
    "    input_layer = Input(shape=(max_sequence_length,))\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_matrix.shape[1],\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False,  # Set to False for pre-trained embeddings\n",
    "        input_length=max_sequence_length\n",
    "    )(input_layer)\n",
    "\n",
    "    convs = []\n",
    "    for kernel_size in [3, 4, 5]:\n",
    "        conv = Conv1D(\n",
    "            filters=64,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(0.01)\n",
    "        )(embedding_layer)\n",
    "        pool = GlobalMaxPooling1D()(conv)\n",
    "        convs.append(pool)\n",
    "\n",
    "    merged = Concatenate()(convs)\n",
    "    dense = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(merged)\n",
    "    drop = Dropout(0.2)(dense)\n",
    "    output = Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01))(drop)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def process_fold_data(train_texts, val_texts, tokenizer, max_sequence_length):\n",
    "    \"\"\"Process text data for a single fold\"\"\"\n",
    "    # Fit tokenizer on training data only\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "    \n",
    "    # Convert texts to sequences\n",
    "    X_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    X_val = tokenizer.texts_to_sequences(val_texts)\n",
    "    \n",
    "    # Pad sequences\n",
    "    X_train = pad_sequences(X_train, maxlen=max_sequence_length)\n",
    "    X_val = pad_sequences(X_val, maxlen=max_sequence_length)\n",
    "    \n",
    "    return X_train, X_val, tokenizer\n",
    "\n",
    "def main():\n",
    "    # Parameters\n",
    "    max_sequence_length = 300\n",
    "    vocab_size = 5000\n",
    "    embedding_dim = 100\n",
    "    \n",
    "    # Load GloVe embeddings\n",
    "    glove_path = './glove.6B.100d.txt'\n",
    "    embeddings_index = load_glove_embeddings(glove_path, embedding_dim)\n",
    "    \n",
    "    # Prepare for cross-validation\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_metrics = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(data['processed_full_content'], data['label']), 1):\n",
    "        print(f\"\\nFold {fold}\")\n",
    "        \n",
    "        # Split data\n",
    "        train_texts = data['processed_full_content'].iloc[train_idx]\n",
    "        val_texts = data['processed_full_content'].iloc[val_idx]\n",
    "        y_train = data['label'].iloc[train_idx].values\n",
    "        y_val = data['label'].iloc[val_idx].values\n",
    "        \n",
    "        # Initialize new tokenizer for each fold\n",
    "        tokenizer = Tokenizer(num_words=vocab_size)\n",
    "        \n",
    "        # Process data for this fold\n",
    "        X_train, X_val, tokenizer = process_fold_data(\n",
    "            train_texts, val_texts, tokenizer, max_sequence_length\n",
    "        )\n",
    "        \n",
    "        # Create embedding matrix for this fold's vocabulary\n",
    "        embedding_matrix = create_embedding_matrix(\n",
    "            tokenizer.word_index, embeddings_index, vocab_size, embedding_dim\n",
    "        )\n",
    "        \n",
    "        # Compute class weights\n",
    "        class_weights = compute_class_weight('balanced', \n",
    "                                          classes=np.unique(y_train), \n",
    "                                          y=y_train)\n",
    "        class_weights_dict = dict(enumerate(class_weights))\n",
    "        \n",
    "        # Create and train model\n",
    "        model = create_model(vocab_size, embedding_matrix, max_sequence_length)\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=10,\n",
    "            batch_size=256,\n",
    "            validation_data=(X_val, y_val),\n",
    "            class_weight=class_weights_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = (model.predict(X_val) > 0.5).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='binary')\n",
    "        \n",
    "        fold_metrics.append({\n",
    "            'fold': fold,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nFold {fold} Results:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-score: {f1:.4f}\")\n",
    "    \n",
    "    # Calculate and print average metrics\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean([m['accuracy'] for m in fold_metrics]),\n",
    "        'precision': np.mean([m['precision'] for m in fold_metrics]),\n",
    "        'recall': np.mean([m['recall'] for m in fold_metrics]),\n",
    "        'f1': np.mean([m['f1'] for m in fold_metrics])\n",
    "    }\n",
    "    \n",
    "    print(\"\\nAverage Metrics Across All Folds:\")\n",
    "    for metric, value in avg_metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural network + Custom-trained word2vec word embeddings + 5-Fold Cross Validation + L2 Regularization + GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing filters=64, dropout_rate=0.2\n",
      "\n",
      "Fold 1\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.8760 - loss: 1.0972 - val_accuracy: 0.9591 - val_loss: 0.2982\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9594 - loss: 0.2668 - val_accuracy: 0.9677 - val_loss: 0.1964\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9680 - loss: 0.1878 - val_accuracy: 0.9695 - val_loss: 0.1770\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9720 - loss: 0.1661 - val_accuracy: 0.9706 - val_loss: 0.1670\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9752 - loss: 0.1529 - val_accuracy: 0.9702 - val_loss: 0.1603\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9785 - loss: 0.1420 - val_accuracy: 0.9688 - val_loss: 0.1595\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.9805 - loss: 0.1355 - val_accuracy: 0.9700 - val_loss: 0.1553\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9822 - loss: 0.1294 - val_accuracy: 0.9721 - val_loss: 0.1496\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9849 - loss: 0.1221 - val_accuracy: 0.9739 - val_loss: 0.1449\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9868 - loss: 0.1170 - val_accuracy: 0.9726 - val_loss: 0.1449\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 1 F1-score: 0.9692\n",
      "\n",
      "Fold 2\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.8779 - loss: 1.1304 - val_accuracy: 0.9608 - val_loss: 0.2950\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9597 - loss: 0.2617 - val_accuracy: 0.9672 - val_loss: 0.1883\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9678 - loss: 0.1833 - val_accuracy: 0.9673 - val_loss: 0.1717\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9721 - loss: 0.1641 - val_accuracy: 0.9695 - val_loss: 0.1618\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9761 - loss: 0.1516 - val_accuracy: 0.9722 - val_loss: 0.1543\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9782 - loss: 0.1424 - val_accuracy: 0.9714 - val_loss: 0.1491\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9809 - loss: 0.1340 - val_accuracy: 0.9731 - val_loss: 0.1459\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9837 - loss: 0.1259 - val_accuracy: 0.9720 - val_loss: 0.1488\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9847 - loss: 0.1213 - val_accuracy: 0.9720 - val_loss: 0.1460\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9875 - loss: 0.1157 - val_accuracy: 0.9730 - val_loss: 0.1439\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 2 F1-score: 0.9702\n",
      "\n",
      "Fold 3\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.8616 - loss: 1.1818 - val_accuracy: 0.9498 - val_loss: 0.3362\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9577 - loss: 0.2895 - val_accuracy: 0.9565 - val_loss: 0.2169\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9660 - loss: 0.1981 - val_accuracy: 0.9585 - val_loss: 0.1885\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9705 - loss: 0.1684 - val_accuracy: 0.9638 - val_loss: 0.1704\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9751 - loss: 0.1533 - val_accuracy: 0.9700 - val_loss: 0.1572\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9772 - loss: 0.1417 - val_accuracy: 0.9700 - val_loss: 0.1512\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9801 - loss: 0.1334 - val_accuracy: 0.9720 - val_loss: 0.1474\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9828 - loss: 0.1255 - val_accuracy: 0.9708 - val_loss: 0.1470\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9857 - loss: 0.1184 - val_accuracy: 0.9698 - val_loss: 0.1472\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9876 - loss: 0.1120 - val_accuracy: 0.9712 - val_loss: 0.1454\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 3 F1-score: 0.9681\n",
      "\n",
      "Fold 4\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.8722 - loss: 1.1695 - val_accuracy: 0.9606 - val_loss: 0.3318\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9584 - loss: 0.2927 - val_accuracy: 0.9641 - val_loss: 0.2125\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9647 - loss: 0.2006 - val_accuracy: 0.9605 - val_loss: 0.1938\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9690 - loss: 0.1720 - val_accuracy: 0.9668 - val_loss: 0.1756\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9728 - loss: 0.1577 - val_accuracy: 0.9695 - val_loss: 0.1655\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9755 - loss: 0.1475 - val_accuracy: 0.9705 - val_loss: 0.1611\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9792 - loss: 0.1377 - val_accuracy: 0.9700 - val_loss: 0.1588\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9817 - loss: 0.1307 - val_accuracy: 0.9718 - val_loss: 0.1531\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9840 - loss: 0.1246 - val_accuracy: 0.9739 - val_loss: 0.1458\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9865 - loss: 0.1176 - val_accuracy: 0.9732 - val_loss: 0.1447\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 4 F1-score: 0.9700\n",
      "\n",
      "Fold 5\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.8910 - loss: 1.0160 - val_accuracy: 0.9577 - val_loss: 0.2607\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9594 - loss: 0.2362 - val_accuracy: 0.9575 - val_loss: 0.2000\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9663 - loss: 0.1807 - val_accuracy: 0.9613 - val_loss: 0.1790\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9707 - loss: 0.1619 - val_accuracy: 0.9689 - val_loss: 0.1592\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9755 - loss: 0.1480 - val_accuracy: 0.9702 - val_loss: 0.1536\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9781 - loss: 0.1384 - val_accuracy: 0.9725 - val_loss: 0.1496\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9809 - loss: 0.1323 - val_accuracy: 0.9718 - val_loss: 0.1462\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9843 - loss: 0.1250 - val_accuracy: 0.9727 - val_loss: 0.1438\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9861 - loss: 0.1183 - val_accuracy: 0.9727 - val_loss: 0.1427\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9878 - loss: 0.1135 - val_accuracy: 0.9720 - val_loss: 0.1422\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 5 F1-score: 0.9685\n",
      "Average F1-score: 0.9692\n",
      "\n",
      "Testing filters=64, dropout_rate=0.3\n",
      "\n",
      "Fold 1\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.8689 - loss: 1.0871 - val_accuracy: 0.9588 - val_loss: 0.2940\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9594 - loss: 0.2650 - val_accuracy: 0.9669 - val_loss: 0.2009\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9645 - loss: 0.1961 - val_accuracy: 0.9670 - val_loss: 0.1767\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9701 - loss: 0.1710 - val_accuracy: 0.9716 - val_loss: 0.1614\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9754 - loss: 0.1534 - val_accuracy: 0.9691 - val_loss: 0.1631\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9773 - loss: 0.1451 - val_accuracy: 0.9732 - val_loss: 0.1514\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9810 - loss: 0.1344 - val_accuracy: 0.9722 - val_loss: 0.1483\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9827 - loss: 0.1288 - val_accuracy: 0.9729 - val_loss: 0.1457\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9845 - loss: 0.1222 - val_accuracy: 0.9721 - val_loss: 0.1468\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9872 - loss: 0.1169 - val_accuracy: 0.9658 - val_loss: 0.1576\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 1 F1-score: 0.9610\n",
      "\n",
      "Fold 2\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.8699 - loss: 1.0718 - val_accuracy: 0.9615 - val_loss: 0.2709\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9601 - loss: 0.2505 - val_accuracy: 0.9656 - val_loss: 0.1964\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9666 - loss: 0.1879 - val_accuracy: 0.9684 - val_loss: 0.1699\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9712 - loss: 0.1658 - val_accuracy: 0.9704 - val_loss: 0.1612\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9756 - loss: 0.1533 - val_accuracy: 0.9709 - val_loss: 0.1574\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9773 - loss: 0.1448 - val_accuracy: 0.9716 - val_loss: 0.1556\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9807 - loss: 0.1350 - val_accuracy: 0.9725 - val_loss: 0.1515\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9835 - loss: 0.1274 - val_accuracy: 0.9728 - val_loss: 0.1493\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9853 - loss: 0.1220 - val_accuracy: 0.9740 - val_loss: 0.1456\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9872 - loss: 0.1176 - val_accuracy: 0.9735 - val_loss: 0.1461\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 2 F1-score: 0.9707\n",
      "\n",
      "Fold 3\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.8762 - loss: 1.0640 - val_accuracy: 0.9557 - val_loss: 0.2733\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9587 - loss: 0.2479 - val_accuracy: 0.9584 - val_loss: 0.2032\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9673 - loss: 0.1860 - val_accuracy: 0.9635 - val_loss: 0.1797\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9693 - loss: 0.1684 - val_accuracy: 0.9655 - val_loss: 0.1664\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9740 - loss: 0.1543 - val_accuracy: 0.9694 - val_loss: 0.1563\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9773 - loss: 0.1434 - val_accuracy: 0.9677 - val_loss: 0.1557\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9805 - loss: 0.1347 - val_accuracy: 0.9699 - val_loss: 0.1499\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9827 - loss: 0.1269 - val_accuracy: 0.9690 - val_loss: 0.1491\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9852 - loss: 0.1208 - val_accuracy: 0.9698 - val_loss: 0.1470\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9864 - loss: 0.1157 - val_accuracy: 0.9701 - val_loss: 0.1449\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 3 F1-score: 0.9669\n",
      "\n",
      "Fold 4\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.8543 - loss: 1.1150 - val_accuracy: 0.9582 - val_loss: 0.2911\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9576 - loss: 0.2618 - val_accuracy: 0.9544 - val_loss: 0.2253\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9634 - loss: 0.1949 - val_accuracy: 0.9556 - val_loss: 0.2027\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9703 - loss: 0.1666 - val_accuracy: 0.9615 - val_loss: 0.1816\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9738 - loss: 0.1553 - val_accuracy: 0.9651 - val_loss: 0.1756\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9776 - loss: 0.1441 - val_accuracy: 0.9673 - val_loss: 0.1647\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9811 - loss: 0.1353 - val_accuracy: 0.9704 - val_loss: 0.1577\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9828 - loss: 0.1285 - val_accuracy: 0.9704 - val_loss: 0.1528\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9848 - loss: 0.1216 - val_accuracy: 0.9721 - val_loss: 0.1500\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9878 - loss: 0.1151 - val_accuracy: 0.9727 - val_loss: 0.1468\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 4 F1-score: 0.9696\n",
      "\n",
      "Fold 5\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.8656 - loss: 1.1900 - val_accuracy: 0.9587 - val_loss: 0.3174\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9586 - loss: 0.2856 - val_accuracy: 0.9580 - val_loss: 0.2181\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9660 - loss: 0.1997 - val_accuracy: 0.9652 - val_loss: 0.1793\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9692 - loss: 0.1725 - val_accuracy: 0.9686 - val_loss: 0.1644\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9741 - loss: 0.1577 - val_accuracy: 0.9683 - val_loss: 0.1591\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9765 - loss: 0.1465 - val_accuracy: 0.9713 - val_loss: 0.1499\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9799 - loss: 0.1370 - val_accuracy: 0.9735 - val_loss: 0.1496\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9820 - loss: 0.1297 - val_accuracy: 0.9723 - val_loss: 0.1449\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9840 - loss: 0.1235 - val_accuracy: 0.9738 - val_loss: 0.1427\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9863 - loss: 0.1173 - val_accuracy: 0.9735 - val_loss: 0.1432\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 5 F1-score: 0.9701\n",
      "Average F1-score: 0.9676\n",
      "\n",
      "Testing filters=64, dropout_rate=0.4\n",
      "\n",
      "Fold 1\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.8404 - loss: 1.2926 - val_accuracy: 0.9588 - val_loss: 0.3280\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9569 - loss: 0.2997 - val_accuracy: 0.9656 - val_loss: 0.2125\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9653 - loss: 0.2032 - val_accuracy: 0.9673 - val_loss: 0.1808\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9711 - loss: 0.1721 - val_accuracy: 0.9705 - val_loss: 0.1645\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9749 - loss: 0.1552 - val_accuracy: 0.9703 - val_loss: 0.1580\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9786 - loss: 0.1427 - val_accuracy: 0.9724 - val_loss: 0.1521\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9817 - loss: 0.1347 - val_accuracy: 0.9708 - val_loss: 0.1548\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9835 - loss: 0.1285 - val_accuracy: 0.9699 - val_loss: 0.1560\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9855 - loss: 0.1228 - val_accuracy: 0.9643 - val_loss: 0.1621\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9870 - loss: 0.1171 - val_accuracy: 0.9691 - val_loss: 0.1553\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 1 F1-score: 0.9649\n",
      "\n",
      "Fold 2\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.8549 - loss: 1.2908 - val_accuracy: 0.9579 - val_loss: 0.3544\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9550 - loss: 0.3141 - val_accuracy: 0.9649 - val_loss: 0.2129\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9653 - loss: 0.2063 - val_accuracy: 0.9677 - val_loss: 0.1799\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9695 - loss: 0.1747 - val_accuracy: 0.9689 - val_loss: 0.1663\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9735 - loss: 0.1607 - val_accuracy: 0.9700 - val_loss: 0.1639\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9769 - loss: 0.1505 - val_accuracy: 0.9726 - val_loss: 0.1572\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9788 - loss: 0.1422 - val_accuracy: 0.9721 - val_loss: 0.1528\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9811 - loss: 0.1348 - val_accuracy: 0.9705 - val_loss: 0.1584\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9838 - loss: 0.1270 - val_accuracy: 0.9710 - val_loss: 0.1535\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9853 - loss: 0.1232 - val_accuracy: 0.9732 - val_loss: 0.1478\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 2 F1-score: 0.9704\n",
      "\n",
      "Fold 3\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.8431 - loss: 1.2511 - val_accuracy: 0.9521 - val_loss: 0.3220\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9571 - loss: 0.2884 - val_accuracy: 0.9623 - val_loss: 0.2063\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9650 - loss: 0.2011 - val_accuracy: 0.9650 - val_loss: 0.1799\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9682 - loss: 0.1753 - val_accuracy: 0.9670 - val_loss: 0.1686\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9729 - loss: 0.1610 - val_accuracy: 0.9697 - val_loss: 0.1598\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9767 - loss: 0.1481 - val_accuracy: 0.9696 - val_loss: 0.1565\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9797 - loss: 0.1390 - val_accuracy: 0.9702 - val_loss: 0.1543\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9816 - loss: 0.1313 - val_accuracy: 0.9704 - val_loss: 0.1524\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9835 - loss: 0.1270 - val_accuracy: 0.9689 - val_loss: 0.1530\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9852 - loss: 0.1195 - val_accuracy: 0.9708 - val_loss: 0.1474\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 3 F1-score: 0.9675\n",
      "\n",
      "Fold 4\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.8658 - loss: 1.1634 - val_accuracy: 0.9548 - val_loss: 0.3191\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9555 - loss: 0.2852 - val_accuracy: 0.9529 - val_loss: 0.2343\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9628 - loss: 0.2037 - val_accuracy: 0.9550 - val_loss: 0.2043\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9688 - loss: 0.1751 - val_accuracy: 0.9691 - val_loss: 0.1716\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9734 - loss: 0.1588 - val_accuracy: 0.9702 - val_loss: 0.1625\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9758 - loss: 0.1493 - val_accuracy: 0.9664 - val_loss: 0.1651\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9797 - loss: 0.1409 - val_accuracy: 0.9700 - val_loss: 0.1560\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9810 - loss: 0.1362 - val_accuracy: 0.9696 - val_loss: 0.1555\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9833 - loss: 0.1276 - val_accuracy: 0.9724 - val_loss: 0.1511\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9854 - loss: 0.1223 - val_accuracy: 0.9740 - val_loss: 0.1484\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 4 F1-score: 0.9711\n",
      "\n",
      "Fold 5\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.8584 - loss: 1.1507 - val_accuracy: 0.9594 - val_loss: 0.2844\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9586 - loss: 0.2616 - val_accuracy: 0.9662 - val_loss: 0.1895\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9656 - loss: 0.1882 - val_accuracy: 0.9682 - val_loss: 0.1710\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9701 - loss: 0.1684 - val_accuracy: 0.9692 - val_loss: 0.1641\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9731 - loss: 0.1580 - val_accuracy: 0.9720 - val_loss: 0.1571\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9767 - loss: 0.1456 - val_accuracy: 0.9726 - val_loss: 0.1503\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9799 - loss: 0.1372 - val_accuracy: 0.9701 - val_loss: 0.1529\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9820 - loss: 0.1291 - val_accuracy: 0.9704 - val_loss: 0.1493\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9845 - loss: 0.1233 - val_accuracy: 0.9674 - val_loss: 0.1576\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9867 - loss: 0.1173 - val_accuracy: 0.9685 - val_loss: 0.1521\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 5 F1-score: 0.9641\n",
      "Average F1-score: 0.9676\n",
      "\n",
      "Testing filters=64, dropout_rate=0.5\n",
      "\n",
      "Fold 1\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.8540 - loss: 1.2190 - val_accuracy: 0.9608 - val_loss: 0.3098\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9579 - loss: 0.2839 - val_accuracy: 0.9666 - val_loss: 0.2029\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9654 - loss: 0.2023 - val_accuracy: 0.9623 - val_loss: 0.1921\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9686 - loss: 0.1764 - val_accuracy: 0.9669 - val_loss: 0.1770\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9736 - loss: 0.1624 - val_accuracy: 0.9706 - val_loss: 0.1654\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9762 - loss: 0.1519 - val_accuracy: 0.9726 - val_loss: 0.1551\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.9799 - loss: 0.1430 - val_accuracy: 0.9731 - val_loss: 0.1505\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.9807 - loss: 0.1373 - val_accuracy: 0.9720 - val_loss: 0.1524\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9824 - loss: 0.1304 - val_accuracy: 0.9727 - val_loss: 0.1479\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9845 - loss: 0.1254 - val_accuracy: 0.9658 - val_loss: 0.1603\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 1 F1-score: 0.9610\n",
      "\n",
      "Fold 2\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.8488 - loss: 1.3479 - val_accuracy: 0.9550 - val_loss: 0.3562\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9537 - loss: 0.3169 - val_accuracy: 0.9636 - val_loss: 0.2119\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.9612 - loss: 0.2124 - val_accuracy: 0.9669 - val_loss: 0.1806\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9681 - loss: 0.1801 - val_accuracy: 0.9677 - val_loss: 0.1718\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9712 - loss: 0.1671 - val_accuracy: 0.9697 - val_loss: 0.1618\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.9757 - loss: 0.1528 - val_accuracy: 0.9686 - val_loss: 0.1623\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9784 - loss: 0.1449 - val_accuracy: 0.9708 - val_loss: 0.1571\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9819 - loss: 0.1345 - val_accuracy: 0.9708 - val_loss: 0.1537\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9819 - loss: 0.1321 - val_accuracy: 0.9736 - val_loss: 0.1500\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9857 - loss: 0.1243 - val_accuracy: 0.9732 - val_loss: 0.1470\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 2 F1-score: 0.9703\n",
      "\n",
      "Fold 3\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.8322 - loss: 1.4235 - val_accuracy: 0.9565 - val_loss: 0.3779\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9555 - loss: 0.3401 - val_accuracy: 0.9616 - val_loss: 0.2250\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9641 - loss: 0.2163 - val_accuracy: 0.9654 - val_loss: 0.1849\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9679 - loss: 0.1809 - val_accuracy: 0.9673 - val_loss: 0.1715\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9730 - loss: 0.1622 - val_accuracy: 0.9681 - val_loss: 0.1628\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9769 - loss: 0.1503 - val_accuracy: 0.9694 - val_loss: 0.1568\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9793 - loss: 0.1423 - val_accuracy: 0.9677 - val_loss: 0.1582\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9815 - loss: 0.1344 - val_accuracy: 0.9673 - val_loss: 0.1566\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9831 - loss: 0.1294 - val_accuracy: 0.9686 - val_loss: 0.1535\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9856 - loss: 0.1226 - val_accuracy: 0.9693 - val_loss: 0.1510\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 3 F1-score: 0.9658\n",
      "\n",
      "Fold 4\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.8559 - loss: 1.2811 - val_accuracy: 0.9487 - val_loss: 0.3560\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9549 - loss: 0.3047 - val_accuracy: 0.9565 - val_loss: 0.2242\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9621 - loss: 0.2066 - val_accuracy: 0.9597 - val_loss: 0.1972\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9675 - loss: 0.1776 - val_accuracy: 0.9629 - val_loss: 0.1845\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9718 - loss: 0.1647 - val_accuracy: 0.9723 - val_loss: 0.1623\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9760 - loss: 0.1522 - val_accuracy: 0.9726 - val_loss: 0.1552\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9783 - loss: 0.1438 - val_accuracy: 0.9736 - val_loss: 0.1526\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9810 - loss: 0.1372 - val_accuracy: 0.9737 - val_loss: 0.1498\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9837 - loss: 0.1298 - val_accuracy: 0.9727 - val_loss: 0.1495\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9852 - loss: 0.1235 - val_accuracy: 0.9720 - val_loss: 0.1505\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 4 F1-score: 0.9683\n",
      "\n",
      "Fold 5\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.8597 - loss: 1.2386 - val_accuracy: 0.9584 - val_loss: 0.3130\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9561 - loss: 0.2873 - val_accuracy: 0.9630 - val_loss: 0.2060\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9635 - loss: 0.2046 - val_accuracy: 0.9651 - val_loss: 0.1775\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9683 - loss: 0.1772 - val_accuracy: 0.9667 - val_loss: 0.1689\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9731 - loss: 0.1627 - val_accuracy: 0.9697 - val_loss: 0.1615\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9756 - loss: 0.1515 - val_accuracy: 0.9712 - val_loss: 0.1555\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9794 - loss: 0.1418 - val_accuracy: 0.9685 - val_loss: 0.1593\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9812 - loss: 0.1361 - val_accuracy: 0.9715 - val_loss: 0.1504\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9834 - loss: 0.1302 - val_accuracy: 0.9690 - val_loss: 0.1524\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.9858 - loss: 0.1235 - val_accuracy: 0.9693 - val_loss: 0.1548\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 5 F1-score: 0.9651\n",
      "Average F1-score: 0.9661\n",
      "\n",
      "Testing filters=128, dropout_rate=0.2\n",
      "\n",
      "Fold 1\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8747 - loss: 1.4429 - val_accuracy: 0.9612 - val_loss: 0.3888\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9593 - loss: 0.3403 - val_accuracy: 0.9669 - val_loss: 0.2319\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9645 - loss: 0.2163 - val_accuracy: 0.9686 - val_loss: 0.1845\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9701 - loss: 0.1738 - val_accuracy: 0.9698 - val_loss: 0.1696\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9757 - loss: 0.1532 - val_accuracy: 0.9693 - val_loss: 0.1641\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9793 - loss: 0.1414 - val_accuracy: 0.9718 - val_loss: 0.1531\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9808 - loss: 0.1335 - val_accuracy: 0.9677 - val_loss: 0.1605\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9835 - loss: 0.1280 - val_accuracy: 0.9714 - val_loss: 0.1494\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9846 - loss: 0.1214 - val_accuracy: 0.9718 - val_loss: 0.1494\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9870 - loss: 0.1160 - val_accuracy: 0.9735 - val_loss: 0.1411\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 1 F1-score: 0.9703\n",
      "\n",
      "Fold 2\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.8752 - loss: 1.5228 - val_accuracy: 0.9587 - val_loss: 0.4054\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9608 - loss: 0.3489 - val_accuracy: 0.9654 - val_loss: 0.2266\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9664 - loss: 0.2140 - val_accuracy: 0.9662 - val_loss: 0.1859\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9702 - loss: 0.1739 - val_accuracy: 0.9700 - val_loss: 0.1648\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9736 - loss: 0.1571 - val_accuracy: 0.9730 - val_loss: 0.1546\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9766 - loss: 0.1471 - val_accuracy: 0.9743 - val_loss: 0.1501\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9800 - loss: 0.1378 - val_accuracy: 0.9734 - val_loss: 0.1470\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9817 - loss: 0.1304 - val_accuracy: 0.9731 - val_loss: 0.1473\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9830 - loss: 0.1260 - val_accuracy: 0.9727 - val_loss: 0.1463\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9862 - loss: 0.1185 - val_accuracy: 0.9739 - val_loss: 0.1447\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 2 F1-score: 0.9712\n",
      "\n",
      "Fold 3\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 194ms/step - accuracy: 0.8898 - loss: 1.3246 - val_accuracy: 0.9522 - val_loss: 0.3166\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.9577 - loss: 0.2756 - val_accuracy: 0.9563 - val_loss: 0.2112\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - accuracy: 0.9651 - loss: 0.1959 - val_accuracy: 0.9611 - val_loss: 0.1869\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m830s\u001b[0m 1s/step - accuracy: 0.9707 - loss: 0.1714 - val_accuracy: 0.9678 - val_loss: 0.1690\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9742 - loss: 0.1568 - val_accuracy: 0.9694 - val_loss: 0.1596\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9773 - loss: 0.1447 - val_accuracy: 0.9696 - val_loss: 0.1535\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m916s\u001b[0m 1s/step - accuracy: 0.9794 - loss: 0.1360 - val_accuracy: 0.9696 - val_loss: 0.1520\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9814 - loss: 0.1280 - val_accuracy: 0.9704 - val_loss: 0.1479\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9848 - loss: 0.1207 - val_accuracy: 0.9679 - val_loss: 0.1519\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9870 - loss: 0.1153 - val_accuracy: 0.9689 - val_loss: 0.1466\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 3 F1-score: 0.9656\n",
      "\n",
      "Fold 4\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.8913 - loss: 1.2711 - val_accuracy: 0.9635 - val_loss: 0.2814\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.9587 - loss: 0.2573 - val_accuracy: 0.9642 - val_loss: 0.1981\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.9666 - loss: 0.1907 - val_accuracy: 0.9639 - val_loss: 0.1836\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.9695 - loss: 0.1698 - val_accuracy: 0.9666 - val_loss: 0.1725\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9737 - loss: 0.1538 - val_accuracy: 0.9677 - val_loss: 0.1665\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.9786 - loss: 0.1416 - val_accuracy: 0.9689 - val_loss: 0.1578\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.9815 - loss: 0.1326 - val_accuracy: 0.9721 - val_loss: 0.1499\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.9841 - loss: 0.1251 - val_accuracy: 0.9735 - val_loss: 0.1457\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 474ms/step - accuracy: 0.9857 - loss: 0.1189 - val_accuracy: 0.9748 - val_loss: 0.1431\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.9887 - loss: 0.1130 - val_accuracy: 0.9735 - val_loss: 0.1417\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 4 F1-score: 0.9706\n",
      "\n",
      "Fold 5\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.8874 - loss: 1.3090 - val_accuracy: 0.9599 - val_loss: 0.2931\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.9585 - loss: 0.2663 - val_accuracy: 0.9640 - val_loss: 0.1981\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.9650 - loss: 0.1912 - val_accuracy: 0.9601 - val_loss: 0.1893\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.9701 - loss: 0.1685 - val_accuracy: 0.9656 - val_loss: 0.1705\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9744 - loss: 0.1526 - val_accuracy: 0.9685 - val_loss: 0.1602\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9781 - loss: 0.1419 - val_accuracy: 0.9733 - val_loss: 0.1495\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9798 - loss: 0.1341 - val_accuracy: 0.9724 - val_loss: 0.1470\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9830 - loss: 0.1275 - val_accuracy: 0.9724 - val_loss: 0.1450\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9838 - loss: 0.1217 - val_accuracy: 0.9750 - val_loss: 0.1413\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9867 - loss: 0.1159 - val_accuracy: 0.9724 - val_loss: 0.1409\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 5 F1-score: 0.9689\n",
      "Average F1-score: 0.9693\n",
      "\n",
      "Testing filters=128, dropout_rate=0.3\n",
      "\n",
      "Fold 1\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.8809 - loss: 1.4104 - val_accuracy: 0.9559 - val_loss: 0.3380\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9581 - loss: 0.2874 - val_accuracy: 0.9635 - val_loss: 0.2089\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9650 - loss: 0.1966 - val_accuracy: 0.9622 - val_loss: 0.1885\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9702 - loss: 0.1685 - val_accuracy: 0.9705 - val_loss: 0.1643\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9759 - loss: 0.1526 - val_accuracy: 0.9710 - val_loss: 0.1618\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9784 - loss: 0.1427 - val_accuracy: 0.9697 - val_loss: 0.1598\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9804 - loss: 0.1358 - val_accuracy: 0.9727 - val_loss: 0.1510\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9831 - loss: 0.1276 - val_accuracy: 0.9737 - val_loss: 0.1450\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9862 - loss: 0.1203 - val_accuracy: 0.9739 - val_loss: 0.1434\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9876 - loss: 0.1151 - val_accuracy: 0.9707 - val_loss: 0.1452\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 1 F1-score: 0.9669\n",
      "\n",
      "Fold 2\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8680 - loss: 1.4484 - val_accuracy: 0.9620 - val_loss: 0.3504\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9585 - loss: 0.3157 - val_accuracy: 0.9672 - val_loss: 0.2145\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9671 - loss: 0.2066 - val_accuracy: 0.9686 - val_loss: 0.1780\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9713 - loss: 0.1740 - val_accuracy: 0.9703 - val_loss: 0.1644\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9746 - loss: 0.1574 - val_accuracy: 0.9725 - val_loss: 0.1550\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9771 - loss: 0.1443 - val_accuracy: 0.9704 - val_loss: 0.1570\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9796 - loss: 0.1386 - val_accuracy: 0.9700 - val_loss: 0.1526\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9834 - loss: 0.1293 - val_accuracy: 0.9714 - val_loss: 0.1483\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9848 - loss: 0.1235 - val_accuracy: 0.9725 - val_loss: 0.1456\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9867 - loss: 0.1168 - val_accuracy: 0.9692 - val_loss: 0.1510\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 2 F1-score: 0.9662\n",
      "\n",
      "Fold 3\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.8685 - loss: 1.4163 - val_accuracy: 0.9548 - val_loss: 0.3441\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9587 - loss: 0.3068 - val_accuracy: 0.9581 - val_loss: 0.2265\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9635 - loss: 0.2110 - val_accuracy: 0.9623 - val_loss: 0.1871\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9694 - loss: 0.1785 - val_accuracy: 0.9670 - val_loss: 0.1667\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9737 - loss: 0.1579 - val_accuracy: 0.9697 - val_loss: 0.1601\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9764 - loss: 0.1481 - val_accuracy: 0.9721 - val_loss: 0.1514\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9786 - loss: 0.1381 - val_accuracy: 0.9703 - val_loss: 0.1528\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9824 - loss: 0.1297 - val_accuracy: 0.9689 - val_loss: 0.1525\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9842 - loss: 0.1235 - val_accuracy: 0.9704 - val_loss: 0.1478\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9865 - loss: 0.1179 - val_accuracy: 0.9708 - val_loss: 0.1471\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 3 F1-score: 0.9676\n",
      "\n",
      "Fold 4\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.8830 - loss: 1.2223 - val_accuracy: 0.9609 - val_loss: 0.2719\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9578 - loss: 0.2488 - val_accuracy: 0.9623 - val_loss: 0.1991\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9658 - loss: 0.1878 - val_accuracy: 0.9615 - val_loss: 0.1918\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9711 - loss: 0.1688 - val_accuracy: 0.9670 - val_loss: 0.1701\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9749 - loss: 0.1542 - val_accuracy: 0.9673 - val_loss: 0.1673\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9787 - loss: 0.1433 - val_accuracy: 0.9741 - val_loss: 0.1536\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9804 - loss: 0.1357 - val_accuracy: 0.9740 - val_loss: 0.1476\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9831 - loss: 0.1272 - val_accuracy: 0.9741 - val_loss: 0.1443\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9855 - loss: 0.1215 - val_accuracy: 0.9741 - val_loss: 0.1430\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9868 - loss: 0.1160 - val_accuracy: 0.9754 - val_loss: 0.1407\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 4 F1-score: 0.9724\n",
      "\n",
      "Fold 5\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.8824 - loss: 1.4319 - val_accuracy: 0.9610 - val_loss: 0.3522\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9592 - loss: 0.3087 - val_accuracy: 0.9608 - val_loss: 0.2165\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9649 - loss: 0.2008 - val_accuracy: 0.9674 - val_loss: 0.1772\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9699 - loss: 0.1737 - val_accuracy: 0.9694 - val_loss: 0.1655\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9738 - loss: 0.1572 - val_accuracy: 0.9726 - val_loss: 0.1568\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9774 - loss: 0.1458 - val_accuracy: 0.9727 - val_loss: 0.1497\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9808 - loss: 0.1347 - val_accuracy: 0.9724 - val_loss: 0.1479\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9838 - loss: 0.1265 - val_accuracy: 0.9712 - val_loss: 0.1526\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9853 - loss: 0.1206 - val_accuracy: 0.9725 - val_loss: 0.1466\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9864 - loss: 0.1161 - val_accuracy: 0.9709 - val_loss: 0.1509\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 5 F1-score: 0.9669\n",
      "Average F1-score: 0.9680\n",
      "\n",
      "Testing filters=128, dropout_rate=0.4\n",
      "\n",
      "Fold 1\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8757 - loss: 1.3904 - val_accuracy: 0.9603 - val_loss: 0.3144\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9584 - loss: 0.2826 - val_accuracy: 0.9677 - val_loss: 0.2022\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9667 - loss: 0.1952 - val_accuracy: 0.9683 - val_loss: 0.1805\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9695 - loss: 0.1737 - val_accuracy: 0.9715 - val_loss: 0.1639\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9754 - loss: 0.1565 - val_accuracy: 0.9697 - val_loss: 0.1621\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9765 - loss: 0.1476 - val_accuracy: 0.9731 - val_loss: 0.1518\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9804 - loss: 0.1375 - val_accuracy: 0.9720 - val_loss: 0.1505\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9819 - loss: 0.1314 - val_accuracy: 0.9673 - val_loss: 0.1597\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9849 - loss: 0.1240 - val_accuracy: 0.9700 - val_loss: 0.1511\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9867 - loss: 0.1187 - val_accuracy: 0.9692 - val_loss: 0.1503\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 1 F1-score: 0.9651\n",
      "\n",
      "Fold 2\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.8706 - loss: 1.4469 - val_accuracy: 0.9602 - val_loss: 0.3430\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9578 - loss: 0.3122 - val_accuracy: 0.9669 - val_loss: 0.2100\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9655 - loss: 0.2069 - val_accuracy: 0.9665 - val_loss: 0.1808\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9687 - loss: 0.1800 - val_accuracy: 0.9674 - val_loss: 0.1715\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9733 - loss: 0.1621 - val_accuracy: 0.9712 - val_loss: 0.1565\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9769 - loss: 0.1504 - val_accuracy: 0.9712 - val_loss: 0.1540\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9781 - loss: 0.1419 - val_accuracy: 0.9707 - val_loss: 0.1536\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9812 - loss: 0.1340 - val_accuracy: 0.9707 - val_loss: 0.1533\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9830 - loss: 0.1267 - val_accuracy: 0.9715 - val_loss: 0.1507\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9858 - loss: 0.1204 - val_accuracy: 0.9720 - val_loss: 0.1497\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 2 F1-score: 0.9692\n",
      "\n",
      "Fold 3\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.8696 - loss: 1.4036 - val_accuracy: 0.9512 - val_loss: 0.3390\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9583 - loss: 0.2922 - val_accuracy: 0.9570 - val_loss: 0.2162\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9656 - loss: 0.1990 - val_accuracy: 0.9646 - val_loss: 0.1801\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9694 - loss: 0.1745 - val_accuracy: 0.9662 - val_loss: 0.1715\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9737 - loss: 0.1591 - val_accuracy: 0.9684 - val_loss: 0.1603\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9768 - loss: 0.1485 - val_accuracy: 0.9714 - val_loss: 0.1537\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9801 - loss: 0.1383 - val_accuracy: 0.9711 - val_loss: 0.1534\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9813 - loss: 0.1321 - val_accuracy: 0.9720 - val_loss: 0.1493\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9836 - loss: 0.1248 - val_accuracy: 0.9716 - val_loss: 0.1478\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9850 - loss: 0.1208 - val_accuracy: 0.9707 - val_loss: 0.1465\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 3 F1-score: 0.9673\n",
      "\n",
      "Fold 4\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.8703 - loss: 1.4706 - val_accuracy: 0.9532 - val_loss: 0.3449\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9549 - loss: 0.2975 - val_accuracy: 0.9638 - val_loss: 0.2101\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9638 - loss: 0.2033 - val_accuracy: 0.9591 - val_loss: 0.1996\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9666 - loss: 0.1793 - val_accuracy: 0.9656 - val_loss: 0.1786\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9738 - loss: 0.1608 - val_accuracy: 0.9716 - val_loss: 0.1610\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9778 - loss: 0.1462 - val_accuracy: 0.9720 - val_loss: 0.1573\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9796 - loss: 0.1380 - val_accuracy: 0.9748 - val_loss: 0.1484\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9815 - loss: 0.1316 - val_accuracy: 0.9744 - val_loss: 0.1470\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9845 - loss: 0.1260 - val_accuracy: 0.9742 - val_loss: 0.1439\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9862 - loss: 0.1198 - val_accuracy: 0.9727 - val_loss: 0.1445\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 4 F1-score: 0.9695\n",
      "\n",
      "Fold 5\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.8680 - loss: 1.4357 - val_accuracy: 0.9572 - val_loss: 0.3295\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9585 - loss: 0.2946 - val_accuracy: 0.9639 - val_loss: 0.2035\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9664 - loss: 0.1985 - val_accuracy: 0.9666 - val_loss: 0.1772\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9701 - loss: 0.1741 - val_accuracy: 0.9685 - val_loss: 0.1683\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9735 - loss: 0.1600 - val_accuracy: 0.9713 - val_loss: 0.1566\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9769 - loss: 0.1489 - val_accuracy: 0.9704 - val_loss: 0.1524\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9788 - loss: 0.1408 - val_accuracy: 0.9733 - val_loss: 0.1478\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9818 - loss: 0.1318 - val_accuracy: 0.9714 - val_loss: 0.1492\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9844 - loss: 0.1245 - val_accuracy: 0.9731 - val_loss: 0.1452\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9867 - loss: 0.1182 - val_accuracy: 0.9704 - val_loss: 0.1463\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 5 F1-score: 0.9664\n",
      "Average F1-score: 0.9675\n",
      "\n",
      "Testing filters=128, dropout_rate=0.5\n",
      "\n",
      "Fold 1\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.8673 - loss: 1.3075 - val_accuracy: 0.9579 - val_loss: 0.2975\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9564 - loss: 0.2703 - val_accuracy: 0.9662 - val_loss: 0.2005\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9653 - loss: 0.1968 - val_accuracy: 0.9685 - val_loss: 0.1789\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9693 - loss: 0.1771 - val_accuracy: 0.9705 - val_loss: 0.1679\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9749 - loss: 0.1606 - val_accuracy: 0.9708 - val_loss: 0.1644\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9760 - loss: 0.1512 - val_accuracy: 0.9720 - val_loss: 0.1539\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9793 - loss: 0.1406 - val_accuracy: 0.9721 - val_loss: 0.1522\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9811 - loss: 0.1362 - val_accuracy: 0.9708 - val_loss: 0.1495\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9830 - loss: 0.1296 - val_accuracy: 0.9667 - val_loss: 0.1593\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9855 - loss: 0.1240 - val_accuracy: 0.9631 - val_loss: 0.1675\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 1 F1-score: 0.9575\n",
      "\n",
      "Fold 2\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.8748 - loss: 1.4941 - val_accuracy: 0.9596 - val_loss: 0.3112\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9581 - loss: 0.2849 - val_accuracy: 0.9595 - val_loss: 0.2078\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9637 - loss: 0.2011 - val_accuracy: 0.9673 - val_loss: 0.1774\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9695 - loss: 0.1754 - val_accuracy: 0.9694 - val_loss: 0.1674\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9739 - loss: 0.1637 - val_accuracy: 0.9708 - val_loss: 0.1585\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9759 - loss: 0.1523 - val_accuracy: 0.9703 - val_loss: 0.1576\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9783 - loss: 0.1449 - val_accuracy: 0.9716 - val_loss: 0.1518\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9810 - loss: 0.1368 - val_accuracy: 0.9716 - val_loss: 0.1496\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9840 - loss: 0.1285 - val_accuracy: 0.9708 - val_loss: 0.1526\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9848 - loss: 0.1255 - val_accuracy: 0.9727 - val_loss: 0.1445\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 2 F1-score: 0.9698\n",
      "\n",
      "Fold 3\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.8654 - loss: 1.3960 - val_accuracy: 0.9553 - val_loss: 0.2990\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9562 - loss: 0.2757 - val_accuracy: 0.9560 - val_loss: 0.2166\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9624 - loss: 0.2036 - val_accuracy: 0.9638 - val_loss: 0.1809\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9681 - loss: 0.1808 - val_accuracy: 0.9699 - val_loss: 0.1637\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9724 - loss: 0.1629 - val_accuracy: 0.9693 - val_loss: 0.1576\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9762 - loss: 0.1510 - val_accuracy: 0.9701 - val_loss: 0.1556\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9781 - loss: 0.1445 - val_accuracy: 0.9712 - val_loss: 0.1527\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9816 - loss: 0.1344 - val_accuracy: 0.9709 - val_loss: 0.1508\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9834 - loss: 0.1286 - val_accuracy: 0.9682 - val_loss: 0.1571\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9845 - loss: 0.1235 - val_accuracy: 0.9714 - val_loss: 0.1485\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 3 F1-score: 0.9678\n",
      "\n",
      "Fold 4\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.8614 - loss: 1.4114 - val_accuracy: 0.9601 - val_loss: 0.3142\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.9534 - loss: 0.2934 - val_accuracy: 0.9599 - val_loss: 0.2183\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9622 - loss: 0.2054 - val_accuracy: 0.9673 - val_loss: 0.1825\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9677 - loss: 0.1810 - val_accuracy: 0.9700 - val_loss: 0.1728\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9718 - loss: 0.1672 - val_accuracy: 0.9720 - val_loss: 0.1624\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.9760 - loss: 0.1534 - val_accuracy: 0.9724 - val_loss: 0.1568\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9787 - loss: 0.1447 - val_accuracy: 0.9716 - val_loss: 0.1537\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9811 - loss: 0.1368 - val_accuracy: 0.9724 - val_loss: 0.1517\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9832 - loss: 0.1306 - val_accuracy: 0.9718 - val_loss: 0.1505\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9849 - loss: 0.1261 - val_accuracy: 0.9738 - val_loss: 0.1466\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 4 F1-score: 0.9706\n",
      "\n",
      "Fold 5\n",
      "Epoch 1/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.8641 - loss: 1.4530 - val_accuracy: 0.9596 - val_loss: 0.3265\n",
      "Epoch 2/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9559 - loss: 0.3030 - val_accuracy: 0.9635 - val_loss: 0.2099\n",
      "Epoch 3/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9637 - loss: 0.2100 - val_accuracy: 0.9686 - val_loss: 0.1766\n",
      "Epoch 4/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9694 - loss: 0.1789 - val_accuracy: 0.9690 - val_loss: 0.1656\n",
      "Epoch 5/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9727 - loss: 0.1641 - val_accuracy: 0.9716 - val_loss: 0.1545\n",
      "Epoch 6/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9764 - loss: 0.1515 - val_accuracy: 0.9733 - val_loss: 0.1517\n",
      "Epoch 7/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9788 - loss: 0.1423 - val_accuracy: 0.9716 - val_loss: 0.1505\n",
      "Epoch 8/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9816 - loss: 0.1363 - val_accuracy: 0.9696 - val_loss: 0.1573\n",
      "Epoch 9/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9819 - loss: 0.1306 - val_accuracy: 0.9709 - val_loss: 0.1508\n",
      "Epoch 10/10\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9849 - loss: 0.1241 - val_accuracy: 0.9717 - val_loss: 0.1484\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 5 F1-score: 0.9680\n",
      "Average F1-score: 0.9667\n",
      "\n",
      "Grid Search Results:\n",
      "Filters: 64, Dropout: 0.2, F1-score: 0.9692\n",
      "Filters: 64, Dropout: 0.3, F1-score: 0.9676\n",
      "Filters: 64, Dropout: 0.4, F1-score: 0.9676\n",
      "Filters: 64, Dropout: 0.5, F1-score: 0.9661\n",
      "Filters: 128, Dropout: 0.2, F1-score: 0.9693\n",
      "Filters: 128, Dropout: 0.3, F1-score: 0.9680\n",
      "Filters: 128, Dropout: 0.4, F1-score: 0.9675\n",
      "Filters: 128, Dropout: 0.5, F1-score: 0.9667\n",
      "\n",
      "Best Parameters:\n",
      "Filters: 128\n",
      "Dropout Rate: 0.2\n",
      "Best F1-Score: 0.9693\n",
      "\n",
      "Results Summary:\n",
      "   filters  dropout_rate  avg_f1_score  \\\n",
      "4      128           0.2      0.969305   \n",
      "0       64           0.2      0.969192   \n",
      "5      128           0.3      0.968000   \n",
      "1       64           0.3      0.967639   \n",
      "2       64           0.4      0.967592   \n",
      "6      128           0.4      0.967508   \n",
      "7      128           0.5      0.966738   \n",
      "3       64           0.5      0.966097   \n",
      "\n",
      "                                         fold_scores  \n",
      "4  [0.9703427455158942, 0.9711580608398351, 0.965...  \n",
      "0  [0.9692266523949692, 0.9701544943820225, 0.968...  \n",
      "5  [0.9668552950687146, 0.966184416942653, 0.9676...  \n",
      "1  [0.9609919449723957, 0.9706760316066725, 0.966...  \n",
      "2  [0.9648695023932087, 0.9703840407768697, 0.967...  \n",
      "6  [0.9650702196615052, 0.9692468984798183, 0.967...  \n",
      "7  [0.9575070821529745, 0.9697610861324165, 0.967...  \n",
      "3  [0.9610012697261019, 0.9702900467248523, 0.965...  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "def train_word2vec_and_create_embeddings(train_texts, word_index, max_words, embedding_dim=100):\n",
    "    \"\"\"Train Word2Vec on training data only and create embedding matrix\"\"\"\n",
    "    # Train Word2Vec on training data only\n",
    "    train_sentences = [text.split() for text in train_texts]\n",
    "    word2vec_model = Word2Vec(sentences=train_sentences, \n",
    "                            vector_size=embedding_dim, \n",
    "                            window=5, \n",
    "                            min_count=2, \n",
    "                            workers=4)\n",
    "    \n",
    "    # Create embedding matrix with correct dimensions\n",
    "    vocab_size = min(max_words, len(word_index) + 1)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i < vocab_size:  # Only include words within max_words limit\n",
    "            if word in word2vec_model.wv:\n",
    "                embedding_matrix[i] = word2vec_model.wv[word]\n",
    "            else:\n",
    "                embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
    "            \n",
    "    return embedding_matrix\n",
    "\n",
    "def create_model(max_sequence_length, vocab_size, embedding_dim, embedding_matrix, \n",
    "                filters, dropout_rate):\n",
    "    input_layer = Input(shape=(max_sequence_length,))\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=True\n",
    "    )(input_layer)\n",
    "\n",
    "    x = Conv1D(\n",
    "        filters=filters,\n",
    "        kernel_size=5,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(0.01)\n",
    "    )(embedding_layer)\n",
    "    \n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    output_layer = Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01))(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'filters': [64, 128],\n",
    "        'dropout_rate': [0.2, 0.3, 0.4, 0.5]\n",
    "    }\n",
    "\n",
    "    # Initialize variables to track results\n",
    "    results = []\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "\n",
    "    # Constants\n",
    "    max_words = 10000\n",
    "    max_sequence_length = 300\n",
    "    embedding_dim = 100\n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    for filters in param_grid['filters']:\n",
    "        for dropout_rate in param_grid['dropout_rate']:\n",
    "            print(f\"\\nTesting filters={filters}, dropout_rate={dropout_rate}\")\n",
    "            \n",
    "            # Initialize cross-validation\n",
    "            kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            fold_scores = []\n",
    "            \n",
    "            # Perform k-fold cross-validation\n",
    "            for fold, (train_idx, val_idx) in enumerate(kfold.split(data['processed_full_content'], data['label']), 1):\n",
    "                print(f\"\\nFold {fold}\")\n",
    "                \n",
    "                # Split data\n",
    "                train_texts = data['processed_full_content'].iloc[train_idx]\n",
    "                val_texts = data['processed_full_content'].iloc[val_idx]\n",
    "                y_train = data['label'].iloc[train_idx]\n",
    "                y_val = data['label'].iloc[val_idx]\n",
    "                \n",
    "                # Fit tokenizer on training data only\n",
    "                tokenizer = Tokenizer(num_words=max_words)\n",
    "                tokenizer.fit_on_texts(train_texts)\n",
    "                \n",
    "                # Convert texts to sequences\n",
    "                X_train = pad_sequences(tokenizer.texts_to_sequences(train_texts), \n",
    "                                      maxlen=max_sequence_length)\n",
    "                X_val = pad_sequences(tokenizer.texts_to_sequences(val_texts), \n",
    "                                    maxlen=max_sequence_length)\n",
    "                \n",
    "                # Get vocab size for this fold\n",
    "                vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
    "                \n",
    "                # Create embedding matrix using training data only\n",
    "                embedding_matrix = train_word2vec_and_create_embeddings(\n",
    "                    train_texts, \n",
    "                    tokenizer.word_index,\n",
    "                    max_words,\n",
    "                    embedding_dim\n",
    "                )\n",
    "                \n",
    "                # Create and train model\n",
    "                model = create_model(\n",
    "                    max_sequence_length=max_sequence_length,\n",
    "                    vocab_size=vocab_size,\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    embedding_matrix=embedding_matrix,\n",
    "                    filters=filters,\n",
    "                    dropout_rate=dropout_rate\n",
    "                )\n",
    "                \n",
    "                # Train model\n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Evaluate using F1-score\n",
    "                y_pred = (model.predict(X_val) > 0.5).astype(int)\n",
    "                fold_score = f1_score(y_val, y_pred)\n",
    "                fold_scores.append(fold_score)\n",
    "                \n",
    "                print(f\"Fold {fold} F1-score: {fold_score:.4f}\")\n",
    "            \n",
    "            # Calculate average score for this parameter combination\n",
    "            avg_score = np.mean(fold_scores)\n",
    "            print(f\"Average F1-score: {avg_score:.4f}\")\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'filters': filters,\n",
    "                'dropout_rate': dropout_rate,\n",
    "                'avg_f1_score': avg_score,\n",
    "                'fold_scores': fold_scores\n",
    "            })\n",
    "            \n",
    "            # Update best parameters if necessary\n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_params = {'filters': filters, 'dropout_rate': dropout_rate}\n",
    "\n",
    "    # Print final results\n",
    "    print(\"\\nGrid Search Results:\")\n",
    "    for result in results:\n",
    "        print(f\"Filters: {result['filters']}, Dropout: {result['dropout_rate']}, \"\n",
    "              f\"F1-score: {result['avg_f1_score']:.4f}\")\n",
    "\n",
    "    print(\"\\nBest Parameters:\")\n",
    "    print(f\"Filters: {best_params['filters']}\")\n",
    "    print(f\"Dropout Rate: {best_params['dropout_rate']}\")\n",
    "    print(f\"Best F1-Score: {best_score:.4f}\")\n",
    "\n",
    "    # Save results to DataFrame for easy analysis\n",
    "    import pandas as pd\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nResults Summary:\")\n",
    "    print(results_df.sort_values('avg_f1_score', ascending=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
