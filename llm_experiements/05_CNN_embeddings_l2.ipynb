{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working dir: /Users/inflaton/code/engd/papers/DM-Fake-News-Detection\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if \"workding_dir\" not in locals():\n",
    "    workding_dir = str(Path.cwd().parent)\n",
    "os.chdir(workding_dir)\n",
    "sys.path.append(workding_dir)\n",
    "print(\"working dir:\", workding_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Detection to Credibility: A Machine Learning Framework for Assessing News Source Reliability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# For concurrency (running functions in parallel)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# For caching (to speed up repeated function calls)\n",
    "from functools import lru_cache\n",
    "\n",
    "# For progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "import nltk\n",
    "\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Part-of-speech tagging\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation (Loading CSV)\n",
    "\n",
    "Load the processed_data `csv` file into pandas DataFrames\n",
    "- `processed_data.csv` is loaded into `data` DataFrame (stemming has been performed to reduce processing time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./processed_data_filtered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    34030\n",
       "1    26461\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60491 entries, 0 to 60490\n",
      "Data columns (total 4 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   label                   60491 non-null  int64 \n",
      " 1   full_content            60491 non-null  object\n",
      " 2   processed_full_content  60491 non-null  object\n",
      " 3   word_count              60491 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 1.8+ MB\n",
      "Dataframe Shape: (60491, 4)\n"
     ]
    }
   ],
   "source": [
    "data.info()\n",
    "print(\"Dataframe Shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural network + Custom-trained word2vec word embeddings + 5-Fold Cross Validation + L2 Regularization + GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing filters=64, dropout_rate=0.3\n",
      "\n",
      "Fold 1\n",
      "Epoch 1/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.8586 - loss: 1.1955 - val_accuracy: 0.9616 - val_loss: 0.3182\n",
      "Epoch 2/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9621 - loss: 0.2824 - val_accuracy: 0.9650 - val_loss: 0.2026\n",
      "Epoch 3/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9669 - loss: 0.1959 - val_accuracy: 0.9675 - val_loss: 0.1732\n",
      "Epoch 4/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9715 - loss: 0.1670 - val_accuracy: 0.9679 - val_loss: 0.1645\n",
      "Epoch 5/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9745 - loss: 0.1539 - val_accuracy: 0.9689 - val_loss: 0.1651\n",
      "Epoch 6/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9790 - loss: 0.1420 - val_accuracy: 0.9662 - val_loss: 0.1619\n",
      "Epoch 7/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9818 - loss: 0.1327 - val_accuracy: 0.9678 - val_loss: 0.1578\n",
      "Epoch 8/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9839 - loss: 0.1277 - val_accuracy: 0.9638 - val_loss: 0.1654\n",
      "Epoch 9/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9853 - loss: 0.1219 - val_accuracy: 0.9659 - val_loss: 0.1598\n",
      "Epoch 10/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9869 - loss: 0.1173 - val_accuracy: 0.9650 - val_loss: 0.1657\n",
      "\u001b[1m379/379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 1 F1-score: 0.9611\n",
      "\n",
      "Fold 2\n",
      "Epoch 1/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.8730 - loss: 1.1428 - val_accuracy: 0.9658 - val_loss: 0.2989\n",
      "Epoch 2/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9622 - loss: 0.2711 - val_accuracy: 0.9710 - val_loss: 0.1876\n",
      "Epoch 3/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9677 - loss: 0.1885 - val_accuracy: 0.9735 - val_loss: 0.1626\n",
      "Epoch 4/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9732 - loss: 0.1613 - val_accuracy: 0.9745 - val_loss: 0.1535\n",
      "Epoch 5/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9777 - loss: 0.1477 - val_accuracy: 0.9765 - val_loss: 0.1455\n",
      "Epoch 6/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.9811 - loss: 0.1352 - val_accuracy: 0.9773 - val_loss: 0.1425\n",
      "Epoch 7/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.9834 - loss: 0.1292 - val_accuracy: 0.9779 - val_loss: 0.1409\n",
      "Epoch 8/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9858 - loss: 0.1233 - val_accuracy: 0.9784 - val_loss: 0.1363\n",
      "Epoch 9/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9872 - loss: 0.1170 - val_accuracy: 0.9783 - val_loss: 0.1351\n",
      "Epoch 10/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.9889 - loss: 0.1117 - val_accuracy: 0.9770 - val_loss: 0.1339\n",
      "\u001b[1m379/379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Fold 2 F1-score: 0.9736\n",
      "\n",
      "Fold 3\n",
      "Epoch 1/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.8538 - loss: 1.2584 - val_accuracy: 0.9633 - val_loss: 0.3248\n",
      "Epoch 2/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9608 - loss: 0.2872 - val_accuracy: 0.9672 - val_loss: 0.1991\n",
      "Epoch 3/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.9684 - loss: 0.1918 - val_accuracy: 0.9655 - val_loss: 0.1791\n",
      "Epoch 4/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9719 - loss: 0.1636 - val_accuracy: 0.9657 - val_loss: 0.1656\n",
      "Epoch 5/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9762 - loss: 0.1509 - val_accuracy: 0.9671 - val_loss: 0.1608\n",
      "Epoch 6/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.9789 - loss: 0.1402 - val_accuracy: 0.9683 - val_loss: 0.1523\n",
      "Epoch 7/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.9818 - loss: 0.1314 - val_accuracy: 0.9700 - val_loss: 0.1482\n",
      "Epoch 8/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.9839 - loss: 0.1248 - val_accuracy: 0.9737 - val_loss: 0.1423\n",
      "Epoch 9/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.9861 - loss: 0.1175 - val_accuracy: 0.9726 - val_loss: 0.1402\n",
      "Epoch 10/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.9882 - loss: 0.1124 - val_accuracy: 0.9729 - val_loss: 0.1403\n",
      "\u001b[1m379/379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 3 F1-score: 0.9688\n",
      "\n",
      "Fold 4\n",
      "Epoch 1/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8470 - loss: 1.3164 - val_accuracy: 0.9623 - val_loss: 0.3445\n",
      "Epoch 2/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.9602 - loss: 0.3058 - val_accuracy: 0.9707 - val_loss: 0.2064\n",
      "Epoch 3/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.9680 - loss: 0.1989 - val_accuracy: 0.9693 - val_loss: 0.1738\n",
      "Epoch 4/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.9717 - loss: 0.1662 - val_accuracy: 0.9712 - val_loss: 0.1625\n",
      "Epoch 5/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.9763 - loss: 0.1504 - val_accuracy: 0.9710 - val_loss: 0.1550\n",
      "Epoch 6/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.9786 - loss: 0.1405 - val_accuracy: 0.9727 - val_loss: 0.1491\n",
      "Epoch 7/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.9824 - loss: 0.1301 - val_accuracy: 0.9735 - val_loss: 0.1450\n",
      "Epoch 8/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.9834 - loss: 0.1228 - val_accuracy: 0.9747 - val_loss: 0.1441\n",
      "Epoch 9/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.9857 - loss: 0.1184 - val_accuracy: 0.9744 - val_loss: 0.1409\n",
      "Epoch 10/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.9881 - loss: 0.1131 - val_accuracy: 0.9731 - val_loss: 0.1414\n",
      "\u001b[1m379/379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 4 F1-score: 0.9694\n",
      "\n",
      "Fold 5\n",
      "Epoch 1/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8423 - loss: 1.3070 - val_accuracy: 0.9608 - val_loss: 0.3201\n",
      "Epoch 2/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.9598 - loss: 0.2913 - val_accuracy: 0.9662 - val_loss: 0.2083\n",
      "Epoch 3/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.9660 - loss: 0.2036 - val_accuracy: 0.9682 - val_loss: 0.1751\n",
      "Epoch 4/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.9709 - loss: 0.1714 - val_accuracy: 0.9689 - val_loss: 0.1644\n",
      "Epoch 5/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9736 - loss: 0.1561 - val_accuracy: 0.9691 - val_loss: 0.1594\n",
      "Epoch 6/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.9788 - loss: 0.1444 - val_accuracy: 0.9699 - val_loss: 0.1533\n",
      "Epoch 7/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9800 - loss: 0.1365 - val_accuracy: 0.9670 - val_loss: 0.1570\n",
      "Epoch 8/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.9815 - loss: 0.1302 - val_accuracy: 0.9646 - val_loss: 0.1616\n",
      "Epoch 9/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.9843 - loss: 0.1234 - val_accuracy: 0.9674 - val_loss: 0.1561\n",
      "Epoch 10/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.9854 - loss: 0.1193 - val_accuracy: 0.9681 - val_loss: 0.1526\n",
      "\u001b[1m379/379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 5 F1-score: 0.9626\n",
      "Average F1-score: 0.9671\n",
      "\n",
      "Results:\n",
      "filters: 64\n",
      "dropout_rate: 0.3\n",
      "avg_accuracy: 0.9712024308847502\n",
      "avg_precision: 0.9680536838601326\n",
      "avg_recall: 0.9667426880066217\n",
      "avg_f1_score: 0.9670886952215152\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding,\n",
    "    Conv1D,\n",
    "    GlobalMaxPooling1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Input,\n",
    ")\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "def train_word2vec_and_create_embeddings(\n",
    "    train_texts, word_index, max_words, embedding_dim=100\n",
    "):\n",
    "    \"\"\"Train Word2Vec on training data only and create embedding matrix\"\"\"\n",
    "    # Train Word2Vec on training data only\n",
    "    train_sentences = [text.split() for text in train_texts]\n",
    "    word2vec_model = Word2Vec(\n",
    "        sentences=train_sentences,\n",
    "        vector_size=embedding_dim,\n",
    "        window=5,\n",
    "        min_count=2,\n",
    "        workers=4,\n",
    "    )\n",
    "\n",
    "    # Create embedding matrix with correct dimensions\n",
    "    vocab_size = min(max_words, len(word_index) + 1)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i < vocab_size:  # Only include words within max_words limit\n",
    "            if word in word2vec_model.wv:\n",
    "                embedding_matrix[i] = word2vec_model.wv[word]\n",
    "            else:\n",
    "                embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def create_model(\n",
    "    max_sequence_length,\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    embedding_matrix,\n",
    "    filters,\n",
    "    dropout_rate,\n",
    "):\n",
    "    input_layer = Input(shape=(max_sequence_length,))\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=True,\n",
    "    )(input_layer)\n",
    "\n",
    "    x = Conv1D(\n",
    "        filters=filters, kernel_size=5, activation=\"relu\", kernel_regularizer=l2(0.01)\n",
    "    )(embedding_layer)\n",
    "\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(64, activation=\"relu\", kernel_regularizer=l2(0.01))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    output_layer = Dense(1, activation=\"sigmoid\", kernel_regularizer=l2(0.01))(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    filters = 64\n",
    "    dropout_rate = 0.3\n",
    "\n",
    "    # Initialize variables to track results\n",
    "    results = []\n",
    "    best_params = None\n",
    "\n",
    "    # Constants\n",
    "    max_words = 10000\n",
    "    max_sequence_length = 300\n",
    "    embedding_dim = 100\n",
    "\n",
    "    print(f\"\\nTesting filters={filters}, dropout_rate={dropout_rate}\")\n",
    "\n",
    "    # Initialize cross-validation\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold, (train_idx, val_idx) in enumerate(\n",
    "        kfold.split(data[\"processed_full_content\"], data[\"label\"]), 1\n",
    "    ):\n",
    "        print(f\"\\nFold {fold}\")\n",
    "\n",
    "        # Split data\n",
    "        train_texts = data[\"processed_full_content\"].iloc[train_idx]\n",
    "        val_texts = data[\"processed_full_content\"].iloc[val_idx]\n",
    "        y_train = data[\"label\"].iloc[train_idx]\n",
    "        y_val = data[\"label\"].iloc[val_idx]\n",
    "\n",
    "        # Fit tokenizer on training data only\n",
    "        tokenizer = Tokenizer(num_words=max_words)\n",
    "        tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "        # Convert texts to sequences\n",
    "        X_train = pad_sequences(\n",
    "            tokenizer.texts_to_sequences(train_texts), maxlen=max_sequence_length\n",
    "        )\n",
    "        X_val = pad_sequences(\n",
    "            tokenizer.texts_to_sequences(val_texts), maxlen=max_sequence_length\n",
    "        )\n",
    "\n",
    "        # Get vocab size for this fold\n",
    "        vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
    "\n",
    "        # Create embedding matrix using training data only\n",
    "        embedding_matrix = train_word2vec_and_create_embeddings(\n",
    "            train_texts, tokenizer.word_index, max_words, embedding_dim\n",
    "        )\n",
    "\n",
    "        # Create and train model\n",
    "        model = create_model(\n",
    "            max_sequence_length=max_sequence_length,\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            embedding_matrix=embedding_matrix,\n",
    "            filters=filters,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=10,\n",
    "            batch_size=64,\n",
    "            validation_data=(X_val, y_val),\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        # Evaluate using F1-score\n",
    "        y_pred = (model.predict(X_val) > 0.5).astype(int)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision = precision_score(y_val, y_pred)\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        print(f\"Fold {fold} F1-score: {f1:.4f}\")\n",
    "\n",
    "    # Calculate average score for this parameter combination\n",
    "    avg_score = np.mean(f1_scores)\n",
    "    print(f\"Average F1-score: {avg_score:.4f}\")\n",
    "\n",
    "    # Store results\n",
    "    results.append(\n",
    "        {\n",
    "            \"filters\": filters,\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "            \"avg_accuracy\": np.mean(accuracy_scores),\n",
    "            \"avg_precision\": np.mean(precision_scores),\n",
    "            \"avg_recall\": np.mean(recall_scores),\n",
    "            \"avg_f1_score\": avg_score,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"\\nResults:\")\n",
    "    for result in results:\n",
    "        for key, value in result.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, epochs=10, batch_size=64):\n",
    "    # Set seeds for reproducibility\n",
    "    seed = 42\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    filters = 64\n",
    "    dropout_rate = 0.3\n",
    "\n",
    "    # Initialize variables to track results\n",
    "    results = []\n",
    "    best_params = None\n",
    "\n",
    "    # Constants\n",
    "    max_words = 10000\n",
    "    max_sequence_length = 300\n",
    "    embedding_dim = 100\n",
    "\n",
    "    print(f\"\\nTesting filters={filters}, dropout_rate={dropout_rate}\")\n",
    "\n",
    "    # Initialize cross-validation\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold, (train_idx, val_idx) in enumerate(\n",
    "        kfold.split(data[\"processed_full_content\"], data[\"label\"]), 1\n",
    "    ):\n",
    "        print(f\"\\nFold {fold}\")\n",
    "\n",
    "        # Split data\n",
    "        train_texts = data[\"processed_full_content\"].iloc[train_idx]\n",
    "        val_texts = data[\"processed_full_content\"].iloc[val_idx]\n",
    "        y_train = data[\"label\"].iloc[train_idx]\n",
    "        y_val = data[\"label\"].iloc[val_idx]\n",
    "\n",
    "        # Fit tokenizer on training data only\n",
    "        tokenizer = Tokenizer(num_words=max_words)\n",
    "        tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "        # Convert texts to sequences\n",
    "        X_train = pad_sequences(\n",
    "            tokenizer.texts_to_sequences(train_texts), maxlen=max_sequence_length\n",
    "        )\n",
    "        X_val = pad_sequences(\n",
    "            tokenizer.texts_to_sequences(val_texts), maxlen=max_sequence_length\n",
    "        )\n",
    "\n",
    "        # Get vocab size for this fold\n",
    "        vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
    "\n",
    "        # Create embedding matrix using training data only\n",
    "        embedding_matrix = train_word2vec_and_create_embeddings(\n",
    "            train_texts, tokenizer.word_index, max_words, embedding_dim\n",
    "        )\n",
    "\n",
    "        # Create and train model\n",
    "        model = create_model(\n",
    "            max_sequence_length=max_sequence_length,\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            embedding_matrix=embedding_matrix,\n",
    "            filters=filters,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        # Evaluate using F1-score\n",
    "        y_pred = (model.predict(X_val) > 0.5).astype(int)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision = precision_score(y_val, y_pred)\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        print(f\"Fold {fold} F1-score: {f1:.4f}\")\n",
    "        break\n",
    "\n",
    "    # Calculate average score for this parameter combination\n",
    "    avg_score = np.mean(f1_scores)\n",
    "    print(f\"Average F1-score: {avg_score:.4f}\")\n",
    "\n",
    "    # Store results\n",
    "    result = {\n",
    "        \"filters\": filters,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"accuracy\": np.mean(accuracy_scores),\n",
    "        \"precision\": np.mean(precision_scores),\n",
    "        \"recall\": np.mean(recall_scores),\n",
    "        \"f1_score\": avg_score,\n",
    "    }\n",
    "\n",
    "    print(\"\\nResult:\")\n",
    "    for key, value in result.items():\n",
    "        print(f\"\\t{key}: {value}\")\n",
    "\n",
    "    return model, train_idx, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing filters=64, dropout_rate=0.3\n",
      "\n",
      "Fold 1\n",
      "Epoch 1/10\n",
      "\u001b[1m5445/5445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 7ms/step - accuracy: 0.9115 - loss: 0.6775 - val_accuracy: 0.9653 - val_loss: 0.1962\n",
      "Epoch 2/10\n",
      "\u001b[1m5445/5445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 7ms/step - accuracy: 0.9611 - loss: 0.2031 - val_accuracy: 0.9691 - val_loss: 0.1713\n",
      "Epoch 3/10\n",
      "\u001b[1m5445/5445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 7ms/step - accuracy: 0.9677 - loss: 0.1727 - val_accuracy: 0.9660 - val_loss: 0.1683\n",
      "Epoch 4/10\n",
      "\u001b[1m5445/5445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 6ms/step - accuracy: 0.9725 - loss: 0.1526 - val_accuracy: 0.9707 - val_loss: 0.1583\n",
      "Epoch 5/10\n",
      "\u001b[1m5445/5445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 7ms/step - accuracy: 0.9753 - loss: 0.1444 - val_accuracy: 0.9716 - val_loss: 0.1500\n",
      "Epoch 6/10\n",
      "\u001b[1m5445/5445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 7ms/step - accuracy: 0.9789 - loss: 0.1331 - val_accuracy: 0.9734 - val_loss: 0.1415\n",
      "Epoch 7/10\n",
      "\u001b[1m5445/5445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 7ms/step - accuracy: 0.9820 - loss: 0.1232 - val_accuracy: 0.9740 - val_loss: 0.1406\n",
      "Epoch 8/10\n",
      "\u001b[1m5445/5445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 8ms/step - accuracy: 0.9846 - loss: 0.1147 - val_accuracy: 0.9745 - val_loss: 0.1408\n",
      "Epoch 9/10\n",
      "\u001b[1m5445/5445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 29ms/step - accuracy: 0.9867 - loss: 0.1087 - val_accuracy: 0.9736 - val_loss: 0.1397\n",
      "Epoch 10/10\n",
      "\u001b[1m5445/5445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 11ms/step - accuracy: 0.9881 - loss: 0.1022 - val_accuracy: 0.9740 - val_loss: 0.1360\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "Fold 1 F1-score: 0.9704\n",
      "Average F1-score: 0.9704\n",
      "\n",
      "Result:\n",
      "\tfilters: 64\n",
      "\tdropout_rate: 0.3\n",
      "\taccuracy: 0.9740495867768595\n",
      "\tprecision: 0.9683972911963883\n",
      "\trecall: 0.972421609369097\n",
      "\tf1_score: 0.9704052780395853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<Functional name=functional_6, built=True>,\n",
       " array([    0,     1,     2, ..., 60488, 60489, 60490]),\n",
       " array([    9,    27,    51, ..., 60451, 60464, 60482]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, train_idx, val_idx = train_model(data, epochs=10, batch_size=64)\n",
    "model, train_idx, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6050"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6050 entries, 9 to 60482\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   label         6050 non-null   int64 \n",
      " 1   full_content  6050 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 141.8+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df = data.iloc[val_idx].copy()\n",
    "test_df = test_df[[\"label\", \"full_content\"]]\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"test_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
