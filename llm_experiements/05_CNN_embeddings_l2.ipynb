{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working dir: /Users/inflaton/code/engd/papers/DM-Fake-News-Detection\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if \"workding_dir\" not in locals():\n",
    "    workding_dir = str(Path.cwd().parent)\n",
    "os.chdir(workding_dir)\n",
    "sys.path.append(workding_dir)\n",
    "print(\"working dir:\", workding_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Detection to Credibility: A Machine Learning Framework for Assessing News Source Reliability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# For concurrency (running functions in parallel)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# For caching (to speed up repeated function calls)\n",
    "from functools import lru_cache\n",
    "\n",
    "# For progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "import nltk\n",
    "\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Part-of-speech tagging\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation (Loading CSV)\n",
    "\n",
    "Load the processed_data `csv` file into pandas DataFrames\n",
    "- `processed_data.csv` is loaded into `data` DataFrame (stemming has been performed to reduce processing time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./processed_data_filtered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    34030\n",
       "1    26461\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60491 entries, 0 to 60490\n",
      "Data columns (total 4 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   label                   60491 non-null  int64 \n",
      " 1   full_content            60491 non-null  object\n",
      " 2   processed_full_content  60491 non-null  object\n",
      " 3   word_count              60491 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 1.8+ MB\n",
      "Dataframe Shape: (60491, 4)\n"
     ]
    }
   ],
   "source": [
    "data.info()\n",
    "print(\"Dataframe Shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural network + Custom-trained word2vec word embeddings + 5-Fold Cross Validation + L2 Regularization + GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing filters=64, dropout_rate=0.3\n",
      "\n",
      "Fold 1\n",
      "Epoch 1/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - accuracy: 0.8731 - loss: 1.1074 - val_accuracy: 0.9588 - val_loss: 0.2836\n",
      "Epoch 2/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 24ms/step - accuracy: 0.9606 - loss: 0.2565 - val_accuracy: 0.9589 - val_loss: 0.2063\n",
      "Epoch 3/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 25ms/step - accuracy: 0.9669 - loss: 0.1882 - val_accuracy: 0.9672 - val_loss: 0.1722\n",
      "Epoch 4/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 29ms/step - accuracy: 0.9727 - loss: 0.1656 - val_accuracy: 0.9658 - val_loss: 0.1681\n",
      "Epoch 5/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9762 - loss: 0.1527 - val_accuracy: 0.9702 - val_loss: 0.1581\n",
      "Epoch 6/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 40ms/step - accuracy: 0.9790 - loss: 0.1419 - val_accuracy: 0.9690 - val_loss: 0.1552\n",
      "Epoch 7/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 43ms/step - accuracy: 0.9817 - loss: 0.1327 - val_accuracy: 0.9691 - val_loss: 0.1553\n",
      "Epoch 8/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.9829 - loss: 0.1254 - val_accuracy: 0.9674 - val_loss: 0.1566\n",
      "Epoch 9/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.9853 - loss: 0.1206 - val_accuracy: 0.9679 - val_loss: 0.1566\n",
      "Epoch 10/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.9862 - loss: 0.1163 - val_accuracy: 0.9686 - val_loss: 0.1533\n",
      "\u001b[1m379/379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "Fold 1 F1-score: 0.9648\n",
      "\n",
      "Fold 2\n",
      "Epoch 1/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.8539 - loss: 1.1454 - val_accuracy: 0.9629 - val_loss: 0.2940\n",
      "Epoch 2/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 25ms/step - accuracy: 0.9615 - loss: 0.2683 - val_accuracy: 0.9669 - val_loss: 0.2007\n",
      "Epoch 3/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 24ms/step - accuracy: 0.9676 - loss: 0.1902 - val_accuracy: 0.9722 - val_loss: 0.1669\n",
      "Epoch 4/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - accuracy: 0.9739 - loss: 0.1616 - val_accuracy: 0.9751 - val_loss: 0.1568\n",
      "Epoch 5/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 23ms/step - accuracy: 0.9772 - loss: 0.1482 - val_accuracy: 0.9761 - val_loss: 0.1483\n",
      "Epoch 6/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 23ms/step - accuracy: 0.9805 - loss: 0.1372 - val_accuracy: 0.9770 - val_loss: 0.1429\n",
      "Epoch 7/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9828 - loss: 0.1285 - val_accuracy: 0.9744 - val_loss: 0.1438\n",
      "Epoch 8/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9852 - loss: 0.1228 - val_accuracy: 0.9766 - val_loss: 0.1379\n",
      "Epoch 9/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9881 - loss: 0.1166 - val_accuracy: 0.9771 - val_loss: 0.1360\n",
      "Epoch 10/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9889 - loss: 0.1114 - val_accuracy: 0.9757 - val_loss: 0.1382\n",
      "\u001b[1m379/379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 2 F1-score: 0.9724\n",
      "\n",
      "Fold 3\n",
      "Epoch 1/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.8742 - loss: 1.0297 - val_accuracy: 0.9642 - val_loss: 0.2471\n",
      "Epoch 2/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.9619 - loss: 0.2289 - val_accuracy: 0.9671 - val_loss: 0.1820\n",
      "Epoch 3/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.9681 - loss: 0.1770 - val_accuracy: 0.9683 - val_loss: 0.1652\n",
      "Epoch 4/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9722 - loss: 0.1581 - val_accuracy: 0.9693 - val_loss: 0.1608\n",
      "Epoch 5/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9768 - loss: 0.1472 - val_accuracy: 0.9707 - val_loss: 0.1558\n",
      "Epoch 6/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 24ms/step - accuracy: 0.9793 - loss: 0.1380 - val_accuracy: 0.9721 - val_loss: 0.1493\n",
      "Epoch 7/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 23ms/step - accuracy: 0.9822 - loss: 0.1287 - val_accuracy: 0.9726 - val_loss: 0.1453\n",
      "Epoch 8/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9856 - loss: 0.1208 - val_accuracy: 0.9727 - val_loss: 0.1455\n",
      "Epoch 9/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 24ms/step - accuracy: 0.9881 - loss: 0.1160 - val_accuracy: 0.9750 - val_loss: 0.1410\n",
      "Epoch 10/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 24ms/step - accuracy: 0.9890 - loss: 0.1110 - val_accuracy: 0.9736 - val_loss: 0.1414\n",
      "\u001b[1m379/379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 3 F1-score: 0.9697\n",
      "\n",
      "Fold 4\n",
      "Epoch 1/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.8671 - loss: 1.0776 - val_accuracy: 0.9651 - val_loss: 0.2693\n",
      "Epoch 2/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - accuracy: 0.9637 - loss: 0.2447 - val_accuracy: 0.9701 - val_loss: 0.1824\n",
      "Epoch 3/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 24ms/step - accuracy: 0.9695 - loss: 0.1787 - val_accuracy: 0.9694 - val_loss: 0.1642\n",
      "Epoch 4/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 23ms/step - accuracy: 0.9746 - loss: 0.1555 - val_accuracy: 0.9706 - val_loss: 0.1580\n",
      "Epoch 5/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 23ms/step - accuracy: 0.9777 - loss: 0.1447 - val_accuracy: 0.9712 - val_loss: 0.1544\n",
      "Epoch 6/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 23ms/step - accuracy: 0.9804 - loss: 0.1359 - val_accuracy: 0.9719 - val_loss: 0.1498\n",
      "Epoch 7/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 23ms/step - accuracy: 0.9839 - loss: 0.1272 - val_accuracy: 0.9740 - val_loss: 0.1433\n",
      "Epoch 8/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 23ms/step - accuracy: 0.9858 - loss: 0.1202 - val_accuracy: 0.9733 - val_loss: 0.1431\n",
      "Epoch 9/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 23ms/step - accuracy: 0.9874 - loss: 0.1155 - val_accuracy: 0.9740 - val_loss: 0.1402\n",
      "Epoch 10/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 23ms/step - accuracy: 0.9887 - loss: 0.1107 - val_accuracy: 0.9745 - val_loss: 0.1398\n",
      "\u001b[1m379/379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 4 F1-score: 0.9708\n",
      "\n",
      "Fold 5\n",
      "Epoch 1/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.8768 - loss: 1.0870 - val_accuracy: 0.9602 - val_loss: 0.2719\n",
      "Epoch 2/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 23ms/step - accuracy: 0.9604 - loss: 0.2490 - val_accuracy: 0.9656 - val_loss: 0.1877\n",
      "Epoch 3/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9671 - loss: 0.1851 - val_accuracy: 0.9673 - val_loss: 0.1664\n",
      "Epoch 4/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 23ms/step - accuracy: 0.9709 - loss: 0.1659 - val_accuracy: 0.9688 - val_loss: 0.1619\n",
      "Epoch 5/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9751 - loss: 0.1518 - val_accuracy: 0.9713 - val_loss: 0.1547\n",
      "Epoch 6/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9789 - loss: 0.1413 - val_accuracy: 0.9716 - val_loss: 0.1495\n",
      "Epoch 7/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9807 - loss: 0.1345 - val_accuracy: 0.9714 - val_loss: 0.1473\n",
      "Epoch 8/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9835 - loss: 0.1278 - val_accuracy: 0.9732 - val_loss: 0.1437\n",
      "Epoch 9/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9844 - loss: 0.1228 - val_accuracy: 0.9716 - val_loss: 0.1457\n",
      "Epoch 10/10\n",
      "\u001b[1m757/757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9863 - loss: 0.1174 - val_accuracy: 0.9714 - val_loss: 0.1452\n",
      "\u001b[1m379/379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 5 F1-score: 0.9666\n",
      "Average F1-score: 0.9689\n",
      "\n",
      "Results:\n",
      "filters: 64\n",
      "dropout_rate: 0.3\n",
      "avg_accuracy: 0.9727563466284401\n",
      "avg_precision: 0.969500359359406\n",
      "avg_recall: 0.9685946826617794\n",
      "avg_f1_score: 0.968854669988494\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding,\n",
    "    Conv1D,\n",
    "    GlobalMaxPooling1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Input,\n",
    ")\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "def train_word2vec_and_create_embeddings(\n",
    "    train_texts, word_index, max_words, embedding_dim=100\n",
    "):\n",
    "    \"\"\"Train Word2Vec on training data only and create embedding matrix\"\"\"\n",
    "    # Train Word2Vec on training data only\n",
    "    train_sentences = [text.split() for text in train_texts]\n",
    "    word2vec_model = Word2Vec(\n",
    "        sentences=train_sentences,\n",
    "        vector_size=embedding_dim,\n",
    "        window=5,\n",
    "        min_count=2,\n",
    "        workers=4,\n",
    "    )\n",
    "\n",
    "    # Create embedding matrix with correct dimensions\n",
    "    vocab_size = min(max_words, len(word_index) + 1)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i < vocab_size:  # Only include words within max_words limit\n",
    "            if word in word2vec_model.wv:\n",
    "                embedding_matrix[i] = word2vec_model.wv[word]\n",
    "            else:\n",
    "                embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def create_model(\n",
    "    max_sequence_length,\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    embedding_matrix,\n",
    "    filters,\n",
    "    dropout_rate,\n",
    "):\n",
    "    input_layer = Input(shape=(max_sequence_length,))\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=True,\n",
    "    )(input_layer)\n",
    "\n",
    "    x = Conv1D(\n",
    "        filters=filters, kernel_size=5, activation=\"relu\", kernel_regularizer=l2(0.01)\n",
    "    )(embedding_layer)\n",
    "\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(64, activation=\"relu\", kernel_regularizer=l2(0.01))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    output_layer = Dense(1, activation=\"sigmoid\", kernel_regularizer=l2(0.01))(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    filters = 64\n",
    "    dropout_rate = 0.3\n",
    "\n",
    "    # Initialize variables to track results\n",
    "    results = []\n",
    "    best_params = None\n",
    "\n",
    "    # Constants\n",
    "    max_words = 10000\n",
    "    max_sequence_length = 300\n",
    "    embedding_dim = 100\n",
    "\n",
    "    print(f\"\\nTesting filters={filters}, dropout_rate={dropout_rate}\")\n",
    "\n",
    "    # Initialize cross-validation\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold, (train_idx, val_idx) in enumerate(\n",
    "        kfold.split(data[\"processed_full_content\"], data[\"label\"]), 1\n",
    "    ):\n",
    "        print(f\"\\nFold {fold}\")\n",
    "\n",
    "        # Split data\n",
    "        train_texts = data[\"processed_full_content\"].iloc[train_idx]\n",
    "        val_texts = data[\"processed_full_content\"].iloc[val_idx]\n",
    "        y_train = data[\"label\"].iloc[train_idx]\n",
    "        y_val = data[\"label\"].iloc[val_idx]\n",
    "\n",
    "        # Fit tokenizer on training data only\n",
    "        tokenizer = Tokenizer(num_words=max_words)\n",
    "        tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "        # Convert texts to sequences\n",
    "        X_train = pad_sequences(\n",
    "            tokenizer.texts_to_sequences(train_texts), maxlen=max_sequence_length\n",
    "        )\n",
    "        X_val = pad_sequences(\n",
    "            tokenizer.texts_to_sequences(val_texts), maxlen=max_sequence_length\n",
    "        )\n",
    "\n",
    "        # Get vocab size for this fold\n",
    "        vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
    "\n",
    "        # Create embedding matrix using training data only\n",
    "        embedding_matrix = train_word2vec_and_create_embeddings(\n",
    "            train_texts, tokenizer.word_index, max_words, embedding_dim\n",
    "        )\n",
    "\n",
    "        # Create and train model\n",
    "        model = create_model(\n",
    "            max_sequence_length=max_sequence_length,\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            embedding_matrix=embedding_matrix,\n",
    "            filters=filters,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=10,\n",
    "            batch_size=64,\n",
    "            validation_data=(X_val, y_val),\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        # Evaluate using F1-score\n",
    "        y_pred = (model.predict(X_val) > 0.5).astype(int)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision = precision_score(y_val, y_pred)\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        print(f\"Fold {fold} F1-score: {f1:.4f}\")\n",
    "\n",
    "    # Calculate average score for this parameter combination\n",
    "    avg_score = np.mean(f1_scores)\n",
    "    print(f\"Average F1-score: {avg_score:.4f}\")\n",
    "\n",
    "    # Store results\n",
    "    results.append(\n",
    "        {\n",
    "            \"filters\": filters,\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "            \"avg_accuracy\": np.mean(accuracy_scores),\n",
    "            \"avg_precision\": np.mean(precision_scores),\n",
    "            \"avg_recall\": np.mean(recall_scores),\n",
    "            \"avg_f1_score\": avg_score,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"\\nResults:\")\n",
    "    for result in results:\n",
    "        for key, value in result.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, epochs=10, batch_size=64):\n",
    "    # Set seeds for reproducibility\n",
    "    seed = 42\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    filters = 64\n",
    "    dropout_rate = 0.3\n",
    "\n",
    "    # Initialize variables to track results\n",
    "    results = []\n",
    "    best_params = None\n",
    "\n",
    "    # Constants\n",
    "    max_words = 10000\n",
    "    max_sequence_length = 300\n",
    "    embedding_dim = 100\n",
    "\n",
    "    print(f\"\\nTesting filters={filters}, dropout_rate={dropout_rate}\")\n",
    "\n",
    "    # Initialize cross-validation\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    for fold, (train_idx, val_idx) in enumerate(\n",
    "        kfold.split(data[\"processed_full_content\"], data[\"label\"]), 1\n",
    "    ):\n",
    "        print(f\"\\nFold {fold}\")\n",
    "\n",
    "        # Split data\n",
    "        train_texts = data[\"processed_full_content\"].iloc[train_idx]\n",
    "        val_texts = data[\"processed_full_content\"].iloc[val_idx]\n",
    "        y_train = data[\"label\"].iloc[train_idx]\n",
    "        y_val = data[\"label\"].iloc[val_idx]\n",
    "\n",
    "        # Fit tokenizer on training data only\n",
    "        tokenizer = Tokenizer(num_words=max_words)\n",
    "        tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "        # Convert texts to sequences\n",
    "        X_train = pad_sequences(\n",
    "            tokenizer.texts_to_sequences(train_texts), maxlen=max_sequence_length\n",
    "        )\n",
    "        X_val = pad_sequences(\n",
    "            tokenizer.texts_to_sequences(val_texts), maxlen=max_sequence_length\n",
    "        )\n",
    "\n",
    "        # Get vocab size for this fold\n",
    "        vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
    "\n",
    "        # Create embedding matrix using training data only\n",
    "        embedding_matrix = train_word2vec_and_create_embeddings(\n",
    "            train_texts, tokenizer.word_index, max_words, embedding_dim\n",
    "        )\n",
    "\n",
    "        # Create and train model\n",
    "        model = create_model(\n",
    "            max_sequence_length=max_sequence_length,\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            embedding_matrix=embedding_matrix,\n",
    "            filters=filters,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, y_val),\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        # Evaluate using F1-score\n",
    "        y_pred = (model.predict(X_val) > 0.5).astype(int)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision = precision_score(y_val, y_pred)\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        print(f\"Fold {fold} F1-score: {f1:.4f}\")\n",
    "        break\n",
    "\n",
    "    # Calculate average score for this parameter combination\n",
    "    avg_score = np.mean(f1_scores)\n",
    "    print(f\"Average F1-score: {avg_score:.4f}\")\n",
    "\n",
    "    # Store results\n",
    "    result = {\n",
    "        \"filters\": filters,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"accuracy\": np.mean(accuracy_scores),\n",
    "        \"precision\": np.mean(precision_scores),\n",
    "        \"recall\": np.mean(recall_scores),\n",
    "        \"f1_score\": avg_score,\n",
    "    }\n",
    "\n",
    "    print(\"\\nResult:\")\n",
    "    for key, value in result.items():\n",
    "        print(f\"\\t{key}: {value}\")\n",
    "\n",
    "    return model, train_idx, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing filters=64, dropout_rate=0.3\n",
      "\n",
      "Fold 1\n",
      "Epoch 1/10\n",
      "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - accuracy: 0.8776 - loss: 1.0792 - val_accuracy: 0.9567 - val_loss: 0.2590\n",
      "Epoch 2/10\n",
      "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - accuracy: 0.9640 - loss: 0.2318 - val_accuracy: 0.9671 - val_loss: 0.1789\n",
      "Epoch 3/10\n",
      "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - accuracy: 0.9696 - loss: 0.1727 - val_accuracy: 0.9707 - val_loss: 0.1590\n",
      "Epoch 4/10\n",
      "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - accuracy: 0.9731 - loss: 0.1581 - val_accuracy: 0.9722 - val_loss: 0.1526\n",
      "Epoch 5/10\n",
      "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - accuracy: 0.9771 - loss: 0.1457 - val_accuracy: 0.9752 - val_loss: 0.1457\n",
      "Epoch 6/10\n",
      "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.9795 - loss: 0.1345 - val_accuracy: 0.9779 - val_loss: 0.1392\n",
      "Epoch 7/10\n",
      "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.9823 - loss: 0.1266 - val_accuracy: 0.9745 - val_loss: 0.1430\n",
      "Epoch 8/10\n",
      "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.9848 - loss: 0.1199 - val_accuracy: 0.9769 - val_loss: 0.1359\n",
      "Epoch 9/10\n",
      "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.9873 - loss: 0.1137 - val_accuracy: 0.9775 - val_loss: 0.1340\n",
      "Epoch 10/10\n",
      "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - accuracy: 0.9882 - loss: 0.1099 - val_accuracy: 0.9762 - val_loss: 0.1356\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 1 F1-score: 0.9727\n",
      "Average F1-score: 0.9727\n",
      "\n",
      "Result:\n",
      "\tfilters: 64\n",
      "\tdropout_rate: 0.3\n",
      "\taccuracy: 0.976198347107438\n",
      "\tprecision: 0.9749525616698292\n",
      "\trecall: 0.9705326785039667\n",
      "\tf1_score: 0.9727375993941689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<Functional name=functional_5, built=True>,\n",
       " array([    0,     1,     2, ..., 60488, 60489, 60490]),\n",
       " array([    9,    27,    51, ..., 60451, 60464, 60482]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, train_idx, val_idx = train_model(data, epochs=10, batch_size=64)\n",
    "model, train_idx, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6050"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6050 entries, 9 to 60482\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   label         6050 non-null   int64 \n",
      " 1   full_content  6050 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 141.8+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df = data.iloc[val_idx].copy()\n",
    "test_df = test_df[[\"label\", \"full_content\"]]\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"results/CNN_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">296</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,064</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_5          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m1,000,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m296\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m32,064\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_5          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,108,869</span> (11.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,108,869\u001b[0m (11.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,036,289</span> (3.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,036,289\u001b[0m (3.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,072,580</span> (7.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,072,580\u001b[0m (7.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model2 = load_model(\"results/CNN_model.keras\")\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_obj = {\n",
    "    \"train_idx\": train_idx.tolist(),\n",
    "    \"val_idx\": val_idx.tolist(),\n",
    "}\n",
    "json.dump(json_obj, open(\"results/train_val_idx.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data, val_idx):\n",
    "    max_words = 10000\n",
    "    max_sequence_length = 300\n",
    "\n",
    "    train_idx = np.setdiff1d(np.arange(len(data)), val_idx)\n",
    "    train_texts = data[\"processed_full_content\"].iloc[train_idx]\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    y_val = data[\"label\"].iloc[val_idx]\n",
    "    val_texts = data[\"processed_full_content\"].iloc[val_idx]\n",
    "\n",
    "    X_val = pad_sequences(\n",
    "        tokenizer.texts_to_sequences(val_texts), maxlen=max_sequence_length\n",
    "    )\n",
    "    y_pred = (model.predict(X_val) > 0.5).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Accuracy: 0.9762\n",
      "Precision: 0.9750\n",
      "Recall: 0.9705\n",
      "F1: 0.9727\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, data, val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Accuracy: 0.9762\n",
      "Precision: 0.9750\n",
      "Recall: 0.9705\n",
      "F1: 0.9727\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model2, data, val_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
