{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working dir: /Users/inflaton/code/engd/papers/DM-Fake-News-Detection\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if \"workding_dir\" not in locals():\n",
    "    workding_dir = str(Path.cwd().parent)\n",
    "os.chdir(workding_dir)\n",
    "sys.path.append(workding_dir)\n",
    "print(\"working dir:\", workding_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying our best model (CNN + Word2Vec) on the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# For concurrency (running functions in parallel)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# For caching (to speed up repeated function calls)\n",
    "from functools import lru_cache\n",
    "\n",
    "# For progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define function to process text\n",
    "import string\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# For concurrency (running functions in parallel)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# For caching (to speed up repeated function calls)\n",
    "from functools import lru_cache\n",
    "\n",
    "# For progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting and Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Language Detection packages\n",
    "# `langdetect` for detecting language\n",
    "from langdetect import detect as langdetect_detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# `langid` for an alternative language detection method\n",
    "from langid import classify as langid_classify\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Part-of-speech tagging\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Regular expressions for text pattern matching\n",
    "import re\n",
    "\n",
    "\n",
    "def process_full_review(text):\n",
    "    # Convert to lowercase and tokenize\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    stemmer = PorterStemmer()\n",
    "    # List of stopwords\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    allowed_words = [\n",
    "        \"no\",\n",
    "        \"not\",\n",
    "        \"don't\",\n",
    "        \"dont\",\n",
    "        \"don\",\n",
    "        \"but\",\n",
    "        \"however\",\n",
    "        \"never\",\n",
    "        \"wasn't\",\n",
    "        \"wasnt\",\n",
    "        \"shouldn't\",\n",
    "        \"shouldnt\",\n",
    "        \"mustn't\",\n",
    "        \"musnt\",\n",
    "    ]\n",
    "\n",
    "    stemmed = [\n",
    "        stemmer.stem(word)\n",
    "        for word in tokens\n",
    "        if word not in stop_words or word in allowed_words\n",
    "    ]\n",
    "    return \" \".join(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'full_content', 'processed_full_content'],\n",
       "        num_rows: 54441\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "\n",
    "datasets = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": [\n",
    "            \"dataset/train_data_1.csv\",\n",
    "            \"dataset/train_data_2.csv\",\n",
    "            \"dataset/train_data_3.csv\",\n",
    "            \"dataset/train_data_4.csv\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inflaton/anaconda3/envs/fake-news/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 9 variables whereas the saved optimizer has 16 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">296</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,064</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_6          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m1,000,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m296\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m32,064\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_6          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,072,580</span> (7.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,072,580\u001b[0m (7.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,036,289</span> (3.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,036,289\u001b[0m (3.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,036,291</span> (3.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,036,291\u001b[0m (3.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"results/CNN_model.keras\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model, train_data, val_data, force_reprocess=False):\n",
    "    # Apply process_full_review function with tqdm progress bar and expand the results into separate columns.\n",
    "    processed_columns = \"processed_full_content\"\n",
    "    if force_reprocess or processed_columns not in train_data.columns:\n",
    "        # Enable tqdm for pandas (progress bar)\n",
    "        tqdm.pandas(desc=\"Processing Train Data\")\n",
    "        \n",
    "        train_data[processed_columns] = train_data[\"full_content\"].progress_apply(\n",
    "            lambda x: pd.Series(process_full_review(x))\n",
    "        )\n",
    "\n",
    "    if force_reprocess or processed_columns not in val_data.columns:\n",
    "        # Enable tqdm for pandas (progress bar)\n",
    "        tqdm.pandas(desc=\"Processing Val Data\")\n",
    "\n",
    "        # Apply process_full_review function with tqdm progress bar and expand the results into separate columns.\n",
    "        val_data[processed_columns] = val_data[\"full_content\"].progress_apply(\n",
    "            lambda x: pd.Series(process_full_review(x))\n",
    "        )\n",
    "\n",
    "    print(\"Evaluating Model\")\n",
    "    \n",
    "    max_words = 10000\n",
    "    max_sequence_length = 300\n",
    "\n",
    "    train_texts = train_data[\"processed_full_content\"]\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    val_texts = val_data[\"processed_full_content\"]\n",
    "\n",
    "    X_val = pad_sequences(\n",
    "        tokenizer.texts_to_sequences(val_texts), maxlen=max_sequence_length\n",
    "    )\n",
    "    y_pred = (model.predict(X_val) > 0.5).astype(int)\n",
    "\n",
    "    if \"label\" not in val_data.columns:\n",
    "        return y_pred\n",
    "\n",
    "    y_val = val_data[\"label\"]\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1275 entries, 0 to 1274\n",
      "Data columns (total 3 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   source                  1275 non-null   object\n",
      " 1   full_content            1275 non-null   object\n",
      " 2   processed_full_content  1275 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 30.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train = datasets[\"train\"].to_pandas()\n",
    "df_test = pd.read_csv(\"dataset/scrapped_news.csv\")\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "Prediction Results:\n",
      "Total articles: 1275\n",
      "Predicted Real: [587]\n",
      "Predicted Fake: [688]\n"
     ]
    }
   ],
   "source": [
    "predictions = evaluate_model(model, df_train, df_test)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nPrediction Results:\")\n",
    "print(f\"Total articles: {len(predictions)}\")\n",
    "print(f\"Predicted Real: {sum(predictions == 1)}\")\n",
    "print(f\"Predicted Fake: {sum(predictions == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"dataset/scrapped_news.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions by Source:\n",
      "                  Predicted Real  Predicted Fake\n",
      "source                                          \n",
      "AP                           120              18\n",
      "BBC                           51              20\n",
      "Breitbart                    137              37\n",
      "CNN                           66              32\n",
      "Guardian                      43              45\n",
      "NPR                           60              17\n",
      "Natural News                  57              71\n",
      "News Max                      84             105\n",
      "The Daily Caller              13             145\n",
      "Zerohedge                     57              97\n",
      "\n",
      "Percentage of Fake News by Source:\n",
      "source\n",
      "The Daily Caller    91.772152\n",
      "Zerohedge           62.987013\n",
      "News Max            55.555556\n",
      "Natural News        55.468750\n",
      "Guardian            51.136364\n",
      "CNN                 32.653061\n",
      "BBC                 28.169014\n",
      "NPR                 22.077922\n",
      "Breitbart           21.264368\n",
      "AP                  13.043478\n",
      "Name: predicted_label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data_copy = df_test.copy()\n",
    "\n",
    "# Add predictions to scraped data\n",
    "data_copy['predicted_label'] = predictions\n",
    "\n",
    "# Print predictions by source\n",
    "print(\"\\nPredictions by Source:\")\n",
    "source_predictions = data_copy.groupby('source')['predicted_label'].value_counts().unstack()\n",
    "source_predictions.columns = ['Predicted Real', 'Predicted Fake']\n",
    "print(source_predictions)\n",
    "\n",
    "# Calculate percentage of fake news by source\n",
    "fake_percentages = data_copy.groupby('source')['predicted_label'].mean() * 100\n",
    "print(\"\\nPercentage of Fake News by Source:\")\n",
    "print(fake_percentages.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:  Breitbart\n",
      "House Republicans say the Biden-Harris White House might have broken the law when they altered President Joe Biden’s remarks in the official transcript to imply he did not call Trump supporters “garbage.”\n",
      "Biden on Tuesday during a video call with Voto Latino in support of Vice President Kamala Harris’s presidential campaign said, “The only garbage I see floating out there is his supporters.”\n",
      "However, after facing immediate backlash from Republicans and even some Democrats, the White House claimed that Biden did not call Trump supporters “garbage,” but was instead referring to one Trump supporter — namely, comedian Tony Hinchliffe.\n",
      "The White House released a transcript that reinforced that argument, adding an apostrophe to “supporters” to read “supporter’s,” and then adding an em-dash to make it seem like Biden had not completed his sentence.\n",
      "The transcript said (emphasis added):\n",
      "The only garbage I see floating out there is his supporter’s — his — his demonization of Latinos is unconscionable, and it’s un-American.\n",
      "Hinchliffe, known for his off-color jokes, had performed at Trump’s Madison Square Garden rally on Sunday and had joked that Puerto Rico was a “floating island of garbage.”\n",
      "Biden also later posted on X that he was referring to Hinchliffe’s “demonization of Latinos.”\n",
      "According to House Republican Conference Chair Elise Stefanik (R-NY) and House Oversight Committee Chairman James Comer (R-KY), the White House’s addition of an apostrophe may have violated the Presidential Records Act.\n",
      "In a letter to White House Counsel Edward Siskel, they demanded that the White House retain and preserve all documents and internal communications related to Biden’s statement and the release of the inaccurate transcript.\n",
      "They also called on the White House to correct the transcript to reflect what Biden said. They said:\n",
      "President Biden’s vindictive words were unsurprising, given his previous statements regarding people who choose not to vote for his preferred candidate. Unsurprising too were the White House’s actions after he said them. Instead of apologizing or clarifying President Biden’s words, the White House instead sought to change them (despite them being recorded on video) by releasing a false transcript of his remarks. The move is not only craven, but it also appears to be in violation of federal law, including the Presidential Records Act of 1978.\n",
      "“White House staff cannot rewrite the words of the President of the United States to be more politically on message. Though President Biden’s relevance continues to diminish, his words continue to matter, even as they become increasingly divisive and erratic,” they added.\n",
      "Follow Breitbart News’s Kristina Wong on ”X”, Truth Social, or on Facebook. GOP: WH May Have Illegally Altered Biden's 'Garbage' Remark\n"
     ]
    }
   ],
   "source": [
    "print(\"source: \", data_copy[data_copy[\"predicted_label\"] == 0].iloc[0][\"source\"])\n",
    "print(data_copy[data_copy[\"predicted_label\"] == 0].iloc[0][\"full_content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information you’ve provided is accurate. In October 2024, during a video call with Voto Latino, President Joe Biden criticized remarks made by comedian Tony Hinchcliffe at a Trump rally, where Hinchcliffe referred to Puerto Rico as a “floating island of garbage.” Biden stated, “The only garbage I see floating out there is his supporters.” Following backlash, the White House released an official transcript altering “supporters” to “supporter’s,” suggesting Biden was referring specifically to Hinchcliffe. This modification led to objections from White House stenographers and prompted House Republicans to question the legality of altering official records, citing potential violations of the Presidential Records Act.  ￼ ￼\n",
    "\n",
    "For more context, here’s a news segment covering the incident: [AP sources: White House altered record of Biden’s ‘garbage’ remarks despite stenographer concerns](https://apnews.com/article/biden-garbage-transcript-puerto-rico-trump-326e2f516a94a470a423011a946b6252?utm_source=chatgpt.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:  Guardian\n",
      "Michigan congresswoman Rashida Tlaib declined to endorse Kamala Harris at a union rally in Detroit, where the war in Gaza is the top issue for the largest block of Arab American voters in the country.\n",
      "Tlaib, the first Palestinian American woman to serve in Congress, is the only one of the so-called leftist “Squad” that has not endorsed the Democrat candidate. The other three members – Ayanna Pressley of Massachusetts, Ilhan Omar of Minnesota and Alexandria Ocasio-Cortez of New York – endorsed Harris in July.\n",
      "“Don’t underestimate the power you all have,” Tlaib told a get-out-the-vote United Auto Workers rallygoers. “More than those ads, those lawn signs, those billboards, you all have more power to turn out people that understand we’ve got to fight back against corporate greed in our country.”\n",
      "Tlaib’s non-endorsement of Harris comes as a voter survey published on Friday suggested that 43% of Muslim American voters support the Green party candidate, Jill Stein.\n",
      "After Hillary Clinton’s loss to Donald Trump in 2016, Democrats blamed Stein voters for the loss of Michigan and Wisconsin to the Republican candidate. Some Democrats fear that the same scenario could play out again next week.\n",
      "Earlier this year, during the presidential primary campaigns, about 100,000 Michigan voters marked their ballots “uncommitted” as a mark of protest against the Biden administration’s support of Israel’s invasion of Gaza after the cross -border Hamas attack in October last year that killed 1,200 people and took more than 200 hostages, mostly civilians.\n",
      "Israel’s attack on Gaza has since killed more than 40,000 people, with many of them women and children. In Lebanon, where Israel has now invaded to fight with Iran-backed Hezbollah, more than 2,897 people have been killed and 13,150 wounded, the country’s health ministry reports. A quarter of those killed were women and children.\n",
      "The US has been a staunch ally of Israel during the fighting, continuing to send arms to the country and limiting its public criticism of Israeli actions.\n",
      "Tlaib has been critical of the Democratic party’s position on the growing and bloody conflict, saying it was “hard not to feel invisible” after the party did not include a Palestinian American speaker at its convention in Chicago in August.\n",
      "In an interview with Zeteo, the news organization founded by former MSNBC host (and Guardian contributor) Mehdi Hasan, Tlaib said the omission “made it clear with their speakers that they value Israeli children more than Palestinian children”.\n",
      "“Our trauma and pain feel unseen and ignored by both parties,” she added. “One party uses our identity as a slur, and the other refuses to hear from us. Where is the shared humanity? Ignoring us won’t stop the genocide.”\n",
      "Harris has faced continued protests on the trail, as demonstrators call for her to break with President Joe Biden and support an arms embargo on Israel. Harris has said Israel “has right to defend itself”, and that Palestinians need “dignity, security”.\n",
      "Confronted by a protester in Wisconsin two weeks ago who accused the Jewish state of genocide, Harris said: “I know what you’re speaking of. I want a ceasefire. I want the hostage deal done. I want the war to end.”\n",
      "At a rally in Dearborn earlier on Friday, Tlaib the criticized Republican presidential nominee, Donald Trump, who has been endorsed by the Muslim mayors of Dearborn Heights and Hamtramck.\n",
      "“Trump is a proud Islamophobe + serial liar who doesn’t stand for peace,” Tlaib posted on X. “The reality is that the Biden admin’s unconditional support for genocide is what got us here. This should be a wake-up call for those who continue to support genocide. This election didn’t have to be close.” Michigan congresswoman Rashida Tlaib declines to endorse Kamala Harris\n"
     ]
    }
   ],
   "source": [
    "print(\"source: \", data_copy[data_copy[\"predicted_label\"] == 1].iloc[-1][\"source\"])\n",
    "print(data_copy[data_copy[\"predicted_label\"] == 1].iloc[-1][\"full_content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information you’ve provided is accurate. In November 2024, Congresswoman Rashida Tlaib declined to endorse Vice President Kamala Harris during a United Auto Workers rally in Detroit. This decision was influenced by the ongoing war in Gaza, a significant concern for Arab American voters in Michigan. Tlaib, the first Palestinian American woman in Congress, has been vocal in her criticism of the Democratic Party’s stance on the conflict, expressing feelings of invisibility and frustration over the lack of Palestinian American representation at the party’s convention. Her non-endorsement coincided with a survey indicating that 43% of Muslim American voters supported Green Party candidate Jill Stein, raising concerns among Democrats about potential impacts on the election outcome.  ￼\n",
    "\n",
    "For more context, here’s a news segment covering Tlaib’s decision: [Michigan congresswoman Rashida Tlaib declines to endorse Kamala Harris](https://www.theguardian.com/us-news/2024/nov/02/rashida-tlaib-decline-endorsement-kamala-harris)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
