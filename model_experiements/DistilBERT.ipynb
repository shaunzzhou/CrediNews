{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Detection to Credibility: A Machine Learning Framework for Assessing News Source Reliability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# For concurrency (running functions in parallel)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# For caching (to speed up repeated function calls)\n",
    "from functools import lru_cache\n",
    "\n",
    "# For progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting and Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Language Detection packages\n",
    "# `langdetect` for detecting language\n",
    "from langdetect import detect as langdetect_detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "# `langid` for an alternative language detection method\n",
    "from langid import classify as langid_classify\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Part-of-speech tagging\n",
    "from nltk import pos_tag\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "# Regular expressions for text pattern matching\n",
    "import re\n",
    "\n",
    "# Word Cloud generation\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>full_content</th>\n",
       "      <th>processed_full_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No comment is expected from Barack Obama Membe...</td>\n",
       "      <td>no comment expect barack obama member fyf911 f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Did they post their votes for Hillary already?</td>\n",
       "      <td>post vote hillari alreadi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Now, most of the demonstrators gathered last n...</td>\n",
       "      <td>demonstr gather last night exercis constitut p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>A dozen politically active pastors came here f...</td>\n",
       "      <td>dozen polit activ pastor came privat dinner fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The RS-28 Sarmat missile, dubbed Satan 2, will...</td>\n",
       "      <td>rs-28 sarmat missil dub satan 2 replac ss-18 f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63855</th>\n",
       "      <td>0</td>\n",
       "      <td>WASHINGTON (Reuters) - Hackers believed to be ...</td>\n",
       "      <td>washington reuter hacker believ work russian g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63856</th>\n",
       "      <td>1</td>\n",
       "      <td>You know, because in fantasyland Republicans n...</td>\n",
       "      <td>know fantasyland republican never question cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63857</th>\n",
       "      <td>0</td>\n",
       "      <td>Migrants Refuse To Leave Train At Refugee Camp...</td>\n",
       "      <td>migrant refus leav train refuge camp hungari t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63858</th>\n",
       "      <td>0</td>\n",
       "      <td>MEXICO CITY (Reuters) - Donald Trump’s combati...</td>\n",
       "      <td>mexico citi reuter donald trump ’ comb style b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63859</th>\n",
       "      <td>1</td>\n",
       "      <td>Goldman Sachs Endorses Hillary Clinton For Pre...</td>\n",
       "      <td>goldman sach endors hillari clinton presid gol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63860 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                       full_content  \\\n",
       "0          1  No comment is expected from Barack Obama Membe...   \n",
       "1          1     Did they post their votes for Hillary already?   \n",
       "2          1  Now, most of the demonstrators gathered last n...   \n",
       "3          0  A dozen politically active pastors came here f...   \n",
       "4          1  The RS-28 Sarmat missile, dubbed Satan 2, will...   \n",
       "...      ...                                                ...   \n",
       "63855      0  WASHINGTON (Reuters) - Hackers believed to be ...   \n",
       "63856      1  You know, because in fantasyland Republicans n...   \n",
       "63857      0  Migrants Refuse To Leave Train At Refugee Camp...   \n",
       "63858      0  MEXICO CITY (Reuters) - Donald Trump’s combati...   \n",
       "63859      1  Goldman Sachs Endorses Hillary Clinton For Pre...   \n",
       "\n",
       "                                  processed_full_content  \n",
       "0      no comment expect barack obama member fyf911 f...  \n",
       "1                              post vote hillari alreadi  \n",
       "2      demonstr gather last night exercis constitut p...  \n",
       "3      dozen polit activ pastor came privat dinner fr...  \n",
       "4      rs-28 sarmat missil dub satan 2 replac ss-18 f...  \n",
       "...                                                  ...  \n",
       "63855  washington reuter hacker believ work russian g...  \n",
       "63856  know fantasyland republican never question cit...  \n",
       "63857  migrant refus leav train refuge camp hungari t...  \n",
       "63858  mexico citi reuter donald trump ’ comb style b...  \n",
       "63859  goldman sach endors hillari clinton presid gol...  \n",
       "\n",
       "[63860 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic DistilBERT\n",
    "\n",
    "DistilBERT is a smaller, faster, and lighter version of BERT, designed to retain most of BERT's language understanding capabilities while being more computationally efficient.\n",
    "\n",
    "DistilBERT has only 6 layers instead of BERT's 12, which makes it half the size of BERT in terms of layers. However, it retains the same hidden size (768), meaning it still processes and represents data similarly to BERT but with fewer computational steps. This results in a smaller number of parameters, making DistilBERT about 40-60% faster to train and use in inference.\n",
    "\n",
    "DistilBERT is trained using knowledge distillation, a technique where a smaller model (the \"student\", DistilBERT) learns to mimic a larger model (the \"teacher\", BERT) rather than directly learning from the raw data. During training, the student model doesn't just learn from the labelled dataset but also from the \"soft labels\" (probabilities) provided by the teacher model. This method allows DistilBERT to capture much of the knowledge from the original BERT model even with fewer layers, preserving 97% of BERT's performance on many NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d, val_d, train_labels, val_labels = train_test_split(data['processed_full_content'],data['label'],test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = list(train_d)\n",
    "texts_val = list(val_d)\n",
    "\n",
    "max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts_train = tokenizer(texts_train, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length)\n",
    "tokenized_texts_val = tokenizer(texts_val, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  3290, 25022,  3775,  2128, 19901,  7426, 21877,  7361,  2140,\n",
       "         2915,  2757,  9857,  2305,  4319, 24497, 18622,  2102,  2415,  2642,\n",
       "         3290,  2112, 26568, 29469,  2954,  2048, 13675, 27605,  2078,  6080,\n",
       "        14056,  2110,  4905,  4962,  2099,  2125,  2594,  2056,  5607,  2165,\n",
       "         2173, 17496,  5101, 25022,  3775, 28480,  6178,  4183,  3675,  2110,\n",
       "        28480,  2187,  2403, 21877,  7361,  2140,  2757,  1022,  1999,  9103,\n",
       "         2099, 14056,  9758,   102])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts_train['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts_train['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(list(train_labels))\n",
    "val_labels = torch.tensor(list(val_labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(tokenized_texts_train['input_ids'], tokenized_texts_train['attention_mask'], train_labels)\n",
    "val_dataset = TensorDataset(tokenized_texts_val['input_ids'], tokenized_texts_val['attention_mask'], val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, prefetch_factor=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "train_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Training:   0%|          | 0/799 [00:00<?, ?it/s]DistilBertSdpaAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "Epoch 1/5 - Training: 100%|██████████| 799/799 [04:51<00:00,  2.75it/s]\n",
      "Epoch 1/5 - Validation: 100%|██████████| 200/200 [00:31<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Training Loss: 0.2304, Training Accuracy: 0.9022\n",
      "Training Precision: 0.9022, Training Recall: 0.9022, Training F1: 0.9022\n",
      "Validation Loss: 0.1637, Validation Accuracy: 0.9374\n",
      "Validation Precision: 0.9413, Validation Recall: 0.9374, Validation F1: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Training: 100%|██████████| 799/799 [05:13<00:00,  2.55it/s]\n",
      "Epoch 2/5 - Validation: 100%|██████████| 200/200 [00:31<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/5\n",
      "Training Loss: 0.1151, Training Accuracy: 0.9564\n",
      "Training Precision: 0.9565, Training Recall: 0.9564, Training F1: 0.9565\n",
      "Validation Loss: 0.1224, Validation Accuracy: 0.9516\n",
      "Validation Precision: 0.9518, Validation Recall: 0.9516, Validation F1: 0.9516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Training: 100%|██████████| 799/799 [04:58<00:00,  2.68it/s]\n",
      "Epoch 3/5 - Validation: 100%|██████████| 200/200 [00:31<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/5\n",
      "Training Loss: 0.1093, Training Accuracy: 0.9591\n",
      "Training Precision: 0.9591, Training Recall: 0.9591, Training F1: 0.9591\n",
      "Validation Loss: 0.1221, Validation Accuracy: 0.9519\n",
      "Validation Precision: 0.9521, Validation Recall: 0.9519, Validation F1: 0.9520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Training: 100%|██████████| 799/799 [05:04<00:00,  2.62it/s]\n",
      "Epoch 4/5 - Validation: 100%|██████████| 200/200 [00:34<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/5\n",
      "Training Loss: 0.1080, Training Accuracy: 0.9594\n",
      "Training Precision: 0.9594, Training Recall: 0.9594, Training F1: 0.9594\n",
      "Validation Loss: 0.1223, Validation Accuracy: 0.9518\n",
      "Validation Precision: 0.9520, Validation Recall: 0.9518, Validation F1: 0.9519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Training: 100%|██████████| 799/799 [05:05<00:00,  2.61it/s]\n",
      "Epoch 5/5 - Validation: 100%|██████████| 200/200 [00:31<00:00,  6.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/5\n",
      "Training Loss: 0.1091, Training Accuracy: 0.9592\n",
      "Training Precision: 0.9592, Training Recall: 0.9592, Training F1: 0.9592\n",
      "Validation Loss: 0.1223, Validation Accuracy: 0.9518\n",
      "Validation Precision: 0.9520, Validation Recall: 0.9518, Validation F1: 0.9519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    tr_correct_preds = 0\n",
    "    all_tr_labels = []\n",
    "    all_tr_preds = []\n",
    "\n",
    "    # Use tqdm to create a progress bar for the training loop\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Training\"):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        tr_loss = outputs.loss\n",
    "        train_loss += tr_loss.item()\n",
    "        tr_loss.backward()\n",
    "\n",
    "        tr_logits = outputs.logits\n",
    "        tr_preds = torch.argmax(tr_logits, dim=1)\n",
    "        tr_correct_preds += torch.sum(tr_preds == labels).item()\n",
    "\n",
    "        # Collect predictions and true labels\n",
    "        all_tr_labels.extend(labels.cpu().numpy())\n",
    "        all_tr_preds.extend(tr_preds.cpu().numpy())\n",
    "\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Calculate average training loss and accuracy\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracy = tr_correct_preds / len(train_d)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Calculate Precision, Recall, F1 for training\n",
    "    train_precision = precision_score(all_tr_labels, all_tr_preds, average='weighted')\n",
    "    train_recall = recall_score(all_tr_labels, all_tr_preds, average='weighted')\n",
    "    train_f1 = f1_score(all_tr_labels, all_tr_preds, average='weighted')\n",
    "\n",
    "    # Validation phase with tqdm progress bar\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    all_val_labels = []\n",
    "    all_val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Validation\"):\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_preds += torch.sum(preds == labels).item()\n",
    "\n",
    "            # Collect predictions and true labels\n",
    "            all_val_labels.extend(labels.cpu().numpy())\n",
    "            all_val_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracy = correct_preds / len(val_d)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Calculate Precision, Recall, F1 for validation\n",
    "    val_precision = precision_score(all_val_labels, all_val_preds, average='weighted')\n",
    "    val_recall = recall_score(all_val_labels, all_val_preds, average='weighted')\n",
    "    val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted')\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Training Precision: {train_precision:.4f}, Training Recall: {train_recall:.4f}, Training F1: {train_f1:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation Precision: {val_precision:.4f}, Validation Recall: {val_recall:.4f}, Validation F1: {val_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
