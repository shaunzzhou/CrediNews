{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# For concurrency (running functions in parallel)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# For caching (to speed up repeated function calls)\n",
    "from functools import lru_cache\n",
    "\n",
    "# For progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting and Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Language Detection packages\n",
    "# `langdetect` for detecting language\n",
    "from langdetect import detect as langdetect_detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "# `langid` for an alternative language detection method\n",
    "from langid import classify as langid_classify\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Part-of-speech tagging\n",
    "from nltk import pos_tag\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "# Regular expressions for text pattern matching\n",
    "import re\n",
    "\n",
    "# Word Cloud generation\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation (Loading CSV)\n",
    "\n",
    "Load the processed_data `csv` file into pandas DataFrames\n",
    "- `processed_data.csv` is loaded into `data` DataFrame (stemming has been performed to reduce processing time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                       full_content  \\\n",
      "0      1  No comment is expected from Barack Obama Membe...   \n",
      "1      1     Did they post their votes for Hillary already?   \n",
      "2      1  Now, most of the demonstrators gathered last n...   \n",
      "3      0  A dozen politically active pastors came here f...   \n",
      "4      1  The RS-28 Sarmat missile, dubbed Satan 2, will...   \n",
      "\n",
      "                              processed_full_content  \n",
      "0  no comment expect barack obama member fyf911 f...  \n",
      "1                          post vote hillari alreadi  \n",
      "2  demonstr gather last night exercis constitut p...  \n",
      "3  dozen polit activ pastor came privat dinner fr...  \n",
      "4  rs-28 sarmat missil dub satan 2 replac ss-18 f...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../processed_data.csv')\n",
    "print(data.head())  # Shows the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with countvectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[6705  299]\n",
      " [ 303 5465]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9568    0.9573    0.9570      7004\n",
      "           1     0.9481    0.9475    0.9478      5768\n",
      "\n",
      "    accuracy                         0.9529     12772\n",
      "   macro avg     0.9524    0.9524    0.9524     12772\n",
      "weighted avg     0.9529    0.9529    0.9529     12772\n",
      "\n",
      "\n",
      "Macro Average F1-Score: 0.9524171294036754\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Vectorization using CountVectorizer\n",
    "MAX_FEATURES = 5000  # Number of features to consider\n",
    "vectorizer = CountVectorizer(max_features=MAX_FEATURES)\n",
    "\n",
    "# Convert text to feature vectors\n",
    "X = vectorizer.fit_transform(data['processed_full_content']).toarray()\n",
    "y = data['label']  # Target labels (binary classification: 0 or 1)\n",
    "\n",
    "# Step 2: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the SVM Model\n",
    "# svm = SVC(kernel='linear', random_state=42)\n",
    "svm = SGDClassifier(loss='hinge', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Make Predictions and Evaluate\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(\"\\nMacro Average F1-Score:\", macro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with tf-idf instead of CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[6784  220]\n",
      " [ 403 5365]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9439    0.9686    0.9561      7004\n",
      "           1     0.9606    0.9301    0.9451      5768\n",
      "\n",
      "    accuracy                         0.9512     12772\n",
      "   macro avg     0.9523    0.9494    0.9506     12772\n",
      "weighted avg     0.9515    0.9512    0.9511     12772\n",
      "\n",
      "\n",
      "Macro Average F1-Score: 0.9506117863026156\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Use TF-IDF instead of CountVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=MAX_FEATURES)\n",
    "X = tfidf_vectorizer.fit_transform(data['processed_full_content']).toarray()\n",
    "y = data['label']  # Target labels (binary classification: 0 or 1)\n",
    "\n",
    "# Step 2: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the SVM Model\n",
    "# svm = SVC(kernel='linear', random_state=42)\n",
    "svm = SGDClassifier(loss='hinge', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Make Predictions and Evaluate\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(\"\\nMacro Average F1-Score:\", macro_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[6180  824]\n",
      " [ 593 5175]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9124    0.8824    0.8971      7004\n",
      "           1     0.8626    0.8972    0.8796      5768\n",
      "\n",
      "    accuracy                         0.8891     12772\n",
      "   macro avg     0.8875    0.8898    0.8884     12772\n",
      "weighted avg     0.8900    0.8891    0.8892     12772\n",
      "\n",
      "\n",
      "Macro Average F1-Score: 0.8883629509041846\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_glove(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Load pre-trained GloVe vectors\n",
    "def get_embedding_matrix(data, embeddings_index, embed_dim=300):\n",
    "    embedding_matrix = []\n",
    "    for text in data:\n",
    "        words = text.split()\n",
    "        embeddings = [embeddings_index[word] for word in words if word in embeddings_index]\n",
    "        if embeddings:\n",
    "            document_embedding = np.mean(embeddings, axis=0)  # Average embeddings for the document\n",
    "        else:\n",
    "            document_embedding = np.zeros(embed_dim)  # Zero vector if no embeddings found\n",
    "        embedding_matrix.append(document_embedding)\n",
    "    return np.array(embedding_matrix)\n",
    "\n",
    "# Load GloVe vectors\n",
    "glove_path = r\"c:\\Users\\Admin\\Downloads\\glove.6B\\glove.6B.300d.txt\"\n",
    "embeddings_index = load_glove(glove_path)\n",
    "\n",
    "# Convert text data to GloVe embeddings\n",
    "X_train = get_embedding_matrix(data['processed_full_content'], embeddings_index)\n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm = SGDClassifier(loss='hinge', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(\"\\nMacro Average F1-Score:\", macro_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with TF-IDF and glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[6725  279]\n",
      " [ 461 5307]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9358    0.9602    0.9479      7004\n",
      "           1     0.9501    0.9201    0.9348      5768\n",
      "\n",
      "    accuracy                         0.9421     12772\n",
      "   macro avg     0.9430    0.9401    0.9413     12772\n",
      "weighted avg     0.9423    0.9421    0.9420     12772\n",
      "\n",
      "\n",
      "Macro Average F1-Score: 0.9413376651927967\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Get TF-IDF features\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf.fit_transform(data['processed_full_content'])\n",
    "\n",
    "# Get GloVe embeddings\n",
    "X_glove = get_embedding_matrix(data['processed_full_content'], embeddings_index)\n",
    "\n",
    "# Combine GloVe and TF-IDF features\n",
    "X_combined = hstack((X_tfidf, StandardScaler().fit_transform(X_glove))).toarray()\n",
    "\n",
    "# Split and train model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "svm = SGDClassifier(loss='hinge', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(\"\\nMacro Average F1-Score:\", macro_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Step 1: Load pre-trained Word2Vec embeddings\n",
    "def load_word2vec(file_path):\n",
    "    return KeyedVectors.load_word2vec_format(file_path, binary=True)\n",
    "\n",
    "# Load Word2Vec vectors\n",
    "word2vec_path = r\"c:\\Users\\Admin\\Downloads\\GoogleNews-vectors-negative300.bin\"  # Example path\n",
    "word2vec = load_word2vec(word2vec_path)\n",
    "\n",
    "# Step 2: Convert each document to Word2Vec embeddings by averaging word vectors\n",
    "def get_word2vec_embedding(text, word2vec, embed_dim=300):\n",
    "    words = text.split()\n",
    "    embeddings = [word2vec[word] for word in words if word in word2vec.key_to_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(embed_dim)\n",
    "\n",
    "# Generate Word2Vec embeddings for the entire dataset\n",
    "X_word2vec = np.array([get_word2vec_embedding(text, word2vec) for text in data['processed_full_content']])\n",
    "\n",
    "# Step 3: Generate CountVectorizer features\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X_count = vectorizer.fit_transform(data['processed_full_content'])\n",
    "\n",
    "# Step 4: Combine Word2Vec embeddings and CountVectorizer features\n",
    "X_combined = hstack((X_count, X_word2vec))\n",
    "\n",
    "# Step 5: Train-test split\n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Train a classifier (e.g., SVM) on the combined features\n",
    "svm = SGDClassifier(loss='hinge', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(\"\\nMacro Average F1-Score:\", macro_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with glove + countVectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[6688  316]\n",
      " [ 251 5517]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9638    0.9549    0.9593      7004\n",
      "           1     0.9458    0.9565    0.9511      5768\n",
      "\n",
      "    accuracy                         0.9556     12772\n",
      "   macro avg     0.9548    0.9557    0.9552     12772\n",
      "weighted avg     0.9557    0.9556    0.9556     12772\n",
      "\n",
      "\n",
      "Macro Average F1-Score: 0.9552296680372214\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Step 1: Load pre-trained GloVe embeddings\n",
    "def load_glove(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Load GloVe vectors\n",
    "glove_path = r\"c:\\Users\\Admin\\Downloads\\glove.6B\\glove.6B.300d.txt\"\n",
    "embeddings_index = load_glove(glove_path)\n",
    "\n",
    "# Step 2: Convert each document to GloVe embeddings by averaging word vectors\n",
    "def get_glove_embedding(text, embeddings_index, embed_dim=300):\n",
    "    words = text.split()\n",
    "    embeddings = [embeddings_index[word] for word in words if word in embeddings_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(embed_dim)\n",
    "\n",
    "# Generate GloVe embeddings for the entire dataset\n",
    "X_glove = np.array([get_glove_embedding(text, embeddings_index) for text in data['processed_full_content']])\n",
    "\n",
    "# Step 3: Generate CountVectorizer features\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X_count = vectorizer.fit_transform(data['processed_full_content'])\n",
    "\n",
    "# Step 4: Combine GloVe embeddings and CountVectorizer features\n",
    "# Use hstack to combine sparse CountVectorizer matrix with dense GloVe embeddings\n",
    "X_combined = hstack((X_count, X_glove))\n",
    "\n",
    "# Step 5: Train-test split\n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Train a classifier (e.g., SVM) on the combined features\n",
    "svm = SGDClassifier(loss='hinge', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(\"\\nMacro Average F1-Score:\", macro_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with glove, count vectoriser and Stratified K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - F1 Score (weighted): 0.9540576271559605\n",
      "Fold 1 - Confusion Matrix:\n",
      " [[11101   492]\n",
      " [  486  9208]]\n",
      "Fold 1 - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9581    0.9576    0.9578     11593\n",
      "           1     0.9493    0.9499    0.9496      9694\n",
      "\n",
      "    accuracy                         0.9541     21287\n",
      "   macro avg     0.9537    0.9537    0.9537     21287\n",
      "weighted avg     0.9541    0.9541    0.9541     21287\n",
      "\n",
      "Fold 2 - F1 Score (weighted): 0.9518794502764808\n",
      "Fold 2 - Confusion Matrix:\n",
      " [[11119   474]\n",
      " [  550  9144]]\n",
      "Fold 2 - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9529    0.9591    0.9560     11593\n",
      "           1     0.9507    0.9433    0.9470      9694\n",
      "\n",
      "    accuracy                         0.9519     21287\n",
      "   macro avg     0.9518    0.9512    0.9515     21287\n",
      "weighted avg     0.9519    0.9519    0.9519     21287\n",
      "\n",
      "Fold 3 - F1 Score (weighted): 0.9522081012321895\n",
      "Fold 3 - Confusion Matrix:\n",
      " [[11118   475]\n",
      " [  542  9151]]\n",
      "Fold 3 - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9535    0.9590    0.9563     11593\n",
      "           1     0.9507    0.9441    0.9474      9693\n",
      "\n",
      "    accuracy                         0.9522     21286\n",
      "   macro avg     0.9521    0.9516    0.9518     21286\n",
      "weighted avg     0.9522    0.9522    0.9522     21286\n",
      "\n",
      "\n",
      "Average F1 Score (weighted) across all folds: 0.952715059554877\n",
      "\n",
      "Average Classification Report across all folds:\n",
      "0:\n",
      "  precision: 0.9548\n",
      "  recall: 0.9586\n",
      "  f1-score: 0.9567\n",
      "  support: 11593.0000\n",
      "1:\n",
      "  precision: 0.9502\n",
      "  recall: 0.9457\n",
      "  f1-score: 0.9480\n",
      "  support: 9693.6667\n",
      "accuracy: 0.9527\n",
      "macro avg:\n",
      "  precision: 0.9525\n",
      "  recall: 0.9522\n",
      "  f1-score: 0.9523\n",
      "  support: 21286.6667\n",
      "weighted avg:\n",
      "  precision: 0.9527\n",
      "  recall: 0.9527\n",
      "  f1-score: 0.9527\n",
      "  support: 21286.6667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# Step 1: Load pre-trained GloVe embeddings\n",
    "def load_glove(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Load GloVe vectors\n",
    "glove_path = r\"c:\\Users\\Admin\\Downloads\\glove.6B\\glove.6B.300d.txt\"\n",
    "embeddings_index = load_glove(glove_path)\n",
    "\n",
    "# Step 2: Convert each document to GloVe embeddings by averaging word vectors\n",
    "def get_glove_embedding(text, embeddings_index, embed_dim=300):\n",
    "    words = text.split()\n",
    "    embeddings = [embeddings_index[word] for word in words if word in embeddings_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(embed_dim)\n",
    "\n",
    "# Generate GloVe embeddings for the entire dataset\n",
    "X_glove = np.array([get_glove_embedding(text, embeddings_index) for text in data['processed_full_content']])\n",
    "\n",
    "# Step 3: Generate CountVectorizer features\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X_count = vectorizer.fit_transform(data['processed_full_content'])\n",
    "\n",
    "# Step 4: Combine GloVe embeddings and CountVectorizer features\n",
    "# Use hstack to combine sparse CountVectorizer matrix with dense GloVe embeddings\n",
    "X_combined = hstack((X_count, X_glove))\n",
    "X_combined = csr_matrix(X_combined)  # Convert to csr_matrix for subscriptable indexing\n",
    "\n",
    "# Define labels\n",
    "y = data['label']\n",
    "\n",
    "# Step 5: Set up Stratified K-Fold Cross-Validation\n",
    "kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "f1_scores = []\n",
    "all_classification_reports = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_combined, y):\n",
    "    # Split the data into training and testing sets for the current fold\n",
    "    X_train, X_test = X_combined[train_index], X_combined[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Train the SVM model on the current fold\n",
    "    svm = SGDClassifier(loss='hinge', random_state=42)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    \n",
    "    # Calculate F1-score for the current fold\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "    classification_rep = classification_report(y_test, y_pred, digits=4, output_dict=True)\n",
    "    all_classification_reports.append(classification_rep)\n",
    "    \n",
    "    # Print F1-score and classification report for the current fold\n",
    "    print(f\"Fold {fold} - F1 Score (weighted): {f1}\")\n",
    "    print(f\"Fold {fold} - Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(f\"Fold {fold} - Classification Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    fold += 1\n",
    "\n",
    "# Step 6: Calculate and display average F1-score across all folds\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "print(\"\\nAverage F1 Score (weighted) across all folds:\", avg_f1_score)\n",
    "\n",
    "# Calculate the average classification report across folds\n",
    "avg_classification_report = {}\n",
    "for key in all_classification_reports[0].keys():\n",
    "    if isinstance(all_classification_reports[0][key], dict):\n",
    "        avg_classification_report[key] = {metric: np.mean([report[key][metric] for report in all_classification_reports]) \n",
    "                                          for metric in all_classification_reports[0][key]}\n",
    "    else:\n",
    "        avg_classification_report[key] = np.mean([report[key] for report in all_classification_reports])\n",
    "\n",
    "print(\"\\nAverage Classification Report across all folds:\")\n",
    "for label, metrics in avg_classification_report.items():\n",
    "    if isinstance(metrics, dict):\n",
    "        print(f\"{label}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{label}: {metrics:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - F1 Score (weighted): 0.954830207313335\n",
      "Fold 1 - Confusion Matrix:\n",
      " [[6655  300]\n",
      " [ 277 5540]]\n",
      "Fold 1 - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9600    0.9569    0.9585      6955\n",
      "           1     0.9486    0.9524    0.9505      5817\n",
      "\n",
      "    accuracy                         0.9548     12772\n",
      "   macro avg     0.9543    0.9546    0.9545     12772\n",
      "weighted avg     0.9548    0.9548    0.9548     12772\n",
      "\n",
      "Fold 2 - F1 Score (weighted): 0.9519500861173588\n",
      "Fold 2 - Confusion Matrix:\n",
      " [[6611  345]\n",
      " [ 269 5547]]\n",
      "Fold 2 - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9609    0.9504    0.9556      6956\n",
      "           1     0.9414    0.9537    0.9476      5816\n",
      "\n",
      "    accuracy                         0.9519     12772\n",
      "   macro avg     0.9512    0.9521    0.9516     12772\n",
      "weighted avg     0.9520    0.9519    0.9520     12772\n",
      "\n",
      "Fold 3 - F1 Score (weighted): 0.9556160302718006\n",
      "Fold 3 - Confusion Matrix:\n",
      " [[6656  300]\n",
      " [ 267 5549]]\n",
      "Fold 3 - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9614    0.9569    0.9591      6956\n",
      "           1     0.9487    0.9541    0.9514      5816\n",
      "\n",
      "    accuracy                         0.9556     12772\n",
      "   macro avg     0.9551    0.9555    0.9553     12772\n",
      "weighted avg     0.9556    0.9556    0.9556     12772\n",
      "\n",
      "Fold 4 - F1 Score (weighted): 0.9539084684721251\n",
      "Fold 4 - Confusion Matrix:\n",
      " [[6620  336]\n",
      " [ 253 5563]]\n",
      "Fold 4 - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9632    0.9517    0.9574      6956\n",
      "           1     0.9430    0.9565    0.9497      5816\n",
      "\n",
      "    accuracy                         0.9539     12772\n",
      "   macro avg     0.9531    0.9541    0.9536     12772\n",
      "weighted avg     0.9540    0.9539    0.9539     12772\n",
      "\n",
      "Fold 5 - F1 Score (weighted): 0.9566182761560942\n",
      "Fold 5 - Confusion Matrix:\n",
      " [[6688  268]\n",
      " [ 286 5530]]\n",
      "Fold 5 - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9590    0.9615    0.9602      6956\n",
      "           1     0.9538    0.9508    0.9523      5816\n",
      "\n",
      "    accuracy                         0.9566     12772\n",
      "   macro avg     0.9564    0.9561    0.9563     12772\n",
      "weighted avg     0.9566    0.9566    0.9566     12772\n",
      "\n",
      "\n",
      "Average F1 Score (weighted) across all folds: 0.9545846136661428\n",
      "\n",
      "Average Classification Report across all folds:\n",
      "0:\n",
      "  precision: 0.9609\n",
      "  recall: 0.9555\n",
      "  f1-score: 0.9582\n",
      "  support: 6955.8000\n",
      "1:\n",
      "  precision: 0.9471\n",
      "  recall: 0.9535\n",
      "  f1-score: 0.9503\n",
      "  support: 5816.2000\n",
      "accuracy: 0.9546\n",
      "macro avg:\n",
      "  precision: 0.9540\n",
      "  recall: 0.9545\n",
      "  f1-score: 0.9542\n",
      "  support: 12772.0000\n",
      "weighted avg:\n",
      "  precision: 0.9546\n",
      "  recall: 0.9546\n",
      "  f1-score: 0.9546\n",
      "  support: 12772.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Load pre-trained GloVe embeddings\n",
    "def load_glove(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Load GloVe vectors\n",
    "glove_path = r\"c:\\Users\\Admin\\Downloads\\glove.6B\\glove.6B.300d.txt\"\n",
    "embeddings_index = load_glove(glove_path)\n",
    "\n",
    "# Step 2: Convert each document to GloVe embeddings by averaging word vectors\n",
    "def get_glove_embedding(text, embeddings_index, embed_dim=300):\n",
    "    words = text.split()\n",
    "    embeddings = [embeddings_index[word] for word in words if word in embeddings_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(embed_dim)\n",
    "\n",
    "# Generate GloVe embeddings for the entire dataset\n",
    "X_glove = np.array([get_glove_embedding(text, embeddings_index) for text in data['processed_full_content']])\n",
    "\n",
    "# Step 3: Generate CountVectorizer features\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X_count = vectorizer.fit_transform(data['processed_full_content'])\n",
    "\n",
    "# Step 4: Standardize GloVe embeddings and combine with CountVectorizer features\n",
    "scaler = StandardScaler()\n",
    "X_glove_scaled = scaler.fit_transform(X_glove)\n",
    "\n",
    "# Use hstack to combine sparse CountVectorizer matrix with dense (scaled) GloVe embeddings\n",
    "X_combined = hstack((X_count, X_glove_scaled))\n",
    "X_combined = csr_matrix(X_combined)  # Convert to csr_matrix for subscriptable indexing\n",
    "\n",
    "# Define labels\n",
    "y = data['label']\n",
    "\n",
    "# Step 5: Set up Stratified K-Fold Cross-Validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "f1_scores = []\n",
    "all_classification_reports = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_combined, y):\n",
    "    # Split the data into training and testing sets for the current fold\n",
    "    X_train, X_test = X_combined[train_index], X_combined[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Train the SVM model with L2 regularization (penalty='l2')\n",
    "    svm = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, random_state=42)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    \n",
    "    # Calculate F1-score for the current fold\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "    classification_rep = classification_report(y_test, y_pred, digits=4, output_dict=True)\n",
    "    all_classification_reports.append(classification_rep)\n",
    "    \n",
    "    # Print F1-score and classification report for the current fold\n",
    "    print(f\"Fold {fold} - F1 Score (weighted): {f1}\")\n",
    "    print(f\"Fold {fold} - Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(f\"Fold {fold} - Classification Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    fold += 1\n",
    "\n",
    "# Step 6: Calculate and display average F1-score across all folds\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "print(\"\\nAverage F1 Score (weighted) across all folds:\", avg_f1_score)\n",
    "\n",
    "# Calculate the average classification report across folds\n",
    "avg_classification_report = {}\n",
    "for key in all_classification_reports[0].keys():\n",
    "    if isinstance(all_classification_reports[0][key], dict):\n",
    "        avg_classification_report[key] = {metric: np.mean([report[key][metric] for report in all_classification_reports]) \n",
    "                                          for metric in all_classification_reports[0][key]}\n",
    "    else:\n",
    "        avg_classification_report[key] = np.mean([report[key] for report in all_classification_reports])\n",
    "\n",
    "print(\"\\nAverage Classification Report across all folds:\")\n",
    "for label, metrics in avg_classification_report.items():\n",
    "    if isinstance(metrics, dict):\n",
    "        print(f\"{label}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{label}: {metrics:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with Count Vectoriser, Glove, Stratified K-fold Cross-validation, L2 Regularisation, Gridsearch hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "18 fits failed out of a total of 36.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "8 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'log_loss', 'squared_hinge', 'squared_error', 'epsilon_insensitive', 'modified_huber', 'perceptron', 'squared_epsilon_insensitive', 'hinge', 'huber'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'squared_error', 'epsilon_insensitive', 'modified_huber', 'log_loss', 'huber', 'squared_hinge', 'squared_epsilon_insensitive', 'perceptron'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'perceptron', 'epsilon_insensitive', 'squared_error', 'squared_hinge', 'log_loss', 'huber', 'squared_epsilon_insensitive', 'modified_huber'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'epsilon_insensitive', 'perceptron', 'huber', 'hinge', 'modified_huber', 'squared_error', 'squared_epsilon_insensitive', 'log_loss', 'squared_hinge'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'perceptron', 'epsilon_insensitive', 'hinge', 'squared_hinge', 'squared_epsilon_insensitive', 'modified_huber', 'log_loss', 'squared_error', 'huber'}. Got 'log' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.95029665 0.95029665        nan        nan 0.95694344 0.95694344\n",
      "        nan        nan 0.95875832 0.95875832        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - F1 Score (weighted): 0.9617665341623912\n",
      "Best Parameters for Fold 1: {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2'}\n",
      "Fold 1 - Confusion Matrix:\n",
      " [[6754  201]\n",
      " [ 287 5530]]\n",
      "Fold 1 - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9592    0.9711    0.9651      6955\n",
      "           1     0.9649    0.9507    0.9577      5817\n",
      "\n",
      "    accuracy                         0.9618     12772\n",
      "   macro avg     0.9621    0.9609    0.9614     12772\n",
      "weighted avg     0.9618    0.9618    0.9618     12772\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "18 fits failed out of a total of 36.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'epsilon_insensitive', 'perceptron', 'huber', 'hinge', 'modified_huber', 'squared_error', 'squared_epsilon_insensitive', 'log_loss', 'squared_hinge'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "7 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'log_loss', 'squared_hinge', 'squared_error', 'epsilon_insensitive', 'modified_huber', 'perceptron', 'squared_epsilon_insensitive', 'hinge', 'huber'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'perceptron', 'epsilon_insensitive', 'squared_error', 'squared_hinge', 'log_loss', 'huber', 'squared_epsilon_insensitive', 'modified_huber'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'squared_error', 'epsilon_insensitive', 'modified_huber', 'log_loss', 'huber', 'squared_hinge', 'squared_epsilon_insensitive', 'perceptron'}. Got 'log' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.94938322 0.94938322        nan        nan 0.95634208 0.95634208\n",
      "        nan        nan 0.96015084 0.96015084        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - F1 Score (weighted): 0.9575828834614227\n",
      "Best Parameters for Fold 2: {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2'}\n",
      "Fold 2 - Confusion Matrix:\n",
      " [[6771  185]\n",
      " [ 356 5460]]\n",
      "Fold 2 - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9500    0.9734    0.9616      6956\n",
      "           1     0.9672    0.9388    0.9528      5816\n",
      "\n",
      "    accuracy                         0.9576     12772\n",
      "   macro avg     0.9586    0.9561    0.9572     12772\n",
      "weighted avg     0.9579    0.9576    0.9576     12772\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "18 fits failed out of a total of 36.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'log_loss', 'squared_hinge', 'squared_error', 'epsilon_insensitive', 'modified_huber', 'perceptron', 'squared_epsilon_insensitive', 'hinge', 'huber'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'epsilon_insensitive', 'perceptron', 'huber', 'hinge', 'modified_huber', 'squared_error', 'squared_epsilon_insensitive', 'log_loss', 'squared_hinge'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'squared_error', 'epsilon_insensitive', 'modified_huber', 'log_loss', 'huber', 'squared_hinge', 'squared_epsilon_insensitive', 'perceptron'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'perceptron', 'epsilon_insensitive', 'squared_error', 'squared_hinge', 'log_loss', 'huber', 'squared_epsilon_insensitive', 'modified_huber'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_hinge', 'huber', 'perceptron', 'epsilon_insensitive', 'modified_huber', 'squared_error', 'squared_epsilon_insensitive', 'hinge', 'log_loss'}. Got 'log' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.94734273 0.94734273        nan        nan 0.95527504 0.95527504\n",
      "        nan        nan 0.95862228 0.95862228        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - F1 Score (weighted): 0.9631268049126205\n",
      "Best Parameters for Fold 3: {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2'}\n",
      "Fold 3 - Confusion Matrix:\n",
      " [[6712  244]\n",
      " [ 227 5589]]\n",
      "Fold 3 - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9673    0.9649    0.9661      6956\n",
      "           1     0.9582    0.9610    0.9596      5816\n",
      "\n",
      "    accuracy                         0.9631     12772\n",
      "   macro avg     0.9627    0.9629    0.9628     12772\n",
      "weighted avg     0.9631    0.9631    0.9631     12772\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "18 fits failed out of a total of 36.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "7 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'log_loss', 'squared_hinge', 'squared_error', 'epsilon_insensitive', 'modified_huber', 'perceptron', 'squared_epsilon_insensitive', 'hinge', 'huber'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_hinge', 'huber', 'perceptron', 'epsilon_insensitive', 'modified_huber', 'squared_error', 'squared_epsilon_insensitive', 'hinge', 'log_loss'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'epsilon_insensitive', 'perceptron', 'huber', 'hinge', 'modified_huber', 'squared_error', 'squared_epsilon_insensitive', 'log_loss', 'squared_hinge'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'perceptron', 'epsilon_insensitive', 'squared_error', 'squared_hinge', 'log_loss', 'huber', 'squared_epsilon_insensitive', 'modified_huber'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'perceptron', 'epsilon_insensitive', 'hinge', 'squared_hinge', 'squared_epsilon_insensitive', 'modified_huber', 'log_loss', 'squared_error', 'huber'}. Got 'log' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.9497557  0.9497557         nan        nan 0.95673095 0.95673095\n",
      "        nan        nan 0.95902794 0.95902794        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - F1 Score (weighted): 0.9633483014265164\n",
      "Best Parameters for Fold 4: {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2'}\n",
      "Fold 4 - Confusion Matrix:\n",
      " [[6739  217]\n",
      " [ 251 5565]]\n",
      "Fold 4 - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9641    0.9688    0.9664      6956\n",
      "           1     0.9625    0.9568    0.9596      5816\n",
      "\n",
      "    accuracy                         0.9634     12772\n",
      "   macro avg     0.9633    0.9628    0.9630     12772\n",
      "weighted avg     0.9634    0.9634    0.9633     12772\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "18 fits failed out of a total of 36.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'log_loss', 'squared_hinge', 'squared_error', 'epsilon_insensitive', 'modified_huber', 'perceptron', 'squared_epsilon_insensitive', 'hinge', 'huber'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "7 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_hinge', 'huber', 'perceptron', 'epsilon_insensitive', 'modified_huber', 'squared_error', 'squared_epsilon_insensitive', 'hinge', 'log_loss'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'perceptron', 'epsilon_insensitive', 'hinge', 'squared_hinge', 'squared_epsilon_insensitive', 'modified_huber', 'log_loss', 'squared_error', 'huber'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'epsilon_insensitive', 'perceptron', 'huber', 'hinge', 'modified_huber', 'squared_error', 'squared_epsilon_insensitive', 'log_loss', 'squared_hinge'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'perceptron', 'epsilon_insensitive', 'squared_error', 'squared_hinge', 'log_loss', 'huber', 'squared_epsilon_insensitive', 'modified_huber'}. Got 'log' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.95016168 0.95016168        nan        nan 0.95761372 0.95761372\n",
      "        nan        nan 0.95941319 0.95941319        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - F1 Score (weighted): 0.9624713138481806\n",
      "Best Parameters for Fold 5: {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2'}\n",
      "Fold 5 - Confusion Matrix:\n",
      " [[6760  196]\n",
      " [ 283 5533]]\n",
      "Fold 5 - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9598    0.9718    0.9658      6956\n",
      "           1     0.9658    0.9513    0.9585      5816\n",
      "\n",
      "    accuracy                         0.9625     12772\n",
      "   macro avg     0.9628    0.9616    0.9621     12772\n",
      "weighted avg     0.9625    0.9625    0.9625     12772\n",
      "\n",
      "\n",
      "Average F1 Score (weighted) across all folds: 0.9616591675622264\n",
      "\n",
      "Average Classification Report across all folds:\n",
      "0:\n",
      "  precision: 0.9601\n",
      "  recall: 0.9700\n",
      "  f1-score: 0.9650\n",
      "  support: 6955.8000\n",
      "1:\n",
      "  precision: 0.9637\n",
      "  recall: 0.9517\n",
      "  f1-score: 0.9577\n",
      "  support: 5816.2000\n",
      "accuracy: 0.9617\n",
      "macro avg:\n",
      "  precision: 0.9619\n",
      "  recall: 0.9609\n",
      "  f1-score: 0.9613\n",
      "  support: 12772.0000\n",
      "weighted avg:\n",
      "  precision: 0.9617\n",
      "  recall: 0.9617\n",
      "  f1-score: 0.9617\n",
      "  support: 12772.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Load pre-trained GloVe embeddings\n",
    "def load_glove(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Load GloVe vectors\n",
    "glove_path = r\"c:\\Users\\Admin\\Downloads\\glove.6B\\glove.6B.300d.txt\"\n",
    "embeddings_index = load_glove(glove_path)\n",
    "\n",
    "# Step 2: Convert each document to GloVe embeddings by averaging word vectors\n",
    "def get_glove_embedding(text, embeddings_index, embed_dim=300):\n",
    "    words = text.split()\n",
    "    embeddings = [embeddings_index[word] for word in words if word in embeddings_index]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(embed_dim)\n",
    "\n",
    "# Generate GloVe embeddings for the entire dataset\n",
    "X_glove = np.array([get_glove_embedding(text, embeddings_index) for text in data['processed_full_content']])\n",
    "\n",
    "# Step 3: Generate CountVectorizer features\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X_count = vectorizer.fit_transform(data['processed_full_content'])\n",
    "\n",
    "# Step 4: Standardize GloVe embeddings and combine with CountVectorizer features\n",
    "scaler = StandardScaler()\n",
    "X_glove_scaled = scaler.fit_transform(X_glove)\n",
    "\n",
    "# Use hstack to combine sparse CountVectorizer matrix with dense (scaled) GloVe embeddings\n",
    "X_combined = hstack((X_count, X_glove_scaled))\n",
    "X_combined = csr_matrix(X_combined)  # Convert to csr_matrix for subscriptable indexing\n",
    "\n",
    "# Define labels\n",
    "y = data['label']\n",
    "\n",
    "# Step 5: Set up Stratified K-Fold Cross-Validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "f1_scores = []\n",
    "all_classification_reports = []\n",
    "\n",
    "# Step 6: Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'penalty': ['l2'],\n",
    "    'loss': ['hinge', 'log'],  # 'hinge' for SVM, 'log' for logistic regression\n",
    "    'max_iter': [1000, 2000]\n",
    "}\n",
    "\n",
    "# Step 7: Iterate over each fold in cross-validation\n",
    "for train_index, test_index in kf.split(X_combined, y):\n",
    "    # Split the data into training and testing sets for the current fold\n",
    "    X_train, X_test = X_combined[train_index], X_combined[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Set up GridSearchCV with the SGDClassifier and parameter grid\n",
    "    svm = SGDClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(svm, param_grid, scoring='f1_weighted', cv=3, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model from GridSearchCV\n",
    "    best_svm = grid_search.best_estimator_\n",
    "    y_pred = best_svm.predict(X_test)\n",
    "    \n",
    "    # Calculate F1-score for the current fold\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "    classification_rep = classification_report(y_test, y_pred, digits=4, output_dict=True)\n",
    "    all_classification_reports.append(classification_rep)\n",
    "    \n",
    "    # Print F1-score, best parameters, and classification report for the current fold\n",
    "    print(f\"Fold {fold} - F1 Score (weighted): {f1}\")\n",
    "    print(f\"Best Parameters for Fold {fold}: {grid_search.best_params_}\")\n",
    "    print(f\"Fold {fold} - Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(f\"Fold {fold} - Classification Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    fold += 1\n",
    "\n",
    "# Step 8: Calculate and display average F1-score across all folds\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "print(\"\\nAverage F1 Score (weighted) across all folds:\", avg_f1_score)\n",
    "\n",
    "# Calculate the average classification report across folds\n",
    "avg_classification_report = {}\n",
    "for key in all_classification_reports[0].keys():\n",
    "    if isinstance(all_classification_reports[0][key], dict):\n",
    "        avg_classification_report[key] = {metric: np.mean([report[key][metric] for report in all_classification_reports]) \n",
    "                                          for metric in all_classification_reports[0][key]}\n",
    "    else:\n",
    "        avg_classification_report[key] = np.mean([report[key] for report in all_classification_reports])\n",
    "\n",
    "print(\"\\nAverage Classification Report across all folds:\")\n",
    "for label, metrics in avg_classification_report.items():\n",
    "    if isinstance(metrics, dict):\n",
    "        print(f\"{label}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{label}: {metrics:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
