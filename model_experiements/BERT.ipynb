{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Detection to Credibility: A Machine Learning Framework for Assessing News Source Reliability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# For concurrency (running functions in parallel)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# For caching (to speed up repeated function calls)\n",
    "from functools import lru_cache\n",
    "\n",
    "# For progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting and Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Language Detection packages\n",
    "# `langdetect` for detecting language\n",
    "from langdetect import detect as langdetect_detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "# `langid` for an alternative language detection method\n",
    "from langid import classify as langid_classify\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Part-of-speech tagging\n",
    "from nltk import pos_tag\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "# Regular expressions for text pattern matching\n",
    "import re\n",
    "\n",
    "# Word Cloud generation\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>full_content</th>\n",
       "      <th>processed_full_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No comment is expected from Barack Obama Membe...</td>\n",
       "      <td>no comment expect barack obama member fyf911 f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Did they post their votes for Hillary already?</td>\n",
       "      <td>post vote hillari alreadi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Now, most of the demonstrators gathered last n...</td>\n",
       "      <td>demonstr gather last night exercis constitut p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>A dozen politically active pastors came here f...</td>\n",
       "      <td>dozen polit activ pastor came privat dinner fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The RS-28 Sarmat missile, dubbed Satan 2, will...</td>\n",
       "      <td>rs-28 sarmat missil dub satan 2 replac ss-18 f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63855</th>\n",
       "      <td>0</td>\n",
       "      <td>WASHINGTON (Reuters) - Hackers believed to be ...</td>\n",
       "      <td>washington reuter hacker believ work russian g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63856</th>\n",
       "      <td>1</td>\n",
       "      <td>You know, because in fantasyland Republicans n...</td>\n",
       "      <td>know fantasyland republican never question cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63857</th>\n",
       "      <td>0</td>\n",
       "      <td>Migrants Refuse To Leave Train At Refugee Camp...</td>\n",
       "      <td>migrant refus leav train refuge camp hungari t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63858</th>\n",
       "      <td>0</td>\n",
       "      <td>MEXICO CITY (Reuters) - Donald Trump’s combati...</td>\n",
       "      <td>mexico citi reuter donald trump ’ comb style b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63859</th>\n",
       "      <td>1</td>\n",
       "      <td>Goldman Sachs Endorses Hillary Clinton For Pre...</td>\n",
       "      <td>goldman sach endors hillari clinton presid gol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63860 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                       full_content  \\\n",
       "0          1  No comment is expected from Barack Obama Membe...   \n",
       "1          1     Did they post their votes for Hillary already?   \n",
       "2          1  Now, most of the demonstrators gathered last n...   \n",
       "3          0  A dozen politically active pastors came here f...   \n",
       "4          1  The RS-28 Sarmat missile, dubbed Satan 2, will...   \n",
       "...      ...                                                ...   \n",
       "63855      0  WASHINGTON (Reuters) - Hackers believed to be ...   \n",
       "63856      1  You know, because in fantasyland Republicans n...   \n",
       "63857      0  Migrants Refuse To Leave Train At Refugee Camp...   \n",
       "63858      0  MEXICO CITY (Reuters) - Donald Trump’s combati...   \n",
       "63859      1  Goldman Sachs Endorses Hillary Clinton For Pre...   \n",
       "\n",
       "                                  processed_full_content  \n",
       "0      no comment expect barack obama member fyf911 f...  \n",
       "1                              post vote hillari alreadi  \n",
       "2      demonstr gather last night exercis constitut p...  \n",
       "3      dozen polit activ pastor came privat dinner fr...  \n",
       "4      rs-28 sarmat missil dub satan 2 replac ss-18 f...  \n",
       "...                                                  ...  \n",
       "63855  washington reuter hacker believ work russian g...  \n",
       "63856  know fantasyland republican never question cit...  \n",
       "63857  migrant refus leav train refuge camp hungari t...  \n",
       "63858  mexico citi reuter donald trump ’ comb style b...  \n",
       "63859  goldman sach endors hillari clinton presid gol...  \n",
       "\n",
       "[63860 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 3.1/10.0 MB 15.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.8/10.0 MB 14.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.2/10.0 MB 14.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 14.6 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.20.1-cp312-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 19.2 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.26.2 safetensors-0.4.5 tokenizers-0.20.1 transformers-4.46.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic BERT\n",
    "\n",
    "BERT is a transformer model that provides powerful pre-trained embeddings for downstream tasks such as fake news classification using text.\n",
    "\n",
    "To use BERT in Tensorflow, we utilise the `transformers` library by HuggingFace, which simplifies the process of loading pre-trained BERT models and tokenizers.\n",
    "\n",
    "BERT is limited to a maximum input length of 512 tokens.\n",
    "\n",
    "Fine-tuning BERT usually requires fewer epochs (2-4) and smaller batch sizes (16 or 32) due to memory constraints and pre-trained knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d, val_d, train_labels, val_labels = train_test_split(data['processed_full_content'],data['label'],test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = list(train_d)\n",
    "texts_val = list(val_d)\n",
    "\n",
    "max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts_train = tokenizer(texts_train, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length)\n",
    "tokenized_texts_val = tokenizer(texts_val, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  3290, 25022,  3775,  2128, 19901,  7426, 21877,  7361,  2140,\n",
       "         2915,  2757,  9857,  2305,  4319, 24497, 18622,  2102,  2415,  2642,\n",
       "         3290,  2112, 26568, 29469,  2954,  2048, 13675, 27605,  2078,  6080,\n",
       "        14056,  2110,  4905,  4962,  2099,  2125,  2594,  2056,  5607,  2165,\n",
       "         2173, 17496,  5101, 25022,  3775, 28480,  6178,  4183,  3675,  2110,\n",
       "        28480,  2187,  2403, 21877,  7361,  2140,  2757,  1022,  1999,  9103,\n",
       "         2099, 14056,  9758,   102])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts_train['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts_train['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(list(train_labels))\n",
    "val_labels = torch.tensor(list(val_labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(tokenized_texts_train['input_ids'], tokenized_texts_train['attention_mask'], train_labels)\n",
    "val_dataset = TensorDataset(tokenized_texts_val['input_ids'], tokenized_texts_val['attention_mask'], val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2, num_hidden_layers=12, hidden_size=768, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BatchNorm1d\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "optimizer = AdamW([\n",
    "    {'params': model.bert.parameters(), 'lr': 5e-6},\n",
    "    {'params': model.classifier.parameters(), 'lr': 5e-6}\n",
    "], lr=5e-6)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "train_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training: 100%|██████████| 1597/1597 [1:58:19<00:00,  4.45s/it]\n",
      "Epoch 1/3 - Validation: 100%|██████████| 400/400 [09:07<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n",
      "Training Loss: 0.1939, Training Accuracy: 0.9206\n",
      "Training Precision: 0.9209, Training Recall: 0.9206, Training F1: 0.9207\n",
      "Validation Loss: 0.1120, Validation Accuracy: 0.9574\n",
      "Validation Precision: 0.9577, Validation Recall: 0.9574, Validation F1: 0.9573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training: 100%|██████████| 1597/1597 [1:58:44<00:00,  4.46s/it]\n",
      "Epoch 2/3 - Validation: 100%|██████████| 400/400 [08:52<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/3\n",
      "Training Loss: 0.0942, Training Accuracy: 0.9649\n",
      "Training Precision: 0.9649, Training Recall: 0.9649, Training F1: 0.9649\n",
      "Validation Loss: 0.1068, Validation Accuracy: 0.9627\n",
      "Validation Precision: 0.9627, Validation Recall: 0.9627, Validation F1: 0.9626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training: 100%|██████████| 1597/1597 [1:57:48<00:00,  4.43s/it]\n",
      "Epoch 3/3 - Validation: 100%|██████████| 400/400 [09:02<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/3\n",
      "Training Loss: 0.0859, Training Accuracy: 0.9696\n",
      "Training Precision: 0.9696, Training Recall: 0.9696, Training F1: 0.9696\n",
      "Validation Loss: 0.1066, Validation Accuracy: 0.9623\n",
      "Validation Precision: 0.9623, Validation Recall: 0.9623, Validation F1: 0.9623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    tr_correct_preds = 0\n",
    "    all_tr_labels = []\n",
    "    all_tr_preds = []\n",
    "\n",
    "    # Use tqdm to create a progress bar for the training loop\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Training\"):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        tr_loss = outputs.loss\n",
    "        train_loss += tr_loss.item()\n",
    "        tr_loss.backward()\n",
    "\n",
    "        tr_logits = outputs.logits\n",
    "        tr_preds = torch.argmax(tr_logits, dim=1)\n",
    "        tr_correct_preds += torch.sum(tr_preds == labels).item()\n",
    "\n",
    "        # Collect predictions and true labels\n",
    "        all_tr_labels.extend(labels.cpu().numpy())\n",
    "        all_tr_preds.extend(tr_preds.cpu().numpy())\n",
    "\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Calculate average training loss and accuracy\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracy = tr_correct_preds / len(train_d)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Calculate Precision, Recall, F1 for training\n",
    "    train_precision = precision_score(all_tr_labels, all_tr_preds, average='weighted')\n",
    "    train_recall = recall_score(all_tr_labels, all_tr_preds, average='weighted')\n",
    "    train_f1 = f1_score(all_tr_labels, all_tr_preds, average='weighted')\n",
    "\n",
    "    # Validation phase with tqdm progress bar\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    all_val_labels = []\n",
    "    all_val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Validation\"):\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_preds += torch.sum(preds == labels).item()\n",
    "\n",
    "            # Collect predictions and true labels\n",
    "            all_val_labels.extend(labels.cpu().numpy())\n",
    "            all_val_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracy = correct_preds / len(val_d)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Calculate Precision, Recall, F1 for validation\n",
    "    val_precision = precision_score(all_val_labels, all_val_preds, average='weighted')\n",
    "    val_recall = recall_score(all_val_labels, all_val_preds, average='weighted')\n",
    "    val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted')\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Training Precision: {train_precision:.4f}, Training Recall: {train_recall:.4f}, Training F1: {train_f1:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation Precision: {val_precision:.4f}, Validation Recall: {val_recall:.4f}, Validation F1: {val_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
