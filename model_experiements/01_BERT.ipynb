{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# For concurrency (running functions in parallel)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# For caching (to speed up repeated function calls)\n",
    "from functools import lru_cache\n",
    "\n",
    "# For progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "import nltk\n",
    "\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Part-of-speech tagging\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working dir: /home/dariusng2103/projects/dm_project/DM-Fake-News-Detection\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if \"workding_dir\" not in locals():\n",
    "    workding_dir = str(Path.cwd().parent)\n",
    "os.chdir(workding_dir)\n",
    "sys.path.append(workding_dir)\n",
    "print(\"working dir:\", workding_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load train and test datasets, for both original and rewritten (using LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'full_content', 'processed_full_content'],\n",
       "        num_rows: 54441\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'full_content', 'processed_full_content'],\n",
       "        num_rows: 6050\n",
       "    })\n",
       "    rewritten_train: Dataset({\n",
       "        features: ['label', 'full_content', 'processed_full_content'],\n",
       "        num_rows: 54441\n",
       "    })\n",
       "    rewritten_test: Dataset({\n",
       "        features: ['label', 'full_content', 'processed_full_content'],\n",
       "        num_rows: 6050\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "\n",
    "datasets = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": [\n",
    "            \"dataset/train_data_1.csv\",\n",
    "            \"dataset/train_data_2.csv\",\n",
    "            \"dataset/train_data_3.csv\",\n",
    "            \"dataset/train_data_4.csv\",\n",
    "        ],\n",
    "        \"test\": \"dataset/test_data.csv\",\n",
    "        \"rewritten_train\": [\n",
    "            \"dataset/rewritten_train_data_1.csv\",\n",
    "            \"dataset/rewritten_train_data_2.csv\",\n",
    "            \"dataset/rewritten_train_data_3.csv\",\n",
    "            \"dataset/rewritten_train_data_4.csv\",\n",
    "        ],\n",
    "        \"rewritten_test\": \"dataset/rewritten_test_data.csv\",\n",
    "    },\n",
    ")\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic BERT\n",
    "\n",
    "BERT is a transformer model that provides powerful pre-trained embeddings for downstream tasks such as fake news classification using text.\n",
    "\n",
    "### Transformer Architecture\n",
    "To use BERT in Tensorflow, we utilise the `transformers` library by HuggingFace, which simplifies the process of loading pre-trained BERT models and tokenizers. The transformer uses a mechanism called 'self-attention' to weigh the importance of each word in a sentence relative to others, allowing it to process entire sentences at once instead of word-by-word.\n",
    "\n",
    "### Bidirectional Context Understanding\n",
    "BERT reads text in both directions at once which helps BERT to understand the full context of each word in relation to its surrounding words, making it excellent at capturing meaning, nuances, and relationships in language.\n",
    "\n",
    "### Pretraining Tasks\n",
    "- **Masked Language Modelling (MLM):** Randomly masks some words in sentences and trains the model to predict them. This task encourages BERT to learn contextual relationships and gain a better understanding of language.\n",
    "\n",
    "- **Next Sentence Prediction (NSP):** Trains BERT to understand relationships between sentences by predicting whether one sentence naturally follows another. This helps BERT with tasks that require understanding sentence-pairs, like question-answering.\n",
    "\n",
    "BERT is limited to a maximum input length of 512 tokens.\n",
    "\n",
    "Fine-tuning BERT usually requires fewer epochs (2-4) and smaller batch sizes (16 or 32) due to memory constraints and pre-trained knowledge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertForSequenceClassification, AdamW\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorDataset, DataLoader\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
